{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4942f81c-bf15-4b65-9e36-1edd8f14182d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/m13518040/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import gc\n",
    "import time\n",
    "\n",
    "import torch\n",
    "gc.collect()\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"4\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"  \n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "torch.cuda.set_device(device)\n",
    "# print(device)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import swifter\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "from utils.utils_init_dataset import set_seed, load_dataset_loader\n",
    "from utils.utils_semantic_use import USE\n",
    "from utils.utils_data_utils import DocumentSentimentDataset, DocumentSentimentDataLoader, EmotionDetectionDataset, EmotionDetectionDataLoader\n",
    "from utils.utils_metrics import document_sentiment_metrics_fn\n",
    "from utils.utils_init_model import text_logit, fine_tuning_model, eval_model, init_model, logit_prob, load_word_index\n",
    "from utils.get_args import get_args\n",
    "\n",
    "# from attack.adv_attack import attack\n",
    "import os, sys\n",
    "\n",
    "from utils.utils_init_model import text_logit, fine_tuning_model, eval_model, init_model, logit_prob, load_word_index\n",
    "import time\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from attack.attack_helper import get_synonyms, codemix_perturbation, codemix_perturbation, codemix_perturbation_cache, synonym_replacement, swap_minimum_importance_words, trans_dict, read_dict\n",
    "\n",
    "# from attack.adv_attack import attack\n",
    "import main\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words_set = []\n",
    "for w in stopwords.words('indonesian'):\n",
    "    stop_words_set.append(w)\n",
    "\n",
    "from icecream import ic\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "from itertools import chain\n",
    "from deep_translator import GoogleTranslator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca6aceaa-d129-40ef-a44f-03a9bb94c071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack(text_ls,\n",
    "           true_label,\n",
    "           predictor,\n",
    "           tokenizer,\n",
    "           att_ratio,\n",
    "           perturbation_technique,\n",
    "           translator=None,\n",
    "           lang_codemix=None,\n",
    "           sim_predictor=None,\n",
    "           sim_score_threshold=0,\n",
    "           sim_score_window=15,\n",
    "           batch_size=32, \n",
    "           import_score_threshold=-1.):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    label_dict = {\n",
    "        'positive': 0, \n",
    "        'neutral': 1, \n",
    "        'negative': 2}\n",
    "    \n",
    "    original_text = text_ls\n",
    "    splitted_text = text_ls.split()\n",
    "    \n",
    "    orig_label, orig_probs, orig_prob = logit_prob(text_ls, predictor, tokenizer)\n",
    "    \n",
    "    if len(splitted_text) > sim_score_window:\n",
    "        sim_score_threshold = 0.1  \n",
    "    #     SEK SALAAHHHHHH\n",
    "    if true_label != orig_label:\n",
    "        running_time = round(time.time() - start_time, 2)\n",
    "        # perturbed_text, perturbed_semantic_sim, orig_label, orig_probs, perturbed_label, perturbed_probs, translated_words, running_time\n",
    "        orig_probs = np.round(orig_probs.detach().cpu().numpy().tolist(),4)\n",
    "        return original_text, 1.000, orig_label, orig_probs, orig_label, orig_probs,'', running_time\n",
    "    else:\n",
    "        text_ls = splitted_text\n",
    "#         isalnum karena kata yg ada nonalnum nya punya prediksi yang ga konsisten\n",
    "        text_ls = [word for word in text_ls if word.isalnum()]\n",
    "        len_text = len(text_ls)\n",
    "        \n",
    "        half_sim_score_window = (sim_score_window - 1) // 2\n",
    "        \n",
    "        leave_1_texts = [' '.join(text_ls[:ii] + [tokenizer.mask_token] + text_ls[min(ii + 1, len_text):]) for ii in range(len_text)]\n",
    "        \n",
    "        ic(leave_1_texts)\n",
    "        \n",
    "        leave_1_probs = []\n",
    "        leave_1_probs_argmax = []\n",
    "        \n",
    "        if len(text_ls) <=1:\n",
    "            leave_1_probs_argmax, leave_1_probs, _ = logit_prob(leave_1_texts, predictor, tokenizer)\n",
    "            leave_1_probs_argmax = [leave_1_probs_argmax]\n",
    "            leave_1_probs = [leave_1_probs.detach().cpu().numpy()]\n",
    "            # ic(leave_1_probs)\n",
    "            leave_1_probs = torch.tensor(np.array(leave_1_probs)).to(\"cuda:0\")\n",
    "        else:\n",
    "            leave_1_probs_argmax, leave_1_probs, _ = logit_prob(leave_1_texts, predictor, tokenizer,batch=True)\n",
    "            leave_1_probs = torch.tensor(np.array(leave_1_probs.detach().cpu().numpy())).to(\"cuda:0\")\n",
    "        \n",
    "        orig_prob_extended=np.empty(len_text)\n",
    "        orig_prob_extended.fill(orig_prob)\n",
    "        \n",
    "        orig_prob_extended = torch.tensor(orig_prob_extended).to(\"cuda:0\")\n",
    "        \n",
    "        arr1 = orig_prob_extended - leave_1_probs[:,orig_label] + float(leave_1_probs_argmax != orig_label)\n",
    "        arr2 = (leave_1_probs.max(dim=-1)[0].to(\"cuda:0\") - orig_probs[leave_1_probs_argmax].to(\"cuda:0\"))\n",
    "        \n",
    "        import_scores = arr1*arr2\n",
    "        import_scores = [im * -1 for im in import_scores]\n",
    "        \n",
    "        words_perturb = []\n",
    "        for idx, score in sorted(enumerate(import_scores), key=lambda x: x[1], reverse=True):\n",
    "            try:\n",
    "                if score > import_score_threshold and text_ls[idx] not in stop_words_set:\n",
    "                    words_perturb.append((idx, text_ls[idx], score.item()))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(idx, len(text_ls), import_scores.shape, text_ls, len(leave_1_texts))\n",
    "        \n",
    "        # ic(sorted(words_perturb, key=lambda x: x[2]))\n",
    "        ic(words_perturb)\n",
    "        \n",
    "        num_perturbation = math.floor(len(words_perturb)*att_ratio)\n",
    "        \n",
    "        if num_perturbation < 1:\n",
    "            num_perturbation = 1\n",
    "        \n",
    "#       top words perturb berisi list kata terpenting yang tidak akan diswitch ketika first_codemix_sim_score < sim_score_threshold\n",
    "        top_words_perturb = words_perturb[:num_perturbation]\n",
    "    \n",
    "        ic(top_words_perturb)\n",
    "        \n",
    "        trans_word = [twp[1] for twp in top_words_perturb]\n",
    "        \n",
    "        if perturbation_technique == \"codemixing\":\n",
    "            perturbed_text,translated_words = codemix_perturbation(text_ls, lang_codemix, top_words_perturb)\n",
    "        elif perturbation_technique == \"synonym_replacement\":\n",
    "            perturbed_text,translated_words = synonym_replacement(text_ls, top_words_perturb)\n",
    "        \n",
    "        ic(perturbed_text)\n",
    "        \n",
    "        first_perturbation_sim_score = sim_predictor.semantic_sim(original_text, perturbed_text)\n",
    "        \n",
    "        ic(first_perturbation_sim_score)\n",
    "        \n",
    "        words_perturb_candidates = []\n",
    "        if first_perturbation_sim_score < sim_score_threshold:\n",
    "            words_perturb_candidates.append(top_words_perturb)\n",
    "            swapped = swap_minimum_importance_words(words_perturb, top_words_perturb)\n",
    "            for s in swapped:\n",
    "                words_perturb_candidates.append(s)\n",
    "\n",
    "            words_perturb_candidates = [[tuple(w) for w in wpc] for wpc in words_perturb_candidates]\n",
    "\n",
    "            candidate_comparison = {}\n",
    "            for wpc in words_perturb_candidates:\n",
    "                \n",
    "                if perturbation_technique == \"codemixing\":\n",
    "#                     bisa dicache, gaperlu request satu2\n",
    "                    perturbed_candidate,translated_words = codemix_perturbation(text_ls, lang_codemix, wpc)\n",
    "                elif perturbation_technique == \"synonym_replacement\":\n",
    "                    perturbed_candidate,translated_words = synonym_replacement(text_ls, wpc)\n",
    "                    \n",
    "                perturbed_candidate_sim_score = sim_predictor.semantic_sim(original_text, perturbed_candidate)\n",
    "                \n",
    "                candidate_comparison[perturbed_candidate] = (perturbed_candidate_sim_score, wpc[-1][-1], wpc)\n",
    "\n",
    "            sorted_candidate_comparison = sorted(candidate_comparison.keys(), key=lambda x: (float(candidate_comparison[x][0]), float(candidate_comparison[x][1])), reverse=True)\n",
    "\n",
    "            perturbed_text = sorted_candidate_comparison[0]\n",
    "            \n",
    "            top_words_perturb = candidate_comparison[sorted_candidate_comparison[0]][-1]\n",
    "            trans_word = [twp[1] for twp in top_words_perturb]\n",
    "            \n",
    "        perturbed_semantic_sim = sim_predictor.semantic_sim(original_text, perturbed_text)\n",
    "        if perturbed_semantic_sim < sim_score_threshold:\n",
    "            perturbed_text = original_text\n",
    "            perturbed_semantic_sim = 1.000\n",
    "        \n",
    "        perturbed_label, perturbed_probs, perturbed_prob = logit_prob(perturbed_text, predictor, tokenizer)\n",
    "        \n",
    "        orig_probs = np.round(orig_probs.detach().cpu().numpy().tolist(),4)\n",
    "        perturbed_probs = np.round(perturbed_probs.detach().cpu().numpy().tolist(),4)\n",
    "        \n",
    "        running_time = round(time.time() - start_time, 4)\n",
    "        \n",
    "        return perturbed_text, perturbed_semantic_sim, orig_label, orig_probs, perturbed_label, perturbed_probs, translated_words, running_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e533330-48af-4e33-a7d7-148e3415d5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_target=\"IndoBERT-Large\"\n",
    "downstream_task=\"sentiment\"\n",
    "attack_strategy=\"adversarial\"\n",
    "finetune_epoch=1\n",
    "num_sample=50\n",
    "# exp_name=\n",
    "perturbation_technique=\"codemixing\"\n",
    "perturb_ratio=0.8\n",
    "perturb_lang=\"en\"\n",
    "seed=26092020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5600622c-804b-46ca-a345-0e8415f1eda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "use = USE()\n",
    "\n",
    "tokenizer, config, finetuned_model = init_model(model_target, downstream_task, seed)\n",
    "w2i, i2w = load_word_index(downstream_task)\n",
    "\n",
    "train_dataset, train_loader, train_path = load_dataset_loader(downstream_task, 'train', tokenizer)\n",
    "valid_dataset, valid_loader, valid_path = load_dataset_loader(downstream_task, 'valid', tokenizer)\n",
    "test_dataset, test_loader, test_path = load_dataset_loader(downstream_task, 'test', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b735fefd-a0f7-452e-a2c7-d573141f4476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| leave_1_texts: ['[MASK] menerima dan belajar dari setiap kesalahan karena itu yang akan '\n",
      "                    'menjadikan mu kuat dalam menjalani',\n",
      "                    'ikhlas [MASK] dan belajar dari setiap kesalahan karena itu yang akan '\n",
      "                    'menjadikan mu kuat dalam menjalani',\n",
      "                    'ikhlas menerima [MASK] belajar dari setiap kesalahan karena itu yang akan '\n",
      "                    'menjadikan mu kuat dalam menjalani',\n",
      "                    'ikhlas menerima dan [MASK] dari setiap kesalahan karena itu yang akan '\n",
      "                    'menjadikan mu kuat dalam menjalani',\n",
      "                    'ikhlas menerima dan belajar [MASK] setiap kesalahan karena itu yang akan '\n",
      "                    'menjadikan mu kuat dalam menjalani',\n",
      "                    'ikhlas menerima dan belajar dari [MASK] kesalahan karena itu yang akan '\n",
      "                    'menjadikan mu kuat dalam menjalani',\n",
      "                    'ikhlas menerima dan belajar dari setiap [MASK] karena itu yang akan '\n",
      "                    'menjadikan mu kuat dalam menjalani',\n",
      "                    'ikhlas menerima dan belajar dari setiap kesalahan [MASK] itu yang akan '\n",
      "                    'menjadikan mu kuat dalam menjalani',\n",
      "                    'ikhlas menerima dan belajar dari setiap kesalahan karena [MASK] yang akan '\n",
      "                    'menjadikan mu kuat dalam menjalani',\n",
      "                    'ikhlas menerima dan belajar dari setiap kesalahan karena itu [MASK] akan '\n",
      "                    'menjadikan mu kuat dalam menjalani',\n",
      "                    'ikhlas menerima dan belajar dari setiap kesalahan karena itu yang [MASK] '\n",
      "                    'menjadikan mu kuat dalam menjalani',\n",
      "                    'ikhlas menerima dan belajar dari setiap kesalahan karena itu yang akan '\n",
      "                    '[MASK] mu kuat dalam menjalani',\n",
      "                    'ikhlas menerima dan belajar dari setiap kesalahan karena itu yang akan '\n",
      "                    'menjadikan [MASK] kuat dalam menjalani',\n",
      "                    'ikhlas menerima dan belajar dari setiap kesalahan karena itu yang akan '\n",
      "                    'menjadikan mu [MASK] dalam menjalani',\n",
      "                    'ikhlas menerima dan belajar dari setiap kesalahan karena itu yang akan '\n",
      "                    'menjadikan mu kuat [MASK] menjalani',\n",
      "                    'ikhlas menerima dan belajar dari setiap kesalahan karena itu yang akan '\n",
      "                    'menjadikan mu kuat dalam [MASK]']\n",
      "ic| words_perturb: [(0, 'ikhlas', 0.05440767772205746),\n",
      "                    (1, 'menerima', 0.0224133920792724),\n",
      "                    (11, 'menjadikan', 0.007895631257643032),\n",
      "                    (15, 'menjalani', 0.0058537278574171125),\n",
      "                    (3, 'belajar', -0.004041122187370405),\n",
      "                    (13, '"
     ]
    }
   ],
   "source": [
    "text,label = None,None\n",
    "\n",
    "row_text = \"ikhlas menerima kesalahan, dan belajar dari setiap kesalahan , karena itu yang akan menjadikan mu kuat dalam menjalani kehidupan.\"\n",
    "row_sentiment = 0\n",
    "\n",
    "translator = None\n",
    "if perturbation_technique == \"codemixing\":\n",
    "    translator = read_dict(os.getcwd() + r\"/dicts/new_dict_\"+perturb_lang+\".txt\")\n",
    "\n",
    "attack(\n",
    "    text_ls = row_text,\n",
    "    true_label = row_sentiment,\n",
    "    predictor = finetuned_model,\n",
    "    tokenizer = tokenizer, \n",
    "    att_ratio = perturb_ratio,\n",
    "    perturbation_technique = perturbation_technique,\n",
    "    translator=translator,\n",
    "    lang_codemix = perturb_lang,\n",
    "    sim_predictor = use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209654f1-7df9-4c18-a32f-e3bf597d0192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3faced-5743-41df-87c4-2501bdc088f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a986cfea-2322-499e-935d-989a628d8389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629eb873-b377-4faa-9cbf-d645498ab652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c89cc4-87c5-4035-bc15-a87faa6545f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4517dd70-d8c9-4bd3-b8b1-aacb1590d7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de349a7-2731-4806-94ec-34c54741b5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a3d32a-1f08-4371-a338-1120bbffc25c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6df894-26f1-4964-b122-8c6298a6589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dataset = valid_dataset.load_dataset(valid_path).head(num_sample)\n",
    "# exp_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f026c3d-7c27-44dc-a9b3-bdea5014b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(perturbation_technique)\n",
    "if downstream_task == 'sentiment':\n",
    "    text = 'text'\n",
    "    label = 'sentiment'\n",
    "    exp_dataset[['perturbed_text', 'perturbed_semantic_sim', 'pred_label', 'pred_proba', 'perturbed_label', 'perturbed_proba', 'translated_word(s)', 'running_time(s)']] = exp_dataset.progress_apply(\n",
    "        lambda row: attack(\n",
    "            text_ls = row_text,\n",
    "            true_label = row_sentiment,\n",
    "            predictor = finetuned_model,\n",
    "            tokenizer = tokenizer, \n",
    "            att_ratio = perturb_ratio,\n",
    "            perturbation_technique = perturbation_technique,\n",
    "            translator=translator,\n",
    "            lang_codemix = perturb_lang,\n",
    "            sim_predictor = use), axis=1, result_type='expand'\n",
    "    )\n",
    "elif downstream_task == 'emotion':\n",
    "    text = 'tweet'\n",
    "    label = 'label'\n",
    "    exp_dataset[['perturbed_text', 'perturbed_semantic_sim', 'pred_label', 'pred_proba', 'perturbed_label', 'perturbed_proba', 'translated_word(s)', 'running_time(s)']] = exp_dataset.progress_apply(\n",
    "        lambda row: attack(\n",
    "            text_ls = row.tweet,\n",
    "            true_label = row.label,\n",
    "            predictor = finetuned_model,\n",
    "            tokenizer = tokenizer, \n",
    "            att_ratio = perturb_ratio,\n",
    "            perturbation_technique = perturbation_technique,\n",
    "            translator=translator,\n",
    "            lang_codemix = perturb_lang,\n",
    "            sim_predictor = use), axis=1, result_type='expand'\n",
    "    )\n",
    "\n",
    "before_attack = accuracy_score(exp_dataset[label], exp_dataset['pred_label'])\n",
    "after_attack = accuracy_score(exp_dataset[label], exp_dataset['perturbed_label'])\n",
    "\n",
    "exp_dataset.loc[exp_dataset.index[0], 'before_attack_acc'] = before_attack\n",
    "exp_dataset.loc[exp_dataset.index[0], 'after_attack_acc'] = after_attack\n",
    "exp_dataset.loc[exp_dataset.index[0], 'avg_running_time(s)'] = exp_dataset[\"running_time(s)\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58203d50-e315-4e71-ad1e-e127a23f64d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\n",
    "    model_target=\"IndoBERT\",\n",
    "    downstream_task=\"sentiment\",\n",
    "    attack_strategy=\"adversarial\",\n",
    "    finetune_epoch=1,\n",
    "    num_sample=2,\n",
    "    exp_name=\"fixx\",\n",
    "    perturbation_technique=\"codemixing\",\n",
    "    perturb_ratio=0.4,\n",
    "    perturb_lang=\"en\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166894a9-ccfc-4bf9-84ad-cfa00abc4548",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"betapa bejad kader gerindra yang anggota dprd mencabuli anak smp , rakyat harus cerdas partai mana yang harus di tengelamkan di 2019\"\n",
    "true_label = 2\n",
    "\n",
    "perturbed_text, perturbed_semantic_sim, pred_label, pred_proba, perturbed_label, perturbed_proba, translated_words, running_time = attack(\n",
    "    text_ls = txt,\n",
    "    true_label = true_label,\n",
    "    predictor = finetuned_model,\n",
    "    tokenizer = tokenizer, \n",
    "    att_ratio = 0.8,\n",
    "    perturbation_technique = perturbation_technique,\n",
    "    lang_codemix = perturb_lang,\n",
    "    sim_predictor = use\n",
    ")\n",
    "\n",
    "ic(perturbed_text)\n",
    "ic(perturbed_semantic_sim)\n",
    "ic(pred_label)\n",
    "ic(pred_proba)\n",
    "ic(perturbed_label)\n",
    "ic(perturbed_proba)\n",
    "ic(translated_words)\n",
    "ic(running_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383d43b9-3257-4c74-abc7-1a7c48c2d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686d7659-f7b8-4747-9387-7b3425161b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dataset.to_csv(os.getcwd() + r'/result/'+\"test\"+\".csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
