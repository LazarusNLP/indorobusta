{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a666a356-cf28-4988-a99c-c5e8f8ee887d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMOD_DIR=/usr/share/lmod/lmod/libexec/\n",
      "TMUX=/tmp/tmux-22086/default,1030980,0\n",
      "CUDA_LAUNCH_BLOCKING=1\n",
      "SSH_CLIENT=103.107.4.31 63325 22\n",
      "USER=m13518040\n",
      "LMOD_COLORIZE=yes\n",
      "MPLBACKEND=module://matplotlib_inline.backend_inline\n",
      "LMOD_PKG=/usr/share/lmod/lmod\n",
      "LD_LIBRARY_PATH=/usr/local/cuda/lib:/usr/local/cuda/lib\n",
      "HOME=/home/m13518040\n",
      "MOTD_SHOWN=pam\n",
      "OLDPWD=/raid/data/m13518040\n",
      "SSH_TTY=/dev/pts/2\n",
      "PAGER=cat\n",
      "TF_CPP_MIN_LOG_LEVEL=1\n",
      "LMOD_sys=Linux\n",
      "TF2_BEHAVIOR=1\n",
      "CUDA_VISIBLE_DEVICES=4,5\n",
      "LOGNAME=m13518040\n",
      "TERM=xterm-color\n",
      "CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "PATH=/home/m13518040/.local/bin:/usr/local/cuda/bin:/opt/bin/:/usr/local/cuda/bin:/home/m13518040/.local/bin:/usr/local/cuda/bin:/opt/bin/:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n",
      "LMOD_FULL_SETTARG_SUPPORT=no\n",
      "LMOD_PREPEND_BLOCK=normal\n",
      "KMP_DUPLICATE_LIB_OK=True\n",
      "KMP_INIT_AT_FORK=FALSE\n",
      "LANG=en_US.UTF-8\n",
      "MODULEPATH_ROOT=/sw/modules\n",
      "SHELL=/bin/sh\n",
      "LMOD_CMD=/usr/share/lmod/lmod/libexec/lmod\n",
      "BASH_ENV=/usr/share/lmod/lmod/init/bash\n",
      "GIT_PAGER=cat\n",
      "PWD=/raid/data/m13518040/tugas-akhir-repository\n",
      "CLICOLOR=1\n",
      "SSH_CONNECTION=103.107.4.31 63325 167.205.35.247 22\n",
      "XDG_DATA_DIRS=/usr/local/share:/usr/share:/var/lib/snapd/desktop\n",
      "JPY_PARENT_PID=1034665\n",
      "PYDEVD_USE_FRAME_EVAL=NO\n",
      "LMOD_arch=x86_64\n",
      "MANPATH=/usr/share/lmod/lmod/share/man::\n",
      "MODULEPATH=/sw/modules/all\n",
      "TMUX_PANE=%0\n",
      "LMOD_SETTARG_CMD=:\n",
      "MODULESHOME=/usr/share/lmod/lmod\n",
      "LMOD_DIR=/usr/share/lmod/lmod/libexec/\n",
      "TMUX=/tmp/tmux-22086/default,1030980,0\n",
      "CUDA_LAUNCH_BLOCKING=1\n",
      "SSH_CLIENT=103.107.4.31 63325 22\n",
      "USER=m13518040\n",
      "LMOD_COLORIZE=yes\n",
      "MPLBACKEND=module://matplotlib_inline.backend_inline\n",
      "LMOD_PKG=/usr/share/lmod/lmod\n",
      "LD_LIBRARY_PATH=/usr/local/cuda/lib:/usr/local/cuda/lib\n",
      "HOME=/home/m13518040\n",
      "MOTD_SHOWN=pam\n",
      "OLDPWD=/raid/data/m13518040\n",
      "SSH_TTY=/dev/pts/2\n",
      "PAGER=cat\n",
      "TF_CPP_MIN_LOG_LEVEL=1\n",
      "LMOD_sys=Linux\n",
      "TF2_BEHAVIOR=1\n",
      "CUDA_VISIBLE_DEVICES=4\n",
      "LOGNAME=m13518040\n",
      "TERM=xterm-color\n",
      "CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "PATH=/home/m13518040/.local/bin:/usr/local/cuda/bin:/opt/bin/:/usr/local/cuda/bin:/home/m13518040/.local/bin:/usr/local/cuda/bin:/opt/bin/:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n",
      "LMOD_FULL_SETTARG_SUPPORT=no\n",
      "LMOD_PREPEND_BLOCK=normal\n",
      "KMP_DUPLICATE_LIB_OK=True\n",
      "KMP_INIT_AT_FORK=FALSE\n",
      "LANG=en_US.UTF-8\n",
      "MODULEPATH_ROOT=/sw/modules\n",
      "SHELL=/bin/sh\n",
      "LMOD_CMD=/usr/share/lmod/lmod/libexec/lmod\n",
      "BASH_ENV=/usr/share/lmod/lmod/init/bash\n",
      "GIT_PAGER=cat\n",
      "PWD=/raid/data/m13518040/tugas-akhir-repository\n",
      "CLICOLOR=1\n",
      "SSH_CONNECTION=103.107.4.31 63325 167.205.35.247 22\n",
      "XDG_DATA_DIRS=/usr/local/share:/usr/share:/var/lib/snapd/desktop\n",
      "JPY_PARENT_PID=1034665\n",
      "PYDEVD_USE_FRAME_EVAL=NO\n",
      "LMOD_arch=x86_64\n",
      "MANPATH=/usr/share/lmod/lmod/share/man::\n",
      "MODULEPATH=/sw/modules/all\n",
      "TMUX_PANE=%0\n",
      "LMOD_SETTARG_CMD=:\n",
      "MODULESHOME=/usr/share/lmod/lmod\n"
     ]
    }
   ],
   "source": [
    "# !export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64\n",
    "!env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "!env CUDA_VISIBLE_DEVICES=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8a28c34-c97c-4875-b788-deea22e2fc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 40 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/m13518040/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/m13518040/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw to /home/m13518040/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import gc\n",
    "import swifter\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "gc.collect()\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5\"  \n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "# import jax.numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw')\n",
    "\n",
    "stop_words_set = []\n",
    "for w in stopwords.words('indonesian'):\n",
    "    stop_words_set.append(w)\n",
    "\n",
    "import math\n",
    "import re\n",
    "import copy\n",
    "\n",
    "from operator import itemgetter\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "from utils.utils_init_dataset import set_seed, load_dataset_loader\n",
    "from utils.utils_semantic_use import USE\n",
    "from utils.utils_data_utils import DocumentSentimentDataset, DocumentSentimentDataLoader, EmotionDetectionDataset, EmotionDetectionDataLoader\n",
    "from utils.utils_metrics import document_sentiment_metrics_fn\n",
    "from utils.utils_init_model import text_logit, fine_tuning_model, eval_model, init_model, logit_prob, load_word_index\n",
    "from utils.utils_forward_fn import forward_sequence_classification\n",
    "\n",
    "# debugger\n",
    "from icecream import ic\n",
    "\n",
    "# !pip install pandarallel\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cuda:3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "836e07b9-a236-4c59-896c-ea20950129c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 09:24:20.863288: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib:/usr/local/cuda/lib\n",
      "2022-04-16 09:24:20.864375: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "model_target=\"IndoBERT\"\n",
    "downstream_task=\"sentiment\"\n",
    "attack_strategy=\"adversarial\"\n",
    "# finetune_epoch=1\n",
    "num_sample=50\n",
    "# exp_name=\n",
    "perturbation_technique=\"codemixing\"\n",
    "perturb_ratio=0.4\n",
    "perturb_lang=\"en\"\n",
    "seed=26092020\n",
    "dataset=\"valid\"\n",
    "\n",
    "set_seed(seed)\n",
    "use = USE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84b5a7b6-6ab2-49c4-a665-bb520a566f55",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid device ordinal",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m valid_dataset, valid_loader, valid_path \u001b[38;5;241m=\u001b[39m load_dataset_loader(downstream_task, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m, tokenizer)\n\u001b[1;32m      6\u001b[0m test_dataset, test_loader, test_path \u001b[38;5;241m=\u001b[39m load_dataset_loader(downstream_task, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, tokenizer)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mfinetuned_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# finetuned_model = fine_tuning_model(model, i2w, train_loader, valid_loader, finetune_epoch)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:899\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    896\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 899\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 570\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    574\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    575\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    581\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 570\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    574\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    575\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    581\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 570\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    574\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    575\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    581\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:593\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 593\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:897\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    896\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 897\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: invalid device ordinal"
     ]
    }
   ],
   "source": [
    "tokenizer, config, finetuned_model = init_model(model_target, downstream_task, seed)\n",
    "w2i, i2w = load_word_index(downstream_task)\n",
    "\n",
    "train_dataset, train_loader, train_path = load_dataset_loader(downstream_task, 'train', tokenizer)\n",
    "valid_dataset, valid_loader, valid_path = load_dataset_loader(downstream_task, 'valid', tokenizer)\n",
    "test_dataset, test_loader, test_path = load_dataset_loader(downstream_task, 'test', tokenizer)\n",
    "\n",
    "finetuned_model.to(device)\n",
    "# finetuned_model = fine_tuning_model(model, i2w, train_loader, valid_loader, finetune_epoch)\n",
    "\n",
    "if dataset == \"valid\":\n",
    "    exp_dataset = valid_dataset.load_dataset(valid_path)[380:430]\n",
    "elif dataset == \"train\":\n",
    "    exp_dataset = train_dataset.load_dataset(train_path)\n",
    "# exp_dataset = dd.from_pandas(exp_dataset, npartitions=10)\n",
    "# exp_dataset = te.DataFrame.from_pandas(exp_dataset)\n",
    "text,label = None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4387054-7f09-4d66-bdf5-f3f4addb856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "total_loss, total_correct, total_labels = 0, 0, 0\n",
    "list_hyp, list_label = [], []\n",
    "\n",
    "pbar = tqdm(test_loader, leave=True, total=len(test_loader))\n",
    "for i, batch_data in enumerate(pbar):\n",
    "    ic(batch_data)\n",
    "    _, batch_hyp, _ = forward_sequence_classification(finetuned_model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "    list_hyp += batch_hyp\n",
    "\n",
    "# Save prediction\n",
    "df = pd.DataFrame({'label':list_hyp}).reset_index()\n",
    "df.to_csv('pred.txt', index=False)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce752fbe-45b2-4505-ba52-cfa966e4f9cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c4fe57-5a3b-4afc-bde4-1a81e607f428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc01f6a-e4a5-4a41-9ef5-81bac3f82b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f1f60b-0752-4aaa-8cab-4685e7a35e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dd8c72-3016-4282-b3b1-67a11c7ede70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56ced3b-403c-488c-b085-28baacda82a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
