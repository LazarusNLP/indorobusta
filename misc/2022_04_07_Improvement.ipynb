{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "546ac2b4-f52d-4f77-8b42-4bc237320037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMOD_DIR=/usr/share/lmod/lmod/libexec/\n",
      "TMUX=/tmp/tmux-22086/default,1030980,0\n",
      "SSH_CLIENT=103.107.4.31 63325 22\n",
      "USER=m13518040\n",
      "LMOD_COLORIZE=yes\n",
      "MPLBACKEND=module://matplotlib_inline.backend_inline\n",
      "LMOD_PKG=/usr/share/lmod/lmod\n",
      "LD_LIBRARY_PATH=/usr/local/cuda/lib:/usr/local/cuda/lib\n",
      "HOME=/home/m13518040\n",
      "MOTD_SHOWN=pam\n",
      "OLDPWD=/raid/data/m13518040\n",
      "SSH_TTY=/dev/pts/2\n",
      "PAGER=cat\n",
      "LMOD_sys=Linux\n",
      "LOGNAME=m13518040\n",
      "TERM=xterm-color\n",
      "PATH=/home/m13518040/.local/bin:/usr/local/cuda/bin:/opt/bin/:/usr/local/cuda/bin:/home/m13518040/.local/bin:/usr/local/cuda/bin:/opt/bin/:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n",
      "LMOD_FULL_SETTARG_SUPPORT=no\n",
      "LMOD_PREPEND_BLOCK=normal\n",
      "LANG=en_US.UTF-8\n",
      "MODULEPATH_ROOT=/sw/modules\n",
      "SHELL=/bin/sh\n",
      "LMOD_CMD=/usr/share/lmod/lmod/libexec/lmod\n",
      "BASH_ENV=/usr/share/lmod/lmod/init/bash\n",
      "GIT_PAGER=cat\n",
      "PWD=/raid/data/m13518040/tugas-akhir-repository\n",
      "CLICOLOR=1\n",
      "SSH_CONNECTION=103.107.4.31 63325 167.205.35.247 22\n",
      "XDG_DATA_DIRS=/usr/local/share:/usr/share:/var/lib/snapd/desktop\n",
      "JPY_PARENT_PID=1034665\n",
      "PYDEVD_USE_FRAME_EVAL=NO\n",
      "LMOD_arch=x86_64\n",
      "MANPATH=/usr/share/lmod/lmod/share/man::\n",
      "MODULEPATH=/sw/modules/all\n",
      "TMUX_PANE=%0\n",
      "LMOD_SETTARG_CMD=:\n",
      "MODULESHOME=/usr/share/lmod/lmod\n",
      "CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "LMOD_DIR=/usr/share/lmod/lmod/libexec/\n",
      "TMUX=/tmp/tmux-22086/default,1030980,0\n",
      "SSH_CLIENT=103.107.4.31 63325 22\n",
      "USER=m13518040\n",
      "LMOD_COLORIZE=yes\n",
      "MPLBACKEND=module://matplotlib_inline.backend_inline\n",
      "LMOD_PKG=/usr/share/lmod/lmod\n",
      "LD_LIBRARY_PATH=/usr/local/cuda/lib:/usr/local/cuda/lib\n",
      "HOME=/home/m13518040\n",
      "MOTD_SHOWN=pam\n",
      "OLDPWD=/raid/data/m13518040\n",
      "SSH_TTY=/dev/pts/2\n",
      "PAGER=cat\n",
      "LMOD_sys=Linux\n",
      "LOGNAME=m13518040\n",
      "TERM=xterm-color\n",
      "PATH=/home/m13518040/.local/bin:/usr/local/cuda/bin:/opt/bin/:/usr/local/cuda/bin:/home/m13518040/.local/bin:/usr/local/cuda/bin:/opt/bin/:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n",
      "LMOD_FULL_SETTARG_SUPPORT=no\n",
      "LMOD_PREPEND_BLOCK=normal\n",
      "LANG=en_US.UTF-8\n",
      "MODULEPATH_ROOT=/sw/modules\n",
      "SHELL=/bin/sh\n",
      "LMOD_CMD=/usr/share/lmod/lmod/libexec/lmod\n",
      "BASH_ENV=/usr/share/lmod/lmod/init/bash\n",
      "GIT_PAGER=cat\n",
      "PWD=/raid/data/m13518040/tugas-akhir-repository\n",
      "CLICOLOR=1\n",
      "SSH_CONNECTION=103.107.4.31 63325 167.205.35.247 22\n",
      "XDG_DATA_DIRS=/usr/local/share:/usr/share:/var/lib/snapd/desktop\n",
      "JPY_PARENT_PID=1034665\n",
      "PYDEVD_USE_FRAME_EVAL=NO\n",
      "LMOD_arch=x86_64\n",
      "MANPATH=/usr/share/lmod/lmod/share/man::\n",
      "MODULEPATH=/sw/modules/all\n",
      "TMUX_PANE=%0\n",
      "LMOD_SETTARG_CMD=:\n",
      "MODULESHOME=/usr/share/lmod/lmod\n",
      "CUDA_VISIBLE_DEVICES=4\n"
     ]
    }
   ],
   "source": [
    "# !export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64\n",
    "!env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "!env CUDA_VISIBLE_DEVICES=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15edb277-e4c7-4c1b-998c-32c720b17121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils_init_model import text_logit, fine_tuning_model, eval_model, init_model, load_word_index\n",
    "import time\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# import jax.numpy as jnp\n",
    "# from jax import random\n",
    "\n",
    "import math\n",
    "# from .attack_helper import get_synonyms, codemix_perturbation, synonym_replacement, swap_minimum_importance_words, trans_dict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "stop_words_set = []\n",
    "for w in stopwords.words('indonesian'):\n",
    "    stop_words_set.append(w)\n",
    "\n",
    "from icecream import ic\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "import copy\n",
    "from deep_translator import GoogleTranslator\n",
    "import numpy as np\n",
    "import random\n",
    "# from utils.utils_semantic_use import USE\n",
    "from utils.utils_init_dataset import set_seed, load_dataset_loader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "# tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d543c47-539a-4917-aea4-6e26eea1c708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jax\n",
    "# !pip install --upgrade \"jax[cuda]\" -f https://storage.googleapis.com/jax-releases/jax_releases.html  # Note: wheels only available on linux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cef73d5-b954-43f7-b4b2-4b1d8b46cd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class USE(object):\n",
    "    def __init__(self):\n",
    "        import tensorflow_hub as hub\n",
    "        super(USE, self).__init__()\n",
    "        module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
    "        self.embed = hub.load(module_url)\n",
    "\n",
    "    def semantic_sim(self, sentA : str, sentB : str) -> float:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sentA: The first sentence.\n",
    "            sentB: The second sentence.\n",
    "\n",
    "        Returns:\n",
    "            Cosine distance between two sentences.\n",
    "        \n",
    "        \"\"\"\n",
    "        if sentA.lower() == sentB.lower():\n",
    "            return 1.000\n",
    "        ret = np.array(self.embed([sentA, sentB]).numpy())\n",
    "        score = ret[0].dot(ret[1]) / (np.linalg.norm(ret[0]) * np.linalg.norm(ret[1]))\n",
    "        if score>1:\n",
    "            return 1.000\n",
    "        return score\n",
    "\n",
    "    def after_attack(self, input, adversarial_sample):\n",
    "        if adversarial_sample is not None:\n",
    "            return self.calc_score(input[\"x\"], adversarial_sample)\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "# USE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d653914f-8ae1-44af-b961-ba0fbc4dba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class USE(object):\n",
    "#     def __init__(self, module_url=None):\n",
    "#         super(USE, self).__init__()\n",
    "#         module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
    "#         self.embed = hub.load(module_url)\n",
    "        \n",
    "#         def embed(input):\n",
    "#             return model(input)\n",
    "        \n",
    "#         # config = tf.compat.v1.ConfigProto()\n",
    "#         # config.gpu_options.allow_growth = True\n",
    "#         # self.sess = tf.compat.v1.Session(config=config)\n",
    "#         # self.sess.run([tf.compat.v1.global_variables_initializer(), tf.compat.v1.tables_initializer()])\n",
    "\n",
    "#     def semantic_sim(self, sents1, sents2):\n",
    "#         # with tf.Session() as session:\n",
    "#         #     session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "#         #     with tf.device('GPU'):\n",
    "#         #         message_embeddings_ = session.run(self.embed([sents1, sents2]))\n",
    "        \n",
    "#         if sents1.lower() == sents2.lower():\n",
    "#             return 1.000\n",
    "        \n",
    "#         message_embeddings_ = self.embed([sents1, sents2])\n",
    "#         # message_embeddings_ = message_embeddings_.eval(session=self.sess)\n",
    "#         corr = np.tensordot(message_embeddings_, message_embeddings_, axes=(-1,-1))\n",
    "        \n",
    "#         if corr[0][1] > 1:\n",
    "#             return 1.000\n",
    "#         return corr[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3576aabe-dd88-40dd-8ff3-64d225667ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "def get_synonyms(word):\n",
    "    if word in stop_words_set:\n",
    "        return [word]\n",
    "\n",
    "    word_lemmas = wordnet.lemmas(word, lang=\"ind\")\n",
    "    \n",
    "    hypernyms = []\n",
    "    for lem in word_lemmas:\n",
    "        hypernyms.append(lem.synset().hypernyms())\n",
    "\n",
    "    if not any(hypernyms):\n",
    "        return [word]\n",
    "    \n",
    "    lemma_corp = []\n",
    "    \n",
    "    for hypernym in hypernyms:\n",
    "        if(len(hypernym) < 1):\n",
    "            continue\n",
    "        else:\n",
    "            lemma_corp.append(hypernym[0].lemmas(lang=\"ind\"))\n",
    "            \n",
    "    lemmas = set()\n",
    "    for list_lemmas in lemma_corp:\n",
    "        if(len(list_lemmas) < 1):\n",
    "            lemmas.add(word)\n",
    "        else:\n",
    "            for l in list_lemmas:\n",
    "                lemmas.add(l.name())\n",
    "    \n",
    "    clean_synonyms = set()\n",
    "    for syn in lemmas.copy():\n",
    "        synonym = syn.replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "        synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "        clean_synonyms.add(synonym) \n",
    "    if word in clean_synonyms:\n",
    "        clean_synonyms.remove(word)\n",
    "    \n",
    "    if len(list(clean_synonyms)) < 1:\n",
    "        return [word]\n",
    "    else:\n",
    "        return list(clean_synonyms)\n",
    "\n",
    "def codemix_perturbation(words, target_lang, words_perturb):\n",
    "    \"\"\"\n",
    "    'su': 'sundanese'\n",
    "    'jw': 'javanese'\n",
    "    'ms': 'malay'\n",
    "    'en': 'english'\n",
    "    \"\"\"\n",
    "    translator = GoogleTranslator(source=\"id\", target=target_lang)\n",
    "    new_wp = []\n",
    "    for wp in words_perturb:\n",
    "        if wp[1].isalpha():\n",
    "            new_wp.append(wp[1])\n",
    "    \n",
    "    if len(new_wp) == 1:\n",
    "        new_wp_trans = dict(zip(new_wp, translator.translate(new_wp[0])))\n",
    "    elif len(new_wp) == 0:\n",
    "        return ' '.join(words), {}\n",
    "        \n",
    "    new_wp_trans = dict(zip(new_wp, translator.translate_batch(new_wp)))\n",
    "    \n",
    "    supported_langs = [\"su\", \"jw\", \"ms\", \"en\", \"fr\", \"it\"]\n",
    "    \n",
    "    if target_lang not in supported_langs:\n",
    "        raise ValueError('Language Unavailable')\n",
    "    \n",
    "    new_words = words.copy()\n",
    "    \n",
    "    if len(words_perturb) >= 1:\n",
    "        for perturb_word in new_wp_trans.keys():\n",
    "            new_words = [new_wp_trans[perturb_word] if word == perturb_word and word.isalpha() else word for word in new_words]\n",
    "\n",
    "    return ' '.join(new_words), new_wp_trans\n",
    "\n",
    "def synonym_replacement(words, words_perturb):    \n",
    "    # new_words = words.copy()\n",
    "    new_wp = []\n",
    "    for wp in words_perturb:\n",
    "        if wp[1].isalpha():\n",
    "            new_wp.append(wp[1])\n",
    "    \n",
    "    if len(new_wp) == 1:\n",
    "        new_wp_trans = dict(zip(new_wp, get_synonyms(new_wp[0])[0]))\n",
    "    elif len(new_wp) == 0:\n",
    "        return ' '.join(words), {}\n",
    "    \n",
    "    wp_trans=[]\n",
    "    for wp in new_wp:\n",
    "        wp_trans.append(get_synonyms(wp)[0])\n",
    "    \n",
    "    new_wp_trans = dict(zip(new_wp, wp_trans))\n",
    "    \n",
    "    new_words = words.copy()\n",
    "    \n",
    "    if len(words_perturb) >= 1:\n",
    "        for perturb_word in new_wp_trans.keys():\n",
    "            new_words = [new_wp_trans[perturb_word] if word == perturb_word and word.isalpha() else word for word in new_words]\n",
    "    \n",
    "    return ' '.join(new_words), new_wp_trans\n",
    "\n",
    "# fungsi untuk mencari kandidat lain ketika sebuah kandidat perturbasi kurang dari sim_score_threshold\n",
    "def swap_minimum_importance_words(words_perturb, top_importance_words):\n",
    "    print(\"pointer\")\n",
    "    def get_minimums(word_tups):\n",
    "        arr = []\n",
    "        for wt in word_tups:\n",
    "            if wt[2] == min(top_importance_words, key = lambda t: t[2])[2]:\n",
    "                arr.append(wt)\n",
    "        return arr\n",
    "    \n",
    "    # get list of words with minimum importance score\n",
    "    minimum_import = get_minimums(top_importance_words)\n",
    "    unlisted = list(set(words_perturb).symmetric_difference(set(top_importance_words)))\n",
    "    \n",
    "    ic(unlisted)\n",
    "    \n",
    "    len_wp = len(top_importance_words)\n",
    "    len_ul = len(unlisted)\n",
    "    \n",
    "    res = []\n",
    "    for i in range(len_wp):\n",
    "        if top_importance_words[i] in minimum_import:\n",
    "            temp_wp = list(copy.deepcopy(top_importance_words))\n",
    "            if len(temp_wp) == 0:\n",
    "                break\n",
    "            \n",
    "            swapped_wp = np.array([(temp_wp) for i in range(len_ul)])\n",
    "            \n",
    "            ic(swapped_wp)\n",
    "            for j in range(len(swapped_wp)):\n",
    "                temp_sm = np.vstack((swapped_wp[j], tuple(unlisted[j])))\n",
    "                \n",
    "                res.append(temp_sm.tolist())\n",
    "            temp_wp.pop(i)\n",
    "                \n",
    "    return res\n",
    "\n",
    "def trans_dict(wordlist, perturbation_technique, lang=None):\n",
    "    translated_wordlist = {}\n",
    "    # ic(wordlist)\n",
    "    if perturbation_technique == \"codemixing\":\n",
    "        translator = GoogleTranslator(source=\"id\", target=lang)\n",
    "        \n",
    "        for word in wordlist:\n",
    "            if word.isalpha():\n",
    "                translated_wordlist[word] = translator.translate(word)\n",
    "            else:\n",
    "                translated_wordlist[word] = word\n",
    "    else:\n",
    "        for word in wordlist:\n",
    "            translated_wordlist[word] = get_synonyms(word)[0]\n",
    "            \n",
    "    return translated_wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c0efbfd-7e66-4e0b-8f0f-3e7bb95904fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_prob(text_ls, predictor, tokenizer, batch=False):\n",
    "    with torch.inference_mode():\n",
    "        if(batch==True):\n",
    "            subwords = tokenizer.batch_encode_plus(text_ls, return_tensors=\"pt\", padding=True)['input_ids'].to(finetuned_model.device)\n",
    "            logits = finetuned_model(subwords).logits\n",
    "            # ic(logits)\n",
    "            orig_label = [t.squeeze().item() for t in torch.topk(logits, k=1, dim=-1)[1]]\n",
    "            orig_probs = F.softmax(logits, dim=-1).squeeze()\n",
    "            orig_prob = [F.softmax(logits, dim=-1).squeeze()[idx][label].detach().cpu().numpy() for idx, label in enumerate(orig_label)]\n",
    "        else:\n",
    "            subwords = tokenizer.encode(text_ls)\n",
    "            subwords = torch.LongTensor(subwords).view(1, -1).to(predictor.device)\n",
    "\n",
    "            logits = predictor(subwords)[0]\n",
    "            orig_label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "            orig_probs = F.softmax(logits, dim=-1).squeeze()\n",
    "            orig_prob = F.softmax(logits, dim=-1).squeeze()[orig_label].detach().cpu().numpy()\n",
    "    \n",
    "    return orig_label, orig_probs, orig_prob\n",
    "\n",
    "    # subwords = tokenizer.encode(text_ls)\n",
    "    # subwords = torch.LongTensor(subwords).view(1, -1).to(predictor.device)\n",
    "    # logits = predictor(subwords)[0]\n",
    "    # orig_label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "    # orig_probs = F.softmax(logits, dim=-1).squeeze()\n",
    "    # orig_prob = F.softmax(logits, dim=-1).squeeze()[orig_label].detach().cpu().numpy()\n",
    "\n",
    "def attack(text_ls,\n",
    "           true_label,\n",
    "           predictor,\n",
    "           tokenizer,\n",
    "           att_ratio,\n",
    "           perturbation_technique,\n",
    "           lang_codemix=None,\n",
    "           sim_predictor=None,\n",
    "           sim_score_threshold=0,\n",
    "           sim_score_window=15,\n",
    "           batch_size=32, \n",
    "           import_score_threshold=-1.):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    label_dict = {\n",
    "        'positive': 0, \n",
    "        'neutral': 1, \n",
    "        'negative': 2}\n",
    "    \n",
    "    original_text = text_ls\n",
    "    splitted_text = text_ls.split()\n",
    "    \n",
    "    orig_label, orig_probs, orig_prob = logit_prob(text_ls, predictor, tokenizer)\n",
    "    \n",
    "    if len(splitted_text) > sim_score_window:\n",
    "        sim_score_threshold = 0.1  \n",
    "    #     SEK SALAAHHHHHH\n",
    "    if true_label != orig_label:\n",
    "        running_time = round(time.time() - start_time, 2)\n",
    "        # perturbed_text, perturbed_semantic_sim, orig_label, orig_probs, perturbed_label, perturbed_probs, translated_words, running_time\n",
    "        orig_probs = np.round(orig_probs.detach().cpu().numpy().tolist(),4)\n",
    "        return original_text, 1.000, orig_label, orig_probs, orig_label, orig_probs,'', running_time\n",
    "    else:\n",
    "        text_ls = splitted_text\n",
    "        text_ls = [word for word in text_ls if word.isalnum()]\n",
    "        len_text = len(text_ls)\n",
    "        # ic(len_text)\n",
    "        # ic(original_text)\n",
    "        half_sim_score_window = (sim_score_window - 1) // 2\n",
    "        # num_queries = 1\n",
    "        \n",
    "        leave_1_texts = [' '.join(text_ls[:ii] + [tokenizer.mask_token] + text_ls[min(ii + 1, len_text):]) for ii in range(len_text)]\n",
    "        # ic(text_ls)\n",
    "        # ic(leave_1_texts)\n",
    "                \n",
    "        leave_1_probs = []\n",
    "        leave_1_probs_argmax = []\n",
    "        # num_queries += len(leave_1_texts)\n",
    "        \n",
    "# torch.inference mode\n",
    "        # start_time_logit = time.time()\n",
    "        if len(text_ls) <=1:\n",
    "            leave_1_probs_argmax, leave_1_probs, _ = logit_prob(leave_1_texts, predictor, tokenizer)\n",
    "            leave_1_probs_argmax = [leave_1_probs_argmax]\n",
    "            leave_1_probs = [leave_1_probs.detach().cpu().numpy()]\n",
    "            # ic(leave_1_probs)\n",
    "            leave_1_probs = torch.tensor(np.array(leave_1_probs)).to(\"cuda:0\")\n",
    "        else:\n",
    "            leave_1_probs_argmax, leave_1_probs, _ = logit_prob(leave_1_texts, predictor, tokenizer,batch=True)\n",
    "            leave_1_probs = torch.tensor(np.array(leave_1_probs.detach().cpu().numpy())).to(\"cuda:0\")\n",
    "        \n",
    "        # end_time_logit = round(time.time() - start_time_logit, 4)\n",
    "        # ic(end_time_logit)\n",
    "        \n",
    "        # ic(leave_1_probs)\n",
    "        \n",
    "        # start_time_arrops = time.time()\n",
    "        # leave_1_probs = torch.tensor(np.array(leave_1_probs)).to(\"cuda:0\")\n",
    "        # leave_1_probs = torch.tensor(np.array(leave_1_probs.detach().cpu().numpy())).to(\"cuda:0\")\n",
    "        # ic(leave_1_probs)\n",
    "        \n",
    "        orig_prob_extended=np.empty(len_text)\n",
    "        orig_prob_extended.fill(orig_prob)\n",
    "        # ic(orig_prob_extended)\n",
    "        orig_prob_extended = torch.tensor(orig_prob_extended).to(\"cuda:0\")\n",
    "        \n",
    "        # ic(leave_1_probs)\n",
    "        arr1 = orig_prob_extended - leave_1_probs[:,orig_label] + float(leave_1_probs_argmax != orig_label)\n",
    "        arr2 = (leave_1_probs.max(dim=-1)[0].to(\"cuda:0\") - orig_probs[leave_1_probs_argmax].to(\"cuda:0\"))\n",
    "        \n",
    "        import_scores = arr1*arr2\n",
    "        import_scores = [im * -1 for im in import_scores]\n",
    "        \n",
    "        words_perturb = []\n",
    "        for idx, score in sorted(enumerate(import_scores), key=lambda x: x[1], reverse=True):\n",
    "            try:\n",
    "                if score > import_score_threshold and text_ls[idx] not in stop_words_set:\n",
    "                    words_perturb.append((idx, text_ls[idx], score.item()))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(idx, len(text_ls), import_scores.shape, text_ls, len(leave_1_texts))\n",
    "        \n",
    "        num_perturbation = math.floor(len(words_perturb)*att_ratio)\n",
    "        \n",
    "        # end_time_arrops = round(time.time() - start_time_arrops, 4)\n",
    "        # ic(end_time_arrops)\n",
    "        \n",
    "        if num_perturbation < 1:\n",
    "            num_perturbation = 1\n",
    "        \n",
    "#       top words perturb berisi list kata terpenting yang tidak akan diswitch ketika first_codemix_sim_score < sim_score_threshold\n",
    "        top_words_perturb = words_perturb[:num_perturbation]\n",
    "        trans_word = [twp[1] for twp in top_words_perturb]\n",
    "        \n",
    "        # running_time_a = round(time.time() - start_time, 4)\n",
    "        # ic(running_time_a)\n",
    "        # ic(top_words_perturb)\n",
    "        # start_time_pert = time.time()\n",
    "        if perturbation_technique == \"codemixing\":\n",
    "            perturbed_text,translated_words = codemix_perturbation(text_ls, lang_codemix, top_words_perturb)\n",
    "        elif perturbation_technique == \"synonym_replacement\":\n",
    "            perturbed_text,translated_words = synonym_replacement(text_ls, top_words_perturb)\n",
    "        # end_time_pert = round(time.time() - start_time_pert, 4)\n",
    "        # ic(end_time_pert)\n",
    "        \n",
    "        \n",
    "        # start_time_use = time.time()\n",
    "        first_perturbation_sim_score = sim_predictor.semantic_sim(original_text, perturbed_text)\n",
    "        # end_time_use = round(time.time() - start_time_use, 4)\n",
    "        \n",
    "        # ic(end_time_use)\n",
    "        \n",
    "        # ic(first_perturbation_sim_score)\n",
    "        \n",
    "        words_perturb_candidates = []\n",
    "        if first_perturbation_sim_score < sim_score_threshold:\n",
    "            words_perturb_candidates.append(top_words_perturb)\n",
    "            swapped = swap_minimum_importance_words(words_perturb, top_words_perturb)\n",
    "            for s in swapped:\n",
    "                words_perturb_candidates.append(s)\n",
    "\n",
    "            words_perturb_candidates = [[tuple(w) for w in wpc] for wpc in words_perturb_candidates]\n",
    "\n",
    "            candidate_comparison = {}\n",
    "            for wpc in words_perturb_candidates:\n",
    "                \n",
    "                if perturbation_technique == \"codemixing\":\n",
    "#                     bisa dicache, gaperlu request satu2\n",
    "                    perturbed_candidate,translated_words = codemix_perturbation(text_ls, lang_codemix, wpc)\n",
    "                elif perturbation_technique == \"synonym_replacement\":\n",
    "                    perturbed_candidate,translated_words = synonym_replacement(text_ls, wpc)\n",
    "                    \n",
    "                perturbed_candidate_sim_score = sim_predictor.semantic_sim(original_text, perturbed_candidate)\n",
    "                \n",
    "                candidate_comparison[perturbed_candidate] = (perturbed_candidate_sim_score, wpc[-1][-1], wpc)\n",
    "\n",
    "            sorted_candidate_comparison = sorted(candidate_comparison.keys(), key=lambda x: (float(candidate_comparison[x][0]), float(candidate_comparison[x][1])), reverse=True)\n",
    "\n",
    "            perturbed_text = sorted_candidate_comparison[0]\n",
    "            \n",
    "            top_words_perturb = candidate_comparison[sorted_candidate_comparison[0]][-1]\n",
    "            trans_word = [twp[1] for twp in top_words_perturb]\n",
    "            \n",
    "        perturbed_semantic_sim = sim_predictor.semantic_sim(original_text, perturbed_text)\n",
    "        if perturbed_semantic_sim < sim_score_threshold:\n",
    "            perturbed_text = original_text\n",
    "            perturbed_semantic_sim = 1.000\n",
    "        \n",
    "        perturbed_label, perturbed_probs, perturbed_prob = logit_prob(perturbed_text, predictor, tokenizer)\n",
    "        \n",
    "        \n",
    "        # translated_words = trans_dict(trans_word, perturbation_technique, lang_codemix)\n",
    "        \n",
    "        orig_probs = np.round(orig_probs.detach().cpu().numpy().tolist(),4)\n",
    "        perturbed_probs = np.round(perturbed_probs.detach().cpu().numpy().tolist(),4)\n",
    "        \n",
    "        running_time = round(time.time() - start_time, 4)\n",
    "        # ic(running_time)\n",
    "        \n",
    "        # ic(perturbed_text, perturbed_semantic_sim, orig_label, orig_probs, perturbed_label, perturbed_probs, translated_words, running_time)\n",
    "        return perturbed_text, perturbed_semantic_sim, orig_label, orig_probs, perturbed_label, perturbed_probs, translated_words, running_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efa9058f-8e82-419e-a12f-0e2abf856df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 08:54:57.938258: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib:/usr/local/cuda/lib\n",
      "2022-04-17 08:54:57.939085: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "model_target=\"IndoBERT\"\n",
    "downstream_task=\"sentiment\"\n",
    "attack_strategy=\"adversarial\"\n",
    "# finetune_epoch=1\n",
    "num_sample=50\n",
    "# exp_name=\n",
    "perturbation_technique=\"codemixing\"\n",
    "perturb_ratio=0.4\n",
    "perturb_lang=\"en\"\n",
    "seed=26092020\n",
    "dataset=\"valid\"\n",
    "\n",
    "set_seed(seed)\n",
    "use = USE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0130c75f-d6be-4709-8b79-6b0137067a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, config, finetuned_model = init_model(model_target, downstream_task, seed)\n",
    "w2i, i2w = load_word_index(downstream_task)\n",
    "\n",
    "train_dataset, train_loader, train_path = load_dataset_loader(downstream_task, 'train', tokenizer)\n",
    "valid_dataset, valid_loader, valid_path = load_dataset_loader(downstream_task, 'valid', tokenizer)\n",
    "test_dataset, test_loader, test_path = load_dataset_loader(downstream_task, 'test', tokenizer)\n",
    "\n",
    "finetuned_model.to(device)\n",
    "# finetuned_model = fine_tuning_model(model, i2w, train_loader, valid_loader, finetune_epoch)\n",
    "\n",
    "if dataset == \"valid\":\n",
    "    # [380:430]\n",
    "    exp_dataset = valid_dataset.load_dataset(valid_path).head(100)\n",
    "elif dataset == \"train\":\n",
    "    exp_dataset = train_dataset.load_dataset(train_path)\n",
    "# exp_dataset = dd.from_pandas(exp_dataset, npartitions=10)\n",
    "# exp_dataset = te.DataFrame.from_pandas(exp_dataset)\n",
    "text,label = None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ecfde42-e908-43db-8383-ba3106ec701f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 100/100 [03:11<00:00,  1.91s/it]\n"
     ]
    }
   ],
   "source": [
    "if downstream_task == 'sentiment':\n",
    "    text = 'text'\n",
    "    label = 'sentiment'\n",
    "    exp_dataset[['perturbed_text', 'perturbed_semantic_sim', 'pred_label', 'pred_proba', 'perturbed_label', 'perturbed_proba', 'translated_word(s)', 'running_time(s)']] = exp_dataset.progress_apply(\n",
    "        lambda row: attack(\n",
    "            text_ls = row.text,\n",
    "            true_label = row.sentiment,\n",
    "            predictor = finetuned_model,\n",
    "            tokenizer = tokenizer, \n",
    "            att_ratio = perturb_ratio,\n",
    "            perturbation_technique = perturbation_technique,\n",
    "            lang_codemix = perturb_lang,\n",
    "            sim_predictor = use), axis=1, result_type='expand'\n",
    "    )\n",
    "elif downstream_task == 'emotion':\n",
    "    text = 'tweet'\n",
    "    label = 'label'\n",
    "    exp_dataset[['perturbed_text', 'perturbed_semantic_sim', 'pred_label', 'pred_proba', 'perturbed_label', 'perturbed_proba', 'translated_word(s)', 'running_time(s)']] = exp_dataset.progress_apply(\n",
    "        lambda row: attack(\n",
    "            text_ls = row.tweet.values,\n",
    "            true_label = row.label.values,\n",
    "            predictor = finetuned_model,\n",
    "            tokenizer = tokenizer, \n",
    "            att_ratio = perturb_ratio,\n",
    "            perturbation_technique = perturbation_technique,\n",
    "            lang_codemix = perturb_lang,\n",
    "            sim_predictor = use), axis=1, result_type='expand'\n",
    "    )\n",
    "\n",
    "before_attack = accuracy_score(exp_dataset[label], exp_dataset['pred_label'])\n",
    "after_attack = accuracy_score(exp_dataset[label], exp_dataset['perturbed_label'])\n",
    "\n",
    "exp_dataset.loc[exp_dataset.index[0], 'before_attack_acc'] = before_attack\n",
    "exp_dataset.loc[exp_dataset.index[0], 'after_attack_acc'] = after_attack\n",
    "exp_dataset.loc[exp_dataset.index[0], 'avg_semantic_sim'] = exp_dataset[\"perturbed_semantic_sim\"].mean()\n",
    "exp_dataset.loc[exp_dataset.index[0], 'avg_running_time(s)'] = exp_dataset[\"running_time(s)\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe121c30-5649-4277-8805-0a7f3c59d6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>perturbed_text</th>\n",
       "      <th>perturbed_semantic_sim</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>pred_proba</th>\n",
       "      <th>perturbed_label</th>\n",
       "      <th>perturbed_proba</th>\n",
       "      <th>translated_word(s)</th>\n",
       "      <th>running_time(s)</th>\n",
       "      <th>before_attack_acc</th>\n",
       "      <th>after_attack_acc</th>\n",
       "      <th>avg_semantic_sim</th>\n",
       "      <th>avg_running_time(s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meski masa kampanye sudah selesai , bukan bera...</td>\n",
       "      <td>1</td>\n",
       "      <td>meski masa kampanye sudah selesai bukan mean h...</td>\n",
       "      <td>0.753872</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0365, 0.8362, 0.1273]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.4917, 0.4487, 0.0596]</td>\n",
       "      <td>{'upaya': 'effort', 'berati': 'mean', 'mengere...</td>\n",
       "      <td>0.7329</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.851678</td>\n",
       "      <td>1.910111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tidak enak</td>\n",
       "      <td>2</td>\n",
       "      <td>tidak nice</td>\n",
       "      <td>0.540959</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0011, 0.001, 0.9979]</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0019, 0.001, 0.9971]</td>\n",
       "      <td>{'enak': 'nice'}</td>\n",
       "      <td>0.6008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>restoran ini menawarkan makanan sunda . kami m...</td>\n",
       "      <td>0</td>\n",
       "      <td>restaurant ini menawarkan food sunda kami meme...</td>\n",
       "      <td>0.823257</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9976, 0.0016, 0.0008]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9654, 0.0226, 0.0121]</td>\n",
       "      <td>{'enak': 'nice', 'makanan': 'food', 'variatif'...</td>\n",
       "      <td>4.2013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lokasi di alun alun masakan padang ini cukup t...</td>\n",
       "      <td>0</td>\n",
       "      <td>location di alun alun masakan field ini cukup ...</td>\n",
       "      <td>0.862981</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9982, 0.0012, 0.0005]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9933, 0.006, 0.0007]</td>\n",
       "      <td>{'mengenyangkan': 'filling', 'gule': 'gule', '...</td>\n",
       "      <td>2.5275</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>betapa bejad kader gerindra yang anggota dprd ...</td>\n",
       "      <td>2</td>\n",
       "      <td>betapa depraved cadre gerindra yang anggota dp...</td>\n",
       "      <td>0.824325</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0045, 0.0029, 0.9926]</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.1607, 0.1407, 0.6986]</td>\n",
       "      <td>{'bejad': 'depraved', 'partai': 'party', 'menc...</td>\n",
       "      <td>1.3139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kampiun bistro berada di jalan . kebon kawung ...</td>\n",
       "      <td>0</td>\n",
       "      <td>champion bistro berada di jalan kebon kawung a...</td>\n",
       "      <td>0.889336</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9971, 0.0025, 0.0004]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9969, 0.0026, 0.0005]</td>\n",
       "      <td>{'musik': 'music', 'nya': 'his', 'bandung': 'B...</td>\n",
       "      <td>2.3597</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kamar nya sempit tidak ada tempat menyimpan ba...</td>\n",
       "      <td>2</td>\n",
       "      <td>room nya narrow tidak ada tempat menyimpan goo...</td>\n",
       "      <td>0.912295</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0098, 0.0017, 0.9886]</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.1308, 0.007, 0.8622]</td>\n",
       "      <td>{'sempit': 'narrow', 'kamar': 'room', 'uchiwa'...</td>\n",
       "      <td>1.4899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>saya tadi mampir ke restauran ini , mungkin le...</td>\n",
       "      <td>0</td>\n",
       "      <td>saya tadi mampir ke restaurant ini mungkin let...</td>\n",
       "      <td>0.904444</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9983, 0.0013, 0.0004]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9958, 0.003, 0.0012]</td>\n",
       "      <td>{'suka': 'like', 'enak': 'nice', 'super': 'sup...</td>\n",
       "      <td>3.2132</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jangan percaya dengan agama buddha . agama itu...</td>\n",
       "      <td>2</td>\n",
       "      <td>jangan percaya dengan agama buddha agama itu s...</td>\n",
       "      <td>0.939600</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0007, 0.0011, 0.9981]</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0012, 0.0016, 0.9972]</td>\n",
       "      <td>{'sesat': 'perverted', 'tuhan': 'lord'}</td>\n",
       "      <td>0.3933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kafe ini menawarkan sesuatu hal yang sebelumny...</td>\n",
       "      <td>0</td>\n",
       "      <td>kafe ini menawarkan sesuatu hal yang sebelumny...</td>\n",
       "      <td>0.930858</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9965, 0.0024, 0.0011]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9968, 0.0015, 0.0017]</td>\n",
       "      <td>{'lezat': 'delicious', 'goreng': 'fry', 'enak'...</td>\n",
       "      <td>1.4795</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>yusri seorang ibu yang tidak tahu diri . sudah...</td>\n",
       "      <td>2</td>\n",
       "      <td>yusri seorang ibu yang tidak tahu diri sudah t...</td>\n",
       "      <td>0.935228</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0008, 0.0011, 0.9982]</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0091, 0.0475, 0.9433]</td>\n",
       "      <td>{'maling': 'thief', 'dasar': 'base', 'terima':...</td>\n",
       "      <td>1.5323</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>menemukan bocah-bocah sampai yang komentar di ...</td>\n",
       "      <td>2</td>\n",
       "      <td>menemukan sampai yang comment di yutub pakai d...</td>\n",
       "      <td>0.772881</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0007, 0.0036, 0.9957]</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.009, 0.1198, 0.8712]</td>\n",
       "      <td>{'kotor': 'dirty', 'sedih': 'sad', 'komentar':...</td>\n",
       "      <td>1.0951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>indomaret adalah penjahat anyar dalam dunia pe...</td>\n",
       "      <td>2</td>\n",
       "      <td>indomaret adalah penjahat anyar dalam world pe...</td>\n",
       "      <td>0.873512</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0007, 0.0159, 0.9834]</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0032, 0.5816, 0.4151]</td>\n",
       "      <td>{'dunia': 'world'}</td>\n",
       "      <td>0.8387</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>malah .. !!! tren si tukang fitnah bicara iman...</td>\n",
       "      <td>2</td>\n",
       "      <td>malah tren si craftsman slander bicara iman ba...</td>\n",
       "      <td>0.781632</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0008, 0.0013, 0.9979]</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0304, 0.6629, 0.3067]</td>\n",
       "      <td>{'busukkan': 'rot', 'fitnah': 'slander', 'dasa...</td>\n",
       "      <td>2.5046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tahun 2018 , bri ingin akuisisi 2 bank cilik</td>\n",
       "      <td>1</td>\n",
       "      <td>tahun 2018 bri ingin acquisition 2 bank little</td>\n",
       "      <td>0.806597</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0031, 0.9961, 0.0009]</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0064, 0.9928, 0.0009]</td>\n",
       "      <td>{'akuisisi': 'acquisition', 'cilik': 'little'}</td>\n",
       "      <td>0.7864</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pilihan bakso nya sangat beragam dan kelebihan...</td>\n",
       "      <td>0</td>\n",
       "      <td>choice meatball his sangat beragam dan excess ...</td>\n",
       "      <td>0.606217</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9962, 0.0029, 0.0009]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9921, 0.0066, 0.0013]</td>\n",
       "      <td>{'nya': 'his', 'kelebihan': 'excess', 'bakso':...</td>\n",
       "      <td>1.3365</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>dulu mau modalin , sediakan tempat , dan carik...</td>\n",
       "      <td>2</td>\n",
       "      <td>dulu mau modalin sediakan tempat dan swatch pe...</td>\n",
       "      <td>0.875228</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0009, 0.0012, 0.998]</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.003, 0.0029, 0.9941]</td>\n",
       "      <td>{'sindir': 'quip', 'banget': 'very', 'carikan'...</td>\n",
       "      <td>1.7213</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tempa nya sangat memanjakan pengunjung . denga...</td>\n",
       "      <td>0</td>\n",
       "      <td>forging nya sangat pamper pengunjung dengan su...</td>\n",
       "      <td>0.934279</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.997, 0.0011, 0.0019]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9812, 0.002, 0.0169]</td>\n",
       "      <td>{'kesejukan': 'coolness', 'tempa': 'forging', ...</td>\n",
       "      <td>2.5157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>musnahkan lgbt , gay , homo , dan lesbian dari...</td>\n",
       "      <td>2</td>\n",
       "      <td>destroy lgbt gay homo dan lesbian dari advance...</td>\n",
       "      <td>0.875519</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0009, 0.0026, 0.9965]</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0019, 0.0304, 0.9678]</td>\n",
       "      <td>{'punah': 'extinct', 'muka': 'advance', 'gay':...</td>\n",
       "      <td>1.8913</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>banyak pilihan menu khas bakso , bakso rawit d...</td>\n",
       "      <td>0</td>\n",
       "      <td>banyak pilihan menu typical bakso bakso rawit ...</td>\n",
       "      <td>0.824812</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9979, 0.0016, 0.0005]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9967, 0.0025, 0.0007]</td>\n",
       "      <td>{'jely': 'jelly', 'granat': 'grenade', 'suka':...</td>\n",
       "      <td>2.0193</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  sentiment  \\\n",
       "0   meski masa kampanye sudah selesai , bukan bera...          1   \n",
       "1                                          tidak enak          2   \n",
       "2   restoran ini menawarkan makanan sunda . kami m...          0   \n",
       "3   lokasi di alun alun masakan padang ini cukup t...          0   \n",
       "4   betapa bejad kader gerindra yang anggota dprd ...          2   \n",
       "5   kampiun bistro berada di jalan . kebon kawung ...          0   \n",
       "6   kamar nya sempit tidak ada tempat menyimpan ba...          2   \n",
       "7   saya tadi mampir ke restauran ini , mungkin le...          0   \n",
       "8   jangan percaya dengan agama buddha . agama itu...          2   \n",
       "9   kafe ini menawarkan sesuatu hal yang sebelumny...          0   \n",
       "10  yusri seorang ibu yang tidak tahu diri . sudah...          2   \n",
       "11  menemukan bocah-bocah sampai yang komentar di ...          2   \n",
       "12  indomaret adalah penjahat anyar dalam dunia pe...          2   \n",
       "13  malah .. !!! tren si tukang fitnah bicara iman...          2   \n",
       "14       tahun 2018 , bri ingin akuisisi 2 bank cilik          1   \n",
       "15  pilihan bakso nya sangat beragam dan kelebihan...          0   \n",
       "16  dulu mau modalin , sediakan tempat , dan carik...          2   \n",
       "17  tempa nya sangat memanjakan pengunjung . denga...          0   \n",
       "18  musnahkan lgbt , gay , homo , dan lesbian dari...          2   \n",
       "19  banyak pilihan menu khas bakso , bakso rawit d...          0   \n",
       "\n",
       "                                       perturbed_text  perturbed_semantic_sim  \\\n",
       "0   meski masa kampanye sudah selesai bukan mean h...                0.753872   \n",
       "1                                          tidak nice                0.540959   \n",
       "2   restaurant ini menawarkan food sunda kami meme...                0.823257   \n",
       "3   location di alun alun masakan field ini cukup ...                0.862981   \n",
       "4   betapa depraved cadre gerindra yang anggota dp...                0.824325   \n",
       "5   champion bistro berada di jalan kebon kawung a...                0.889336   \n",
       "6   room nya narrow tidak ada tempat menyimpan goo...                0.912295   \n",
       "7   saya tadi mampir ke restaurant ini mungkin let...                0.904444   \n",
       "8   jangan percaya dengan agama buddha agama itu s...                0.939600   \n",
       "9   kafe ini menawarkan sesuatu hal yang sebelumny...                0.930858   \n",
       "10  yusri seorang ibu yang tidak tahu diri sudah t...                0.935228   \n",
       "11  menemukan sampai yang comment di yutub pakai d...                0.772881   \n",
       "12  indomaret adalah penjahat anyar dalam world pe...                0.873512   \n",
       "13  malah tren si craftsman slander bicara iman ba...                0.781632   \n",
       "14     tahun 2018 bri ingin acquisition 2 bank little                0.806597   \n",
       "15  choice meatball his sangat beragam dan excess ...                0.606217   \n",
       "16  dulu mau modalin sediakan tempat dan swatch pe...                0.875228   \n",
       "17  forging nya sangat pamper pengunjung dengan su...                0.934279   \n",
       "18  destroy lgbt gay homo dan lesbian dari advance...                0.875519   \n",
       "19  banyak pilihan menu typical bakso bakso rawit ...                0.824812   \n",
       "\n",
       "    pred_label                pred_proba  perturbed_label  \\\n",
       "0            1  [0.0365, 0.8362, 0.1273]                0   \n",
       "1            2   [0.0011, 0.001, 0.9979]                2   \n",
       "2            0  [0.9976, 0.0016, 0.0008]                0   \n",
       "3            0  [0.9982, 0.0012, 0.0005]                0   \n",
       "4            2  [0.0045, 0.0029, 0.9926]                2   \n",
       "5            0  [0.9971, 0.0025, 0.0004]                0   \n",
       "6            2  [0.0098, 0.0017, 0.9886]                2   \n",
       "7            0  [0.9983, 0.0013, 0.0004]                0   \n",
       "8            2  [0.0007, 0.0011, 0.9981]                2   \n",
       "9            0  [0.9965, 0.0024, 0.0011]                0   \n",
       "10           2  [0.0008, 0.0011, 0.9982]                2   \n",
       "11           2  [0.0007, 0.0036, 0.9957]                2   \n",
       "12           2  [0.0007, 0.0159, 0.9834]                1   \n",
       "13           2  [0.0008, 0.0013, 0.9979]                1   \n",
       "14           1  [0.0031, 0.9961, 0.0009]                1   \n",
       "15           0  [0.9962, 0.0029, 0.0009]                0   \n",
       "16           2   [0.0009, 0.0012, 0.998]                2   \n",
       "17           0   [0.997, 0.0011, 0.0019]                0   \n",
       "18           2  [0.0009, 0.0026, 0.9965]                2   \n",
       "19           0  [0.9979, 0.0016, 0.0005]                0   \n",
       "\n",
       "             perturbed_proba  \\\n",
       "0   [0.4917, 0.4487, 0.0596]   \n",
       "1    [0.0019, 0.001, 0.9971]   \n",
       "2   [0.9654, 0.0226, 0.0121]   \n",
       "3    [0.9933, 0.006, 0.0007]   \n",
       "4   [0.1607, 0.1407, 0.6986]   \n",
       "5   [0.9969, 0.0026, 0.0005]   \n",
       "6    [0.1308, 0.007, 0.8622]   \n",
       "7    [0.9958, 0.003, 0.0012]   \n",
       "8   [0.0012, 0.0016, 0.9972]   \n",
       "9   [0.9968, 0.0015, 0.0017]   \n",
       "10  [0.0091, 0.0475, 0.9433]   \n",
       "11   [0.009, 0.1198, 0.8712]   \n",
       "12  [0.0032, 0.5816, 0.4151]   \n",
       "13  [0.0304, 0.6629, 0.3067]   \n",
       "14  [0.0064, 0.9928, 0.0009]   \n",
       "15  [0.9921, 0.0066, 0.0013]   \n",
       "16   [0.003, 0.0029, 0.9941]   \n",
       "17   [0.9812, 0.002, 0.0169]   \n",
       "18  [0.0019, 0.0304, 0.9678]   \n",
       "19  [0.9967, 0.0025, 0.0007]   \n",
       "\n",
       "                                   translated_word(s)  running_time(s)  \\\n",
       "0   {'upaya': 'effort', 'berati': 'mean', 'mengere...           0.7329   \n",
       "1                                    {'enak': 'nice'}           0.6008   \n",
       "2   {'enak': 'nice', 'makanan': 'food', 'variatif'...           4.2013   \n",
       "3   {'mengenyangkan': 'filling', 'gule': 'gule', '...           2.5275   \n",
       "4   {'bejad': 'depraved', 'partai': 'party', 'menc...           1.3139   \n",
       "5   {'musik': 'music', 'nya': 'his', 'bandung': 'B...           2.3597   \n",
       "6   {'sempit': 'narrow', 'kamar': 'room', 'uchiwa'...           1.4899   \n",
       "7   {'suka': 'like', 'enak': 'nice', 'super': 'sup...           3.2132   \n",
       "8             {'sesat': 'perverted', 'tuhan': 'lord'}           0.3933   \n",
       "9   {'lezat': 'delicious', 'goreng': 'fry', 'enak'...           1.4795   \n",
       "10  {'maling': 'thief', 'dasar': 'base', 'terima':...           1.5323   \n",
       "11  {'kotor': 'dirty', 'sedih': 'sad', 'komentar':...           1.0951   \n",
       "12                                 {'dunia': 'world'}           0.8387   \n",
       "13  {'busukkan': 'rot', 'fitnah': 'slander', 'dasa...           2.5046   \n",
       "14     {'akuisisi': 'acquisition', 'cilik': 'little'}           0.7864   \n",
       "15  {'nya': 'his', 'kelebihan': 'excess', 'bakso':...           1.3365   \n",
       "16  {'sindir': 'quip', 'banget': 'very', 'carikan'...           1.7213   \n",
       "17  {'kesejukan': 'coolness', 'tempa': 'forging', ...           2.5157   \n",
       "18  {'punah': 'extinct', 'muka': 'advance', 'gay':...           1.8913   \n",
       "19  {'jely': 'jelly', 'granat': 'grenade', 'suka':...           2.0193   \n",
       "\n",
       "    before_attack_acc  after_attack_acc  avg_semantic_sim  avg_running_time(s)  \n",
       "0                0.96              0.87          0.851678             1.910111  \n",
       "1                 NaN               NaN               NaN                  NaN  \n",
       "2                 NaN               NaN               NaN                  NaN  \n",
       "3                 NaN               NaN               NaN                  NaN  \n",
       "4                 NaN               NaN               NaN                  NaN  \n",
       "5                 NaN               NaN               NaN                  NaN  \n",
       "6                 NaN               NaN               NaN                  NaN  \n",
       "7                 NaN               NaN               NaN                  NaN  \n",
       "8                 NaN               NaN               NaN                  NaN  \n",
       "9                 NaN               NaN               NaN                  NaN  \n",
       "10                NaN               NaN               NaN                  NaN  \n",
       "11                NaN               NaN               NaN                  NaN  \n",
       "12                NaN               NaN               NaN                  NaN  \n",
       "13                NaN               NaN               NaN                  NaN  \n",
       "14                NaN               NaN               NaN                  NaN  \n",
       "15                NaN               NaN               NaN                  NaN  \n",
       "16                NaN               NaN               NaN                  NaN  \n",
       "17                NaN               NaN               NaN                  NaN  \n",
       "18                NaN               NaN               NaN                  NaN  \n",
       "19                NaN               NaN               NaN                  NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dataset.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83b16bb-b416-4519-909b-ad6211215ee6",
   "metadata": {},
   "source": [
    "### After Logit Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "913a33b3-e1fa-401e-9e3e-20eec418fa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| original_text: 'lucu'\n",
      "ic| end_time_logit: 0.0119\n",
      "ic| end_time_arrops: 0.0011\n",
      "ic| running_time_a: 0.1544\n",
      "ic| top_words_perturb: [(0, 'lucu', 0.6613012860584355)]\n",
      "ic| end_time_pert: 0.7008\n",
      "ic| end_time_use: 0.0055\n",
      "ic| running_time: 0.9818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('funny',\n",
       " 0.2863596,\n",
       " 0,\n",
       " array([0.9945, 0.0015, 0.004 ]),\n",
       " 0,\n",
       " array([0.9941, 0.0045, 0.0014]),\n",
       " {'lucu': 'funny'},\n",
       " 0.9818)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack(\n",
    "    text_ls = exp_dataset.iloc[[7]].text.item(),\n",
    "    true_label = exp_dataset.iloc[[7]].sentiment.item(),\n",
    "    predictor = finetuned_model,\n",
    "    tokenizer = tokenizer, \n",
    "    att_ratio = perturb_ratio,\n",
    "    perturbation_technique = perturbation_technique,\n",
    "    lang_codemix = perturb_lang,\n",
    "    sim_predictor = use\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdf9577-ef43-407b-b707-252c0ea1853f",
   "metadata": {},
   "source": [
    "### Coba optimize di USE nya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0752ac79-d4fc-4f0c-be49-d1117ac29329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('meski masa kampanye sudah selesai bukan mean habis pula effort hoist tingkat kedipilihan elektabilitas',\n",
       " 0.7538721,\n",
       " 1,\n",
       " array([0.0365, 0.8362, 0.1273]),\n",
       " 0,\n",
       " array([0.4917, 0.4487, 0.0596]),\n",
       " {'upaya': 'effort', 'berati': 'mean', 'mengerek': 'hoist'},\n",
       " 1.0341)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack(\n",
    "    text_ls = exp_dataset.iloc[[0]].text.item(),\n",
    "    true_label = exp_dataset.iloc[[0]].sentiment.item(),\n",
    "    predictor = finetuned_model,\n",
    "    tokenizer = tokenizer, \n",
    "    att_ratio = perturb_ratio,\n",
    "    perturbation_technique = perturbation_technique,\n",
    "    lang_codemix = perturb_lang,\n",
    "    sim_predictor = use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e374eee1-0149-4793-977b-918220c1b953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " tensor([0.0365, 0.8362, 0.1273], device='cuda:0', grad_fn=<SqueezeBackward0>),\n",
       " array(0.8362002, dtype=float32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logit_prob(text_ls, predictor, tokenizer):\\\n",
    "    subwords = tokenizer.encode(text_ls)\n",
    "    subwords = torch.LongTensor(subwords).view(1, -1).to(predictor.device)\n",
    "\n",
    "    logits = predictor(subwords)[0]\n",
    "    orig_label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "    \n",
    "    orig_probs = F.softmax(logits, dim=-1).squeeze()\n",
    "    orig_prob = F.softmax(logits, dim=-1).squeeze()[orig_label].detach().cpu().numpy()\n",
    "    \n",
    "    return orig_label, orig_probs, orig_prob\n",
    "\n",
    "t = 'meski masa kampanye sudah selesai , bukan berati habis pula upaya mengerek tingkat kedipilihan elektabilitas .'\n",
    "logit_prob(t, finetuned_model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83533730-b3c7-471a-a06f-6233e86e8d2e",
   "metadata": {},
   "source": [
    "### Coba Optimize step 2+codemix perturbation pake batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1475f7-9a37-4239-90ac-47db1fa26e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack(\n",
    "    text_ls = exp_dataset.iloc[[48]].text.item(),\n",
    "    true_label = 0,\n",
    "    predictor = finetuned_model,\n",
    "    tokenizer = tokenizer, \n",
    "    att_ratio = perturb_ratio,\n",
    "    perturbation_technique = perturbation_technique,\n",
    "    lang_codemix = perturb_lang,\n",
    "    sim_predictor = use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "d0f70e96-30c9-4fd1-b2ae-1d3c61bdd7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dataset.to_csv('a.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4100b193-fdc3-4d59-bc11-ab02fa4611f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def codemix_perturbation(words, target_lang, words_perturb):\n",
    "    \"\"\"\n",
    "    'su': 'sundanese'\n",
    "    'jw': 'javanese'\n",
    "    'ms': 'malay'\n",
    "    'en': 'english'\n",
    "    \"\"\"\n",
    "    translator = GoogleTranslator(source=\"id\", target=target_lang)\n",
    "    new_wp = []\n",
    "    for wp in words_perturb:\n",
    "        if wp[1].isalpha():\n",
    "            new_wp.append(wp[1])\n",
    "    \n",
    "    if len(new_wp) == 1:\n",
    "        new_wp_trans = dict(zip(new_wp, translator.translate(new_wp[0])))\n",
    "    elif len(new_wp) == 0:\n",
    "        return ' '.join(new_words), {}\n",
    "        \n",
    "    new_wp_trans = dict(zip(new_wp, translator.translate_batch(new_wp)))\n",
    "    \n",
    "    supported_langs = [\"su\", \"jw\", \"ms\", \"en\", \"fr\", \"it\"]\n",
    "    \n",
    "    if target_lang not in supported_langs:\n",
    "        raise ValueError('Language Unavailable')\n",
    "    \n",
    "    new_words = words.copy()\n",
    "    \n",
    "    if len(words_perturb) >= 1:\n",
    "        for perturb_word in new_wp_trans.keys():\n",
    "            new_words = [new_wp_trans[perturb_word] if word == perturb_word and word.isalpha() else word for word in new_words]\n",
    "\n",
    "    return ' '.join(new_words), new_wp_trans\n",
    "\n",
    "def synonym_replacement(words, words_perturb):    \n",
    "    # new_words = words.copy()\n",
    "    new_wp = []\n",
    "    for wp in words_perturb:\n",
    "        if wp[1].isalpha():\n",
    "            new_wp.append(wp[1])\n",
    "    \n",
    "    if len(new_wp) == 1:\n",
    "        new_wp_trans = dict(zip(new_wp, get_synonyms(new_wp[0])[0]))\n",
    "    elif len(new_wp) == 0:\n",
    "        return ' '.join(new_words), {}\n",
    "    \n",
    "    wp_trans=[]\n",
    "    for wp in new_wp:\n",
    "        wp_trans.append(get_synonyms(wp)[0])\n",
    "        print(wp,wp_trans)\n",
    "    \n",
    "    new_wp_trans = dict(zip(new_wp, wp_trans))\n",
    "    \n",
    "    new_words = words.copy()\n",
    "    \n",
    "    if len(words_perturb) >= 1:\n",
    "        for perturb_word in new_wp_trans.keys():\n",
    "            new_words = [new_wp_trans[perturb_word[1]] if word == perturb_word[1] and word.isalpha() else word for word in new_words]\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "    \n",
    "    return sentence, new_wp_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddac99a0-2976-48c0-8c91-20e9c86de269",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'buat kalian yang cuma tahu tempat makan malam romantis yang ada di dago , boleh dicoba deh makan di burgundy sini , di sini sih menurut saya tempat yang enak banget buat ngadem selain tempat nya yang hening kita juga bisa melihat rerumputan yang hijau meski cuma di taman nya , hehehe . tapi balik lagi buat saya sih makanan , haha . tidak usah diragukan .'\n",
    "words = word.split()\n",
    "wp = [(59, 'diragukan', 0.001480978060399707), (27, 'enak', 0.0006103660272316347), (18, 'burgundy', 0.0005351185233486433), (55, 'makanan', 0.000532135112141674), (12, 'dago', 0.0004499789485095107), (8, 'romantis', 0.0003965279976370084), (46, 'taman', 0.00039581217539819136), (40, 'rerumputan', 0.00039318750260974866), (7, 'malam', 0.000384060906586825)]\n",
    "target_lang = \"en\"\n",
    "\n",
    "start_time = time.time()\n",
    "print(synonym_replacement(words, wp))\n",
    "print(round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92985e6-016d-4609-bb85-70034bbeb2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbert-sentiment-codemixing-jw-adv-0.6-valid\n",
    "# 279/1260\n",
    "850/1260\n",
    "new_wp_trans = dict(zip(new_wp, translator.translate(new_wp[0])))\n",
    "IndexError: list index out of range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00c2493-4560-4d0a-b00a-ac4ec680cb3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c95ec8-1c18-42da-8abc-8bc0bdab7b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e3f4a9-9263-40c8-bc14-523ab596c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack(\n",
    "    text_ls = exp_dataset.iloc[[5]].text.item(),\n",
    "    true_label = exp_dataset.iloc[[5]].sentiment.item(),\n",
    "    predictor = finetuned_model,\n",
    "    tokenizer = tokenizer, \n",
    "    att_ratio = 0.6,\n",
    "    perturbation_technique = perturbation_technique,\n",
    "    lang_codemix = \"jw\",\n",
    "    sim_predictor = use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8d3139-25ad-4e6e-b504-d88b2a388cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24548f59-5e35-4aad-9b84-0ae0a81c5d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9583c461-28d2-4c46-b2ed-0dc9f27a2479",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dataset.iloc[[7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aacafc0-3376-4cd2-884b-dc98c7967b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01636e7-36cb-4cf4-aee6-da4316774ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs:\", len(physical_devices))\n",
    "# !pip list cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55317fa7-d77c-40a1-9ecd-a145385008e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['diragukan', 'enak', 'burgundy', 'makanan', 'dago', 'romantis', 'taman', 'rerumputan', 'malam']\n",
    "\n",
    "start_time = time.time()\n",
    "translated_1 = []\n",
    "translator = GoogleTranslator(source=\"id\", target=\"en\")\n",
    "for txt in texts:\n",
    "    translated_1.append(translator.translate(txt))\n",
    "print(round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd92533-750e-4274-a722-04d0f0c89141",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "translated_2 = translator.translate_batch(texts)\n",
    "print(round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469821c-a347-4a04-88df-c9fb8ad61798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !. myenv/bin/activate\n",
    "# !pip uninstall tensorflow -y\n",
    "# !pip uninstall tensorflow-gpu -y\n",
    "\n",
    "!pip install tensorflow --ignore-installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "318575e4-3cd4-4076-a50c-3f83f6badc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.17.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "442ae727-69da-4521-8280-fea661f194a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_prob(text_ls, predictor, tokenizer):\n",
    "    subwords = tokenizer.encode(text_ls)\n",
    "    subwords = torch.LongTensor(subwords).view(1, -1).to(predictor.device)\n",
    "\n",
    "    logits = predictor(subwords)[0]\n",
    "    orig_label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "    \n",
    "    orig_probs = F.softmax(logits, dim=-1).squeeze()\n",
    "    orig_prob = F.softmax(logits, dim=-1).squeeze()[orig_label].detach().cpu().numpy()\n",
    "    \n",
    "    return orig_label, orig_probs, orig_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad62f6-736d-4323-a8c7-7b011b8004e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64\n",
    "!echo $LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ccd6a637-af2c-4af6-ac25-53081b0922d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_prob(text_ls, predictor, tokenizer):\n",
    "    with torch.inference_mode():\n",
    "        subwords = np.array(tokenizer(text_ls)['input_ids'])\n",
    "        # ic(subwords)\n",
    "        if subwords.ndim <= 1:\n",
    "            subwords = torch.tensor(subwords).view(1, -1).to(predictor.device)\n",
    "            logits = predictor(subwords)[0]\n",
    "            orig_label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "            orig_probs = F.softmax(logits, dim=-1).squeeze()\n",
    "            orig_prob = F.softmax(logits, dim=-1).squeeze()[orig_label].detach().cpu().numpy()\n",
    "        else:\n",
    "            subwords = torch.tensor(subwords).to(predictor.device)\n",
    "            logits = finetuned_model(subwords)[0]\n",
    "            orig_label = [t.squeeze().item() for t in torch.topk(logits, k=1, dim=-1)[1]]\n",
    "            orig_probs = F.softmax(logits, dim=-1).squeeze()\n",
    "            orig_prob = [F.softmax(logits, dim=-1).squeeze()[idx][label].detach().cpu().numpy() for idx, label in enumerate(orig_label)]\n",
    "    \n",
    "    return orig_label, orig_probs, orig_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b2ddd7-1a1c-44b5-9637-e04dab2b3855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,    4, 4700,  500, 5622, 7884,    3],\n",
       "        [   2,  422,    4,  500, 5622, 7884,    3],\n",
       "        [   2,  422, 4700,    4, 5622, 7884,    3],\n",
       "        [   2,  422, 4700,  500,    4, 7884,    3],\n",
       "        [   2,  422, 4700,  500, 5622,    4,    3]], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subwords = np.array([\n",
    "    [2, 4, 4700, 500, 5622, 7884, 3],\n",
    "    [2, 422, 4, 500, 5622, 7884, 3],\n",
    "    [2, 422, 4700, 4, 5622, 7884, 3],\n",
    "    [2, 422, 4700, 500, 4, 7884, 3],\n",
    "    [2, 422, 4700, 500, 5622, 4, 3]\n",
    "])\n",
    "subwords = torch.tensor(subwords).to(finetuned_model.device)\n",
    "subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5647c3cf-f1dc-4535-850a-13463bbc3232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6abd9cb2-0c47-44d7-9755-dc348596bae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "subwords = np.array([2, 4, 4700, 500, 5622, 7884, 3])\n",
    "print(subwords.ndim)\n",
    "subwords = torch.tensor(subwords).view(1,-1).to(finetuned_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7780051c-29bc-4ed4-8032-7e5194c1d357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,    4, 4700,  500, 5622, 7884,    3]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "b9274972-4965-471a-a896-e8d547200dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ls = 'pemancing nanti kasus nya kek anggota fpi ikut membongkar obattan malah ditangkap'.split(' ')\n",
    "# text_ls = 'mau marah sama koneksi indosat'.split(' ')\n",
    "text_ls = 'lucu'.split(' ')\n",
    "len_text = len(text_ls)\n",
    "leave_1_texts = [' '.join(text_ls[:ii] + [tokenizer.mask_token] + text_ls[min(ii + 1, len_text):]) for ii in range(len_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "75caddfd-58f4-4de9-af86-34f312a51df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[MASK]']"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leave_1_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "6b0de845-7c22-4276-a5c2-7850b131fd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| end_time_b: 0.0258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0258"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time_b = time.time()\n",
    "# ic(tokenizer(leave_1_texts)['input_ids'])\n",
    "subwords = tokenizer.batch_encode_plus(leave_1_texts, return_tensors=\"pt\", padding=True)['input_ids'].to(finetuned_model.device)\n",
    "# ic(subwords.shape)\n",
    "# subwords = np.array([\n",
    "#     [2, 4, 4700, 500, 5622, 7884, 3],\n",
    "#     [2, 422, 4, 500, 5622, 7884, 3],\n",
    "#     [2, 422, 4700, 4, 5622, 7884, 3],\n",
    "#     [2, 422, 4700, 500, 4, 7884, 3],\n",
    "#     [2, 422, 4700, 500, 5622, 4, 3]\n",
    "# ])\n",
    "# subwords = torch.tensor(subwords).to(finetuned_model.device)\n",
    "# ic(subwords)\n",
    "logits = finetuned_model(subwords).logits\n",
    "# ic(logits)\n",
    "# log = finetuned_model(subwords)\n",
    "# ic(log)\n",
    "orig_label = [t.squeeze().item() for t in torch.topk(logits, k=1, dim=-1)[1]]\n",
    "# ic(orig_label)\n",
    "orig_probs = F.softmax(logits, dim=-1).squeeze()\n",
    "# ic(orig_probs)\n",
    "orig_prob = [F.softmax(logits, dim=-1).squeeze()[idx][label].detach().cpu().numpy() for idx, label in enumerate(orig_label)]\n",
    "# ic(orig_prob)\n",
    "end_time_b = round(time.time() - start_time_b, 4)\n",
    "ic(end_time_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "1bb4218f-3d16-4177-bc13-077e3a42c4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(text_leave_1): 6\n",
      "ic| text_leave_1: '[MASK]'\n",
      "ic| len(subwords[0]): 3\n",
      "ic| end_time_b: 0.0698\n",
      "ic| orig_probs_leave_1_ext: [array([0.5398942 , 0.04156226, 0.41854346], dtype=float32)]\n",
      "ic| orig_label_leave_1_ext: [0]\n",
      "ic| orig_prob_leave_1_ext: [array(0.5398942, dtype=float32)]\n",
      "ic| subwords_ls: [tensor([[2, 4, 3]], device='cuda:0')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[2, 4, 3]], device='cuda:0')]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logit_prob(text_ls, predictor, tokenizer):\n",
    "    # original_text = text_ls\n",
    "    # print(text_ls)\n",
    "    subwords = tokenizer.encode(text_ls)\n",
    "    subwords = torch.LongTensor(subwords).view(1, -1).to(predictor.device)\n",
    "    logits = predictor(subwords)[0]\n",
    "    orig_label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "    \n",
    "    orig_probs = F.softmax(logits, dim=-1).squeeze()\n",
    "    orig_prob = F.softmax(logits, dim=-1).squeeze()[orig_label].detach().cpu().numpy()\n",
    "    \n",
    "    return orig_label, orig_probs, orig_prob, subwords\n",
    "\n",
    "start_time_b = time.time()\n",
    "\n",
    "orig_probs_leave_1_ext=[]\n",
    "orig_prob_leave_1_ext=[]\n",
    "orig_label_leave_1_ext=[]\n",
    "subwords_ls = []\n",
    "for text_leave_1 in leave_1_texts:\n",
    "    orig_label_leave_1, orig_probs_leave_1, orig_prob_leave_1,subwords = logit_prob(text_leave_1, finetuned_model, tokenizer)\n",
    "    orig_probs_leave_1_ext.append(orig_probs_leave_1.detach().cpu().numpy())\n",
    "    orig_prob_leave_1_ext.append(orig_prob_leave_1)\n",
    "    orig_label_leave_1_ext.append(orig_label_leave_1)\n",
    "    ic(len(text_leave_1))\n",
    "    ic(text_leave_1)\n",
    "    ic(len(subwords[0]))\n",
    "    subwords_ls.append(subwords)\n",
    "\n",
    "end_time_b = round(time.time() - start_time_b, 4)\n",
    "ic(end_time_b)\n",
    "\n",
    "ic(orig_probs_leave_1_ext)\n",
    "ic(orig_label_leave_1_ext)\n",
    "ic(orig_prob_leave_1_ext)\n",
    "ic(subwords_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca52ab1d-96c8-4039-a654-b3017c86f7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig_label\n",
    "# orig_probs = F.softmax(logits, dim=-1).squeeze()\n",
    "# orig_probs\n",
    "for idx, label in enumerate(orig_label):\n",
    "    print(label)\n",
    "    print(F.softmax(logits, dim=-1).squeeze()[idx][label].detach().cpu().numpy())\n",
    "orig_prob = [F.softmax(logits, dim=-1).squeeze()[idx][label].detach().cpu().numpy() for idx, label in enumerate(orig_label)]\n",
    "# orig_prob = F.softmax(logits, dim=-1).squeeze()\n",
    "print(orig_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "facf9973-edbb-43a8-b2c2-98b3bad6faae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mau', 'marah', 'sama', 'koneksi', 'indosat']"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 'mau marah sama koneksi indosat'\n",
    "t.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e652b7ce-a777-4386-b8f6-1afbb740cb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(text_ls) <= 1:\n",
    "        if perturbation_technique == \"codemixing\":\n",
    "            top_words_perturb = (0, text_ls, 1.000)\n",
    "            perturbed_text,translated_words = codemix_perturbation(text_ls, lang_codemix, top_words_perturb)\n",
    "        elif perturbation_technique == \"synonym_replacement\":\n",
    "            perturbed_text,translated_words = synonym_replacement(text_ls, top_words_perturb)\n",
    "            \n",
    "        perturb_sim_score = sim_predictor.semantic_sim(original_text, perturbed_text)\n",
    "        \n",
    "        if perturb_sim_score < sim_score_threshold:\n",
    "            running_time = round(time.time() - start_time, 2)\n",
    "            orig_probs = np.round(orig_probs.detach().cpu().numpy().tolist(),4)\n",
    "            \n",
    "            running_time = round(time.time() - start_time, 2)\n",
    "            return original_text, 1.000, orig_label, orig_probs, orig_label, orig_probs,'', running_time\n",
    "        else:\n",
    "            running_time = round(time.time() - start_time, 2)\n",
    "            perturbed_label, perturbed_probs, perturbed_prob = logit_prob(text_ls, predictor, tokenizer)\n",
    "            orig_probs = np.round(orig_probs.detach().cpu().numpy().tolist(),4)\n",
    "            \n",
    "            running_time = round(time.time() - start_time, 2)\n",
    "            return perturbed_text, perturbed_semantic_sim, orig_label, orig_probs, perturbed_label, perturbed_probs, translated_words, running_time\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
