{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from icecream import ic\n",
    "import numpy as np\n",
    "import math\n",
    "import ast\n",
    "import itertools\n",
    "import random\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def wc():\n",
    "\n",
    "\n",
    "\n",
    "# seeds = [42, 26092020, 24032022]    \n",
    "path_test = str(os.getcwd()) + \"/result/seed\" + str(42) + \"/\" + \"test\" + \"/\"\n",
    "\n",
    "dir_list_test = [f for f in os.listdir(path_test)]\n",
    "\n",
    "# for f in dir_list_test:\n",
    "#     if \"su\" in f or \"en\" in f or \"jw\" in f:\n",
    "#         print(f)\n",
    "\n",
    "# dir_list_test = [\"indobert-emotion-codemixing-en-adv-0.2-test.csv\"]\n",
    "selected_dir_list = []\n",
    "random.seed(42)\n",
    "for file in dir_list_test:\n",
    "    if \".csv\" in file:\n",
    "        df_first_row = pd.read_csv(path_test+file).iloc[0]\n",
    "        parsed_filename = file.split(\"-\")\n",
    "        # print(parsed_filename)\n",
    "        if parsed_filename[3] in ([\"en\", \"su\", \"jw\"]) and parsed_filename[5] in ['0.2','0.4']:\n",
    "            selected_dir_list.append(\"-\".join(parsed_filename))\n",
    "            # print(\"-\".join(parsed_filename[:5]))\n",
    "            # print(\"a\")\n",
    "            # print(parsed_filename)\n",
    "            orig_filename = \"-\".join(parsed_filename)\n",
    "            # print(orig_filename)\n",
    "            path = str(os.getcwd()) + \"/result/seed\" + str(42) + \"/\" + \"test\" + \"/\"+orig_filename\n",
    "\n",
    "            df = pd.read_csv(path)\n",
    "            df = df[(df[\"perturbed_semantic_sim\"] <= 0.985) & (df[\"perturbed_semantic_sim\"] >= 0.5) & (df[\"translated_word(s)\"].str.len() > 2)]\n",
    "\n",
    "            df = df[['perturbed_text']]\n",
    "            # if parsed_filename[1] == \"emotion\":\n",
    "            # else:\n",
    "            #     df = df[['perturbed_text']]\n",
    "            \n",
    "            header_list = ['perturbed_text', 'rating']\n",
    "            df = df.reindex(columns = header_list)                \n",
    "            # print(len(df))\n",
    "            idx_sample = sorted(random.sample(range(1, len(df)), 10))\n",
    "            # print(idx_sample)\n",
    "            df = df.iloc[idx_sample]\n",
    "            df.to_csv(str(os.getcwd()) + \"/qualitative/\" + orig_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_q = str(os.getcwd()) + \"/qualitative/\"\n",
    "\n",
    "dir_list_q = [f for f in os.listdir(path_q)]\n",
    "\n",
    "# from itertools import groupby\n",
    "\n",
    "# res = [list(v) for l,v in groupby(sorted(dir_list_q, key=lambda x:x[0]), lambda x: x[0])]\n",
    "# res\n",
    "# for file in dir_list_q:\n",
    "#     print(file)\n",
    "\n",
    "# selected_dir_list\n",
    "it = iter(dir_list_q)\n",
    "tuples_q = []\n",
    "for file in it:\n",
    "    # parsed_filename = file.split(\"-\")\n",
    "    tuples_q.append((file, next(it)))\n",
    "    # print(parsed_filename)\n",
    "    # if parsed_filename\n",
    "\n",
    "\n",
    "\n",
    "#     if \n",
    "#     # parsed_filename_1 = file.split(\"-\")\n",
    "#     # print(file, next(it))\n",
    "# #     print()\n",
    "tuples_q\n",
    "for tup in tuples_q:\n",
    "    path0 = str(os.getcwd()) + \"/qualitative/\"+tup[0]\n",
    "    path1 = str(os.getcwd()) + \"/qualitative/\"+tup[1]\n",
    "\n",
    "\n",
    "    parsed_filename0 = tup[0].split(\"-\")\n",
    "    parsed_filename1 = tup[1].split(\"-\")\n",
    "    # print(str(\"-\".join(parsed_filename0[:4]))+\"-\"+str(parsed_filename[5])+\"-\"+str(parsed_filename1[5]))\n",
    "    # print(parsed_filename1[5])\n",
    "\n",
    "    df0 = pd.read_csv(path0)\n",
    "    # display(df0)\n",
    "    df1 = pd.read_csv(path1)\n",
    "    # display(df1)\n",
    "    df_comb = pd.concat([df0, df1], axis=0)\n",
    "    # display(df_comb)\n",
    "    # print((str(os.getcwd()) + \"/qualitative_comb/\" + orig_filename, index=False)\n",
    "    df_comb.to_csv(str(os.getcwd()) + \"/qualitative_comb/\" + str(\"-\".join(parsed_filename0[:4]))+\"-\"+str(parsed_filename0[5])+\"-\"+str(parsed_filename1[5]) + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_r = str(os.getcwd()) + \"/qualitative_comb/\"\n",
    "\n",
    "dir_list_em = [f for f in os.listdir(path_r) if \"emotion\" in f]\n",
    "dir_list_sent = [f for f in os.listdir(path_r) if \"sentiment\" in f]\n",
    "# print(dir_list_em)\n",
    "# print(dir_list_sent)\n",
    "\n",
    "all_tuples = list(zip(dir_list_sent, dir_list_em))\n",
    "# all_tuples\n",
    "\n",
    "\n",
    "for tup in all_tuples:\n",
    "    path0 = str(os.getcwd()) + \"/qualitative_comb/\"+tup[0]\n",
    "    path1 = str(os.getcwd()) + \"/qualitative_comb/\"+tup[1]\n",
    "\n",
    "\n",
    "    parsed_filename0 = tup[0].split(\"-\")\n",
    "    parsed_filename1 = tup[1].split(\"-\")\n",
    "    # print(str(\"-\".join(parsed_filename0[:4]))+\"-\"+str(parsed_filename[5])+\"-\"+str(parsed_filename1[5]))\n",
    "    # print(parsed_filename0)\n",
    "\n",
    "    df0 = pd.read_csv(path0)\n",
    "    # display(df0)\n",
    "    df1 = pd.read_csv(path1)\n",
    "    # display(df1)\n",
    "    df_comb = pd.concat([df0, df1], axis=0)\n",
    "    # display(df_comb)\n",
    "    # print((str(os.getcwd()) + \"/qualitative_comb/\" + orig_filename, index=False)\n",
    "    # print(str(\"-\".join(parsed_filename0[:4])) + \".csv\")\n",
    "    df_comb.to_csv(str(os.getcwd()) + \"/qualitative_all/\" + str(\"-\".join(parsed_filename0[:4])) + \".csv\", index=False)\n",
    "    # df_comb.to_csv(str(os.getcwd()) + \"/qualitative_all/\" + str(\"-\".join(parsed_filename0[:4]))+\"-\"+str(parsed_filename0[5])+\"-\"+str(parsed_filename1[5]) + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n"
     ]
    }
   ],
   "source": [
    "filename = \"indobert-emotion-codemixing-en-adv-0.2-test.csv\"\n",
    "path = str(os.getcwd()) + \"/result/seed\" + str(42) + \"/\" + \"test\" + \"/\"+filename\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "# df = df.rename({'tweet': 'text', 'b': 'Y'}, axis=1)\n",
    "df = df[['tweet', 'perturbed_text', \"perturbed_semantic_sim\", \"translated_word(s)\"]]\n",
    "# df\n",
    "df = df[(df[\"perturbed_semantic_sim\"] < 0.95) & (df[\"translated_word(s)\"].str.len() > 2)]\n",
    "print(len(df))\n",
    "\n",
    "df.to_csv(\"a.csv\",index=False)\n",
    "# print(len(ast.literal_eval(df[\"translated_word(s)\"].values[0])))\n",
    "# df[\"translated_word(s)\"].values[0]\n",
    "# df.sample(50)\n",
    "# print(len(df))\n",
    "# len({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random.seed(42)\n",
    "header_list = ['tweet', 'perturbed_text', \"perturbed_semantic_sim\", \"translated_word(s)\", \"Samca\", \"Genta\", \"Farid\"]\n",
    "# df.columns[0] = \"text\"\n",
    "idx_sample = sorted(random.sample(range(1, len(df)), 50))\n",
    "# while(fin=False):\n",
    "df.iloc[idx_sample]\n",
    "\n",
    "df = df.reindex(columns = header_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 13,\n",
       " 14,\n",
       " 16,\n",
       " 17,\n",
       " 45,\n",
       " 48,\n",
       " 50,\n",
       " 53,\n",
       " 58,\n",
       " 72,\n",
       " 80,\n",
       " 82,\n",
       " 102,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 115,\n",
       " 120,\n",
       " 126,\n",
       " 141,\n",
       " 143,\n",
       " 173,\n",
       " 175,\n",
       " 177,\n",
       " 184,\n",
       " 195,\n",
       " 215,\n",
       " 217,\n",
       " 230,\n",
       " 259,\n",
       " 280,\n",
       " 288,\n",
       " 302,\n",
       " 303,\n",
       " 309,\n",
       " 310,\n",
       " 328,\n",
       " 333,\n",
       " 347,\n",
       " 358,\n",
       " 360,\n",
       " 367,\n",
       " 378,\n",
       " 380,\n",
       " 389,\n",
       " 391,\n",
       " 413,\n",
       " 415,\n",
       " 434]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "idx_sample = sorted(random.sample(range(1, len(df)), 50))\n",
    "idx_sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc07d24e2f18896857f0b2a651fe84ba40ce7b297e58d8804a308c8039f752a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
