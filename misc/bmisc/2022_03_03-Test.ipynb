{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4942f81c-bf15-4b65-9e36-1edd8f14182d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/m13518040/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import gc\n",
    "import time\n",
    "\n",
    "import torch\n",
    "gc.collect()\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"4\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"  \n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "torch.cuda.set_device(device)\n",
    "# print(device)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import swifter\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "from utils.utils_init_dataset import set_seed, load_dataset_loader\n",
    "from utils.utils_semantic_use import USE\n",
    "from utils.utils_data_utils import DocumentSentimentDataset, DocumentSentimentDataLoader, EmotionDetectionDataset, EmotionDetectionDataLoader\n",
    "from utils.utils_metrics import document_sentiment_metrics_fn\n",
    "from utils.utils_init_model import text_logit, fine_tuning_model, eval_model, init_model, logit_prob, load_word_index\n",
    "from utils.get_args import get_args\n",
    "\n",
    "# from attack.adv_attack import attack\n",
    "import os, sys\n",
    "\n",
    "from utils.utils_init_model import text_logit, fine_tuning_model, eval_model, init_model, logit_prob, load_word_index\n",
    "import time\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "from attack.attack_helper import get_synonyms, codemix_perturbation, synonym_replacement, swap_minimum_importance_words\n",
    "\n",
    "from attack.adv_attack import attack\n",
    "import main\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words_set = []\n",
    "for w in stopwords.words('indonesian'):\n",
    "    stop_words_set.append(w)\n",
    "\n",
    "from icecream import ic\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "from itertools import chain\n",
    "from deep_translator import GoogleTranslator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca6aceaa-d129-40ef-a44f-03a9bb94c071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trans_dict(wordlist, perturbation_technique, lang=None):\n",
    "#     translated_wordlist = {}\n",
    "#     # ic(wordlist)\n",
    "#     if perturbation_technique == \"codemixing\":\n",
    "#         translator = GoogleTranslator(source=\"id\", target=lang)\n",
    "        \n",
    "#         for word in wordlist:\n",
    "#             if word.isalpha():\n",
    "#                 translated_wordlist[word] = translator.translate(word)\n",
    "#             else:\n",
    "#                 translated_wordlist[word] = word\n",
    "#     else:\n",
    "#         for word in wordlist:\n",
    "#             translated_wordlist[word] = get_synonyms(word)[0]\n",
    "            \n",
    "#     return translated_wordlist\n",
    "\n",
    "\n",
    "# def attack(text_ls,\n",
    "#            true_label,\n",
    "#            predictor,\n",
    "#            tokenizer,\n",
    "#            att_ratio,\n",
    "#            perturbation_technique,\n",
    "#            lang_codemix=None,\n",
    "#            sim_predictor=None,\n",
    "#            sim_score_threshold=0.5,\n",
    "#            sim_score_window=15,\n",
    "#            batch_size=32, \n",
    "#            import_score_threshold=-1.):\n",
    "    \n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     label_dict = {\n",
    "#         'positive': 0, \n",
    "#         'neutral': 1, \n",
    "#         'negative': 2}\n",
    "    \n",
    "#     original_text = text_ls\n",
    "#     orig_label, orig_probs, orig_prob = logit_prob(text_ls, predictor, tokenizer)\n",
    "        \n",
    "# #     SEK SALAAHHHHHH\n",
    "#     if true_label != orig_label:\n",
    "#         running_time = round(time.time() - start_time, 2)\n",
    "#         # perturbed_text, perturbed_semantic_sim, orig_label, orig_probs, perturbed_label, perturbed_probs, translated_words, running_time\n",
    "#         return original_text, 1.000, orig_label, orig_probs, orig_label, orig_probs,'', running_time\n",
    "#     else:\n",
    "#         text_ls = word_tokenize(text_ls)\n",
    "#         text_ls = [word for word in text_ls if word.isalnum()]\n",
    "#         len_text = len(text_ls)\n",
    "#         half_sim_score_window = (sim_score_window - 1) // 2\n",
    "        \n",
    "#         leave_1_texts = [' '.join(text_ls[:ii] + [tokenizer.mask_token] + text_ls[min(ii + 1, len_text):]) for ii in range(len_text)]\n",
    "                \n",
    "#         leave_1_probs = []\n",
    "#         leave_1_probs_argmax = []\n",
    "#         for text_leave_1 in leave_1_texts:\n",
    "#             subwords_leave_1 = tokenizer.encode(text_leave_1)\n",
    "#             subwords_leave_1 = torch.LongTensor(subwords_leave_1).view(1, -1).to(predictor.device)\n",
    "#             logits_leave_1 = predictor(subwords_leave_1)[0]\n",
    "#             orig_label_leave_1 = torch.topk(logits_leave_1, k=1, dim=-1)[1].squeeze().item()\n",
    "            \n",
    "#             leave_1_probs_argmax.append(orig_label_leave_1)\n",
    "#             leave_1_probs.append(F.softmax(logits_leave_1, dim=-1).squeeze().detach().cpu().numpy())\n",
    "            \n",
    "#         leave_1_probs = torch.tensor(leave_1_probs).to(device)\n",
    "        \n",
    "#         orig_prob_extended=np.empty(len_text)\n",
    "#         orig_prob_extended.fill(orig_prob)\n",
    "#         orig_prob_extended = torch.tensor(orig_prob_extended).to(device)\n",
    "        \n",
    "#         arr1 = orig_prob_extended - leave_1_probs[:,orig_label] + float(leave_1_probs_argmax != orig_label)\n",
    "#         arr2 = (leave_1_probs.max(dim=-1)[0].to(device) - orig_probs[leave_1_probs_argmax].to(device))\n",
    "        \n",
    "#         import_scores = arr1*arr2\n",
    "#         import_scores = [im * -1 for im in import_scores]\n",
    "        \n",
    "#         words_perturb = []\n",
    "#         for idx, score in sorted(enumerate(import_scores), key=lambda x: x[1], reverse=True):\n",
    "#             try:\n",
    "#                 if score > import_score_threshold and text_ls[idx] not in stop_words_set:\n",
    "#                     words_perturb.append((idx, text_ls[idx], score.item()))\n",
    "#             except Exception as e:\n",
    "#                 print(e)\n",
    "#                 print(idx, len(text_ls), import_scores.shape, text_ls, len(leave_1_texts))\n",
    "        \n",
    "#         num_perturbation = math.floor(len(words_perturb)*att_ratio)\n",
    "#         if num_perturbation < 1:\n",
    "#             num_perturbation = 1\n",
    "        \n",
    "# #       top words perturb berisi list kata terpenting yang tidak akan diswitch ketika first_codemix_sim_score < sim_score_threshold\n",
    "#         top_words_perturb = words_perturb[:num_perturbation]\n",
    "#         trans_word = [twp[1] for twp in top_words_perturb]\n",
    "\n",
    "#         if perturbation_technique == \"codemixing\":\n",
    "#             perturbed_text = codemix_perturbation(text_ls, lang_codemix, top_words_perturb)\n",
    "#         elif perturbation_technique == \"synonym_replacement\":\n",
    "#             perturbed_text = synonym_replacement(text_ls, top_words_perturb)\n",
    "        \n",
    "#         first_perturbation_sim_score = sim_predictor.semantic_sim(original_text, perturbed_text)\n",
    "\n",
    "#         words_perturb_candidates = []\n",
    "#         if first_perturbation_sim_score < sim_score_threshold:\n",
    "#             words_perturb_candidates.append(top_words_perturb)\n",
    "#             swapped = swap_minimum_importance_words(words_perturb, top_words_perturb)\n",
    "#             for s in swapped:\n",
    "#                 words_perturb_candidates.append(s)\n",
    "\n",
    "#             words_perturb_candidates = [[tuple(w) for w in wpc] for wpc in words_perturb_candidates]\n",
    "\n",
    "#             candidate_comparison = {}\n",
    "#             for wpc in words_perturb_candidates:\n",
    "#                 # ic(wpc)\n",
    "#                 if perturbation_technique == \"codemixing\":\n",
    "#                     perturbed_candidate = codemix_perturbation(text_ls, lang_codemix, wpc)\n",
    "#                 elif perturbation_technique == \"synonym_replacement\":\n",
    "#                     perturbed_candidate = synonym_replacement(text_ls, wpc)\n",
    "                    \n",
    "#                 perturbed_candidate_sim_score = sim_predictor.semantic_sim(original_text, perturbed_candidate)\n",
    "#                 # ic(wpc[-1][-1])\n",
    "#                 candidate_comparison[perturbed_candidate] = (perturbed_candidate_sim_score, wpc[-1][-1], wpc)\n",
    "\n",
    "#             ic(candidate_comparison)\n",
    "#             sorted_candidate_comparison = sorted(candidate_comparison.keys(), key=lambda x: (candidate_comparison[x][0], candidate_comparison[x][1]), reverse=True)\n",
    "\n",
    "#             perturbed_text = sorted_candidate_comparison[0]\n",
    "            \n",
    "#             top_words_perturb = candidate_comparison[sorted_candidate_comparison[0]][-1]\n",
    "#             trans_word = [twp[1] for twp in top_words_perturb]\n",
    "            \n",
    "#         perturbed_semantic_sim = sim_predictor.semantic_sim(original_text, perturbed_text)\n",
    "#         if perturbed_semantic_sim < sim_score_threshold:\n",
    "#             perturbed_text = original_text\n",
    "#             perturbed_semantic_sim = 1.000\n",
    "        \n",
    "#         perturbed_label, perturbed_probs, perturbed_prob = logit_prob(perturbed_text, predictor, tokenizer)\n",
    "        \n",
    "        \n",
    "#         translated_words = trans_dict(trans_word, perturbation_technique, lang_codemix)\n",
    "        \n",
    "#         orig_probs = np.round(orig_probs.detach().cpu().numpy().tolist(),4)\n",
    "#         perturbed_probs = np.round(perturbed_probs.detach().cpu().numpy().tolist(),4)\n",
    "        \n",
    "#         running_time = round(time.time() - start_time, 2)\n",
    "        \n",
    "#         ic(perturbed_text)\n",
    "#         ic(perturbed_semantic_sim)\n",
    "#         ic(orig_label)\n",
    "#         ic(orig_probs)\n",
    "#         ic(perturbed_label)\n",
    "#         ic(perturbed_probs)\n",
    "#         ic(translated_words)\n",
    "#         ic(running_time)\n",
    "        \n",
    "        \n",
    "#         return perturbed_text, perturbed_semantic_sim, orig_label, orig_probs, perturbed_label, perturbed_probs, translated_words, running_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e533330-48af-4e33-a7d7-148e3415d5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_target=\"IndoBERT\"\n",
    "downstream_task=\"sentiment\"\n",
    "attack_strategy=\"adversarial\"\n",
    "finetune_epoch=1\n",
    "num_sample=2\n",
    "# exp_name=\n",
    "perturbation_technique=\"codemixing\"\n",
    "perturb_ratio=0.4\n",
    "perturb_lang=\"en\"\n",
    "seed=26092020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5600622c-804b-46ca-a345-0e8415f1eda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n",
      "2022-03-04 12:06:10.315526: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib:/usr/local/cuda/lib\n",
      "2022-03-04 12:06:10.316279: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-03-04 12:06:11.209366: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "(Epoch 1) TRAIN LOSS:0.3491 LR:0.00000300: 100%|██████████████████| 344/344 [00:57<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.3491 ACC:0.87 F1:0.82 REC:0.79 PRE:0.85 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1921 ACC:0.93 F1:0.90 REC:0.89 PRE:0.90: 100%|█████████| 40/40 [00:03<00:00, 11.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.1921 ACC:0.93 F1:0.90 REC:0.89 PRE:0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "use = USE()\n",
    "\n",
    "tokenizer, config, model = init_model(model_target, downstream_task)\n",
    "w2i, i2w = load_word_index(downstream_task)\n",
    "\n",
    "train_dataset, train_loader, train_path = load_dataset_loader(downstream_task, 'train', tokenizer)\n",
    "valid_dataset, valid_loader, valid_path = load_dataset_loader(downstream_task, 'valid', tokenizer)\n",
    "test_dataset, test_loader, test_path = load_dataset_loader(downstream_task, 'test', tokenizer)\n",
    "# print(model.cuda())\n",
    "finetuned_model = fine_tuning_model(model, i2w, train_loader, valid_loader, finetune_epoch)\n",
    "\n",
    "# exp_dataset.to_csv(os.getcwd() + r'/result/'+exp_name+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a6df894-26f1-4964-b122-8c6298a6589d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meski masa kampanye sudah selesai , bukan berati habis pula upaya mengerek tingkat kedipilihan elektabilitas .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tidak enak</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                             text  \\\n",
       "0  meski masa kampanye sudah selesai , bukan berati habis pula upaya mengerek tingkat kedipilihan elektabilitas .   \n",
       "1                                                                                                      tidak enak   \n",
       "\n",
       "   sentiment  \n",
       "0          1  \n",
       "1          2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dataset = valid_dataset.load_dataset(valid_path).head(num_sample)\n",
    "exp_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f026c3d-7c27-44dc-a9b3-bdea5014b224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| type(perturbed_text): <class 'str'>\n",
      "ic| type(perturbed_semantic_sim): <class 'jaxlib.xla_extension.DeviceArray'>\n",
      "ic| type(orig_label): <class 'int'>\n",
      "ic| type(orig_probs): <class 'numpy.ndarray'>\n",
      "ic| type(perturbed_label): <class 'int'>\n",
      "ic| type(perturbed_probs): <class 'numpy.ndarray'>\n",
      "ic| type(translated_words): <class 'dict'>\n",
      "ic| type(running_time): <class 'float'>\n",
      "ic| type(perturbed_text): <class 'str'>\n",
      "ic| type(perturbed_semantic_sim): <class 'jaxlib.xla_extension.DeviceArray'>\n",
      "ic| type(orig_label): <class 'int'>\n",
      "ic| type(orig_probs): <class 'numpy.ndarray'>\n",
      "ic| type(perturbed_label): <class 'int'>\n",
      "ic| type(perturbed_probs): <class 'numpy.ndarray'>\n",
      "ic| type(translated_words): <class 'dict'>\n",
      "ic| type(running_time): <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "text,label = None,None\n",
    "\n",
    "# print(perturbation_technique)\n",
    "if downstream_task == 'sentiment':\n",
    "    text = 'text'\n",
    "    label = 'sentiment'\n",
    "    exp_dataset[['perturbed_text', 'perturbed_semantic_sim', 'pred_label', 'pred_proba', 'perturbed_label', 'perturbed_proba', 'translated_word(s)', 'running_time(s)']] = exp_dataset.apply(\n",
    "        lambda row: attack(\n",
    "            text_ls = row.text,\n",
    "            true_label = row.sentiment,\n",
    "            predictor = finetuned_model,\n",
    "            tokenizer = tokenizer, \n",
    "            att_ratio = perturb_ratio,\n",
    "            perturbation_technique = perturbation_technique,\n",
    "            lang_codemix = perturb_lang,\n",
    "            sim_predictor = use), axis=1, result_type='expand'\n",
    "    )\n",
    "elif downstream_task == 'emotion':\n",
    "    text = 'tweet'\n",
    "    label = 'label'\n",
    "    exp_dataset[['perturbed_text', 'perturbed_semantic_sim', 'pred_label', 'pred_proba', 'perturbed_label', 'perturbed_proba', 'translated_word(s)', 'running_time(s)']] = exp_dataset.apply(\n",
    "        lambda row: attack(\n",
    "            text_ls = row.tweet,\n",
    "            true_label = row.label,\n",
    "            predictor = finetuned_model,\n",
    "            tokenizer = tokenizer, \n",
    "            att_ratio = perturb_ratio,\n",
    "            perturbation_technique = perturbation_technique,\n",
    "            lang_codemix = perturb_lang,\n",
    "            sim_predictor = use), axis=1, result_type='expand'\n",
    "    )\n",
    "\n",
    "before_attack = accuracy_score(exp_dataset[label], exp_dataset['pred_label'])\n",
    "after_attack = accuracy_score(exp_dataset[label], exp_dataset['perturbed_label'])\n",
    "\n",
    "exp_dataset.loc[exp_dataset.index[0], 'before_attack_acc'] = before_attack\n",
    "exp_dataset.loc[exp_dataset.index[0], 'after_attack_acc'] = after_attack\n",
    "exp_dataset.loc[exp_dataset.index[0], 'avg_running_time(s)'] = exp_dataset[\"running_time(s)\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58203d50-e315-4e71-ad1e-e127a23f64d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3453155/3035608220.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m main(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel_target\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"IndoBERT\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdownstream_task\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mattack_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adversarial\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfinetune_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "main(\n",
    "    model_target=\"IndoBERT\",\n",
    "    downstream_task=\"sentiment\",\n",
    "    attack_strategy=\"adversarial\",\n",
    "    finetune_epoch=1,\n",
    "    num_sample=2,\n",
    "    exp_name=\"fixx\",\n",
    "    perturbation_technique=\"codemixing\",\n",
    "    perturb_ratio=0.4,\n",
    "    perturb_lang=\"en\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "166894a9-ccfc-4bf9-84ad-cfa00abc4548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m13518040/tugas-akhir-repository/attack/adv_attack.py:68: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  leave_1_probs = torch.tensor(leave_1_probs).to(\"cuda:1\")\n",
      "INFO:absl:Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: \n",
      "INFO:absl:Unable to initialize backend 'gpu': NOT_FOUND: Could not find registered platform with name: \"cuda\". Available platform names are: Host Interpreter\n",
      "INFO:absl:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "ic| type(perturbed_text): <class 'str'>\n",
      "ic| type(perturbed_semantic_sim): <class 'jaxlib.xla_extension.DeviceArray'>\n",
      "ic| type(orig_label): <class 'int'>\n",
      "ic| type(orig_probs): <class 'numpy.ndarray'>\n",
      "ic| type(perturbed_label): <class 'int'>\n",
      "ic| type(perturbed_probs): <class 'numpy.ndarray'>\n",
      "ic| type(translated_words): <class 'dict'>\n",
      "ic| type(running_time): <class 'float'>\n",
      "ic| perturbed_text: ('how depraved cadre gerindra yang anggota DPRD molested child junior high '\n",
      "                     'school people harus intelligent party mana yang harus di drown di 2019')\n",
      "ic| perturbed_semantic_sim: DeviceArray(0.5837079, dtype=float32)\n",
      "ic| pred_label: 2\n",
      "ic| pred_proba: array([0.0526, 0.1187, 0.8287])\n",
      "ic| perturbed_label: 1\n",
      "ic| perturbed_proba: array([0.0515, 0.8216, 0.1269])\n",
      "ic| translated_words: {'anak': 'child',\n",
      "                       'bejad': 'depraved',\n",
      "                       'betapa': 'how',\n",
      "                       'cerdas': 'intelligent',\n",
      "                       'dprd': 'DPRD',\n",
      "                       'kader': 'cadre',\n",
      "                       'mencabuli': 'molested',\n",
      "                       'partai': 'party',\n",
      "                       'rakyat': 'people',\n",
      "                       'smp': 'junior high school',\n",
      "                       'tengelamkan': 'drown'}\n",
      "ic| running_time: 9.95\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.95"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"betapa bejad kader gerindra yang anggota dprd mencabuli anak smp , rakyat harus cerdas partai mana yang harus di tengelamkan di 2019\"\n",
    "true_label = 2\n",
    "\n",
    "perturbed_text, perturbed_semantic_sim, pred_label, pred_proba, perturbed_label, perturbed_proba, translated_words, running_time = attack(\n",
    "    text_ls = txt,\n",
    "    true_label = true_label,\n",
    "    predictor = finetuned_model,\n",
    "    tokenizer = tokenizer, \n",
    "    att_ratio = 0.8,\n",
    "    perturbation_technique = perturbation_technique,\n",
    "    lang_codemix = perturb_lang,\n",
    "    sim_predictor = use\n",
    ")\n",
    "\n",
    "ic(perturbed_text)\n",
    "ic(perturbed_semantic_sim)\n",
    "ic(pred_label)\n",
    "ic(pred_proba)\n",
    "ic(perturbed_label)\n",
    "ic(perturbed_proba)\n",
    "ic(translated_words)\n",
    "ic(running_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "383d43b9-3257-4c74-abc7-1a7c48c2d14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>perturbed_text</th>\n",
       "      <th>perturbed_semantic_sim</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>pred_proba</th>\n",
       "      <th>perturbed_label</th>\n",
       "      <th>perturbed_proba</th>\n",
       "      <th>translated_word(s)</th>\n",
       "      <th>running_time(s)</th>\n",
       "      <th>before_attack_acc</th>\n",
       "      <th>after_attack_acc</th>\n",
       "      <th>avg_running_time(s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meski masa kampanye sudah selesai , bukan berati habis pula upaya mengerek tingkat kedipilihan elektabilitas .</td>\n",
       "      <td>1</td>\n",
       "      <td>meski masa campaign sudah finished bukan berati habis pula upaya mengerek level kedipilihan elektabilitas</td>\n",
       "      <td>0.8135486</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0617, 0.5557, 0.3826]</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0985, 0.4105, 0.491]</td>\n",
       "      <td>{'tingkat': 'level', 'kampanye': 'campaign', 'selesai': 'finished'}</td>\n",
       "      <td>4.46</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tidak enak</td>\n",
       "      <td>2</td>\n",
       "      <td>tidak nice</td>\n",
       "      <td>0.5409587</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0133, 0.0083, 0.9784]</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0247, 0.0091, 0.9663]</td>\n",
       "      <td>{'enak': 'nice'}</td>\n",
       "      <td>3.83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                             text  \\\n",
       "0  meski masa kampanye sudah selesai , bukan berati habis pula upaya mengerek tingkat kedipilihan elektabilitas .   \n",
       "1                                                                                                      tidak enak   \n",
       "\n",
       "   sentiment  \\\n",
       "0          1   \n",
       "1          2   \n",
       "\n",
       "                                                                                              perturbed_text  \\\n",
       "0  meski masa campaign sudah finished bukan berati habis pula upaya mengerek level kedipilihan elektabilitas   \n",
       "1                                                                                                 tidak nice   \n",
       "\n",
       "  perturbed_semantic_sim  pred_label                pred_proba  \\\n",
       "0              0.8135486           1  [0.0617, 0.5557, 0.3826]   \n",
       "1              0.5409587           2  [0.0133, 0.0083, 0.9784]   \n",
       "\n",
       "   perturbed_label           perturbed_proba  \\\n",
       "0                2   [0.0985, 0.4105, 0.491]   \n",
       "1                2  [0.0247, 0.0091, 0.9663]   \n",
       "\n",
       "                                                    translated_word(s)  \\\n",
       "0  {'tingkat': 'level', 'kampanye': 'campaign', 'selesai': 'finished'}   \n",
       "1                                                     {'enak': 'nice'}   \n",
       "\n",
       "   running_time(s)  before_attack_acc  after_attack_acc  avg_running_time(s)  \n",
       "0             4.46                1.0               0.5                4.145  \n",
       "1             3.83                NaN               NaN                  NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "686d7659-f7b8-4747-9387-7b3425161b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dataset.to_csv(os.getcwd() + r'/result/'+\"test\"+\".csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
