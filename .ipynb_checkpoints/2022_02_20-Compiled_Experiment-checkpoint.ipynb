{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install icecream\n",
    "# !pip install deep_translator -q\n",
    "# !pip install python-crfsuite -q\n",
    "# !pip install tensorflow-hub==0.7.0 -q\n",
    "# !pip install tensorflow -q\n",
    "\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "e8f79139-a9d1-4165-bc15-afa4cffadffa",
    "_uuid": "bc3ff7e4-d6f3-4dff-adb1-2e008a297636",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/m13518040/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/m13518040/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words_set = []\n",
    "for w in stopwords.words('indonesian'):\n",
    "    stop_words_set.append(w)\n",
    "\n",
    "import math\n",
    "import re\n",
    "import copy\n",
    "\n",
    "from operator import itemgetter\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "from utils.utils_init_dataset import set_seed, load_dataset_loader\n",
    "from utils.utils_semantic_use import USE\n",
    "from utils.utils_data_utils import DocumentSentimentDataset, DocumentSentimentDataLoader, EmotionDetectionDataset, EmotionDetectionDataLoader\n",
    "from utils.utils_forward_fn import forward_sequence_classification\n",
    "from utils.utils_metrics import document_sentiment_metrics_fn\n",
    "from utils.utils_init_model import text_logit, fine_tuning_model, eval_model, init_model\n",
    "\n",
    "# debugger\n",
    "from icecream import ic\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "97eaf6ab-5d4c-4551-a406-9cac89835bc9",
    "_uuid": "0f823d48-2e31-4399-b9bd-8e6f65c9dccb",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def codemix_perturbation(words, target_lang, words_perturb):\n",
    "    \"\"\"\n",
    "    'su': 'sundanese'\n",
    "    'jw': 'javanese\n",
    "    'ms': 'malay','\n",
    "    'en': 'english',\n",
    "    \"\"\"\n",
    "    \n",
    "    translator = GoogleTranslator(source='id', target=target_lang)\n",
    "    \n",
    "    supported_langs = [\"su\", \"jw\", \"ms\", \"en\"]\n",
    "    \n",
    "    if target_lang not in supported_langs:\n",
    "        raise ValueError('Language Unavailable')\n",
    "    \n",
    "    new_words = words.copy()\n",
    "    \n",
    "    if len(words_perturb) >= 1:\n",
    "        for perturb_word in words_perturb:\n",
    "            new_words = [translator.translate(word) if word == perturb_word[1] and word.isalpha() else word for word in new_words]\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def synonym_replacement(words, words_perturb):\n",
    "    return None\n",
    "\n",
    "# fungsi untuk mencari kandidat lain ketika sebuah kandidat perturbasi kurang dari sim_score_threshold\n",
    "def swap_minimum_importance_words(words_perturb, top_importance_words):\n",
    "    def get_minimums(word_tups):\n",
    "        arr = []\n",
    "        for wt in word_tups:\n",
    "            if wt[2] == min(top_importance_words, key = lambda t: t[2])[2]:\n",
    "                arr.append(wt)\n",
    "        return arr\n",
    "    minimum_import = get_minimums(top_importance_words)\n",
    "    unlisted = list(set(words_perturb).symmetric_difference(set(top_importance_words)))\n",
    "\n",
    "    len_wp = len(top_importance_words)\n",
    "    len_ul = len(unlisted)\n",
    "    \n",
    "    res = []\n",
    "    for i in range(len_wp):\n",
    "        if top_importance_words[i] in minimum_import:\n",
    "            temp_wp = list(copy.deepcopy(top_importance_words))\n",
    "            temp_wp.pop(i)\n",
    "            swapped_wp = np.array([(temp_wp) for i in range(len_ul)])\n",
    "            for j in range(len(swapped_wp)):\n",
    "                temp_sm = np.vstack((swapped_wp[j], tuple(unlisted[j])))\n",
    "                \n",
    "                res.append(temp_sm.tolist())\n",
    "                \n",
    "    return res\n",
    "\n",
    "def attack(text_ls,\n",
    "           true_label,\n",
    "           predictor,\n",
    "           tokenizer,\n",
    "           att_ratio,\n",
    "           lang_codemix,\n",
    "           attack_strategy,\n",
    "           sim_predictor=None,\n",
    "           sim_score_threshold=0.5,\n",
    "           sim_score_window=15,\n",
    "           batch_size=32, \n",
    "           import_score_threshold=-1.):\n",
    "    \n",
    "    label_dict = {\n",
    "        'positive': 0, \n",
    "        'neutral': 1, \n",
    "        'negative': 2}\n",
    "    \n",
    "    original_text = text_ls\n",
    "    subwords = tokenizer.encode(text_ls)\n",
    "    subwords = torch.LongTensor(subwords).view(1, -1).to(predictor.device)\n",
    "\n",
    "    logits = predictor(subwords)[0]\n",
    "    orig_label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "    \n",
    "    orig_probs = F.softmax(logits, dim=-1).squeeze()\n",
    "    orig_prob = F.softmax(logits, dim=-1).squeeze()[orig_label].detach().cpu().numpy()\n",
    "        \n",
    "    if true_label != orig_label:\n",
    "        return '', 0, orig_label, orig_label, 0\n",
    "    else:\n",
    "        text_ls = word_tokenize(text_ls)\n",
    "        text_ls = [word for word in text_ls if word.isalnum()]\n",
    "        len_text = len(text_ls)\n",
    "        half_sim_score_window = (sim_score_window - 1) // 2\n",
    "        num_queries = 1\n",
    "        \n",
    "        leave_1_texts = [' '.join(text_ls[:ii] + [tokenizer.mask_token] + text_ls[min(ii + 1, len_text):]) for ii in range(len_text)]\n",
    "                \n",
    "        leave_1_probs = []\n",
    "        leave_1_probs_argmax = []\n",
    "        num_queries += len(leave_1_texts)\n",
    "        for text_leave_1 in leave_1_texts:\n",
    "            subwords_leave_1 = tokenizer.encode(text_leave_1)\n",
    "            subwords_leave_1 = torch.LongTensor(subwords_leave_1).view(1, -1).to(predictor.device)\n",
    "            logits_leave_1 = predictor(subwords_leave_1)[0]\n",
    "            orig_label_leave_1 = torch.topk(logits_leave_1, k=1, dim=-1)[1].squeeze().item()\n",
    "            \n",
    "            leave_1_probs_argmax.append(orig_label_leave_1)\n",
    "            leave_1_probs.append(F.softmax(logits_leave_1, dim=-1).squeeze().detach().cpu().numpy())\n",
    "            \n",
    "        leave_1_probs = torch.tensor(leave_1_probs).to(\"cuda:0\")\n",
    "        \n",
    "        orig_prob_extended=np.empty(len_text)\n",
    "        orig_prob_extended.fill(orig_prob)\n",
    "        orig_prob_extended = torch.tensor(orig_prob_extended).to(\"cuda:0\")\n",
    "        \n",
    "        arr1 = orig_prob_extended - leave_1_probs[:,orig_label] + float(leave_1_probs_argmax != orig_label)\n",
    "        arr2 = (leave_1_probs.max(dim=-1)[0].to(\"cuda:0\") - orig_probs[leave_1_probs_argmax].to(\"cuda:0\"))\n",
    "        \n",
    "        import_scores = arr1*arr2\n",
    "        import_scores = [im * -1 for im in import_scores]\n",
    "        \n",
    "        words_perturb = []\n",
    "        for idx, score in sorted(enumerate(import_scores), key=lambda x: x[1], reverse=True):\n",
    "            try:\n",
    "                if score > import_score_threshold and text_ls[idx] not in stop_words_set:\n",
    "                    words_perturb.append((idx, text_ls[idx], score.item()))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(idx, len(text_ls), import_scores.shape, text_ls, len(leave_1_texts))\n",
    "        \n",
    "        num_perturbation = math.floor(len(words_perturb)*att_ratio)\n",
    "        \n",
    "#       top words perturb berisi list kata terpenting yang tidak akan diswitch ketika first_codemix_sim_score < sim_score_threshold\n",
    "        top_words_perturb = words_perturb[:num_perturbation]\n",
    "        \n",
    "        \n",
    "        if attack_strategy == \"codemixing\":\n",
    "            perturbed_text = codemix_perturbation(text_ls, lang_codemix, words_perturb)\n",
    "        elif attack_strategy == \"synonym_replacement\":\n",
    "            perturbed_text = synonym_replacement(text_ls, words_perturb)\n",
    "        \n",
    "        first_perturbation_sim_score = sim_predictor.semantic_sim(original_text, perturbed_text)\n",
    "                \n",
    "#       cek semantic similarity\n",
    "#       kalo top wordsnya cuma 1 diskip\n",
    "        if len(top_words_perturb) > 1:\n",
    "            words_perturb_candidates = []\n",
    "            if first_perturbation_sim_score < sim_score_threshold:\n",
    "                words_perturb_candidates.append(top_words_perturb)\n",
    "                swapped = swap_minimum_importance_words(words_perturb, top_words_perturb)\n",
    "                for s in swapped:\n",
    "                    words_perturb_candidates.append(s)\n",
    "\n",
    "                words_perturb_candidates = [[tuple(w) for w in wpc] for wpc in words_perturb_candidates]\n",
    "\n",
    "                candidate_comparison = {}\n",
    "                for wpc in words_perturb_candidates:\n",
    "                    if attack_strategy == \"codemixing\":\n",
    "                        perturbed_candidate = codemix_perturbation(text_ls, lang_codemix, words_perturb)\n",
    "                    elif attack_strategy == \"synonym_replacement\":\n",
    "                        perturbed_candidate = synonym_replacement(text_ls, words_perturb)\n",
    "                    \n",
    "                    perturbed_candidate_sim_score = sim_predictor.semantic_sim(original_text, perturbed_candidate)\n",
    "                    candidate_comparison[perturbed_candidate] = (perturbed_candidate_sim_score, wpc[-1][-1])\n",
    "\n",
    "                sorted_candidate_comparison = sorted(candidate_comparison.keys(), key=lambda x: (candidate_comparison[x][0], candidate_comparison[x][1]), reverse=True)\n",
    "                perturbed_text = sorted_candidate_comparison[0]\n",
    "        else:\n",
    "            if first_perturbation_sim_score < sim_score_threshold:\n",
    "                perturbed_text = original_text\n",
    "        \n",
    "        if sim_predictor.semantic_sim(original_text, perturbed_text) < sim_score_threshold:\n",
    "            perturbed_text = original_text\n",
    "        \n",
    "        return perturbed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_codemixing():\n",
    "    return None\n",
    "\n",
    "def run_synonym_replacement():\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ca6f4edd-2b8f-4a31-a38a-04f17f5ed503",
    "_uuid": "659d5919-b3ed-45a4-b635-2b7579d0f919",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 08:37:25.888534: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model initialization..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset..\n",
      "\n",
      "Test initial model on sample text..\n",
      "Text: lokasi di alun alun masakan padang ini cukup terkenal dengan kepala ikan kakap gule , biasa saya pesan nasi bungkus padang berisikan rendang , ayam pop dan perkedel . porsi banyak dan mengenyangkan | Label : positive (38.001%)\n",
      "Text: meski masa kampanye sudah selesai , bukan berati habis pula upaya mengerek tingkat kedipilihan elektabilitas . | Label : neutral (40.470%)\n",
      "Text: kamar nya sempit tidak ada tempat menyimpan barang malah menambah barang . by the way ini kipas2 mau diletakkan mana . mana uchiwa segede ini pula . | Label : neutral (39.304%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.3491 LR:0.00000300: 100%|██████████████████| 344/344 [00:58<00:00,  5.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.3491 ACC:0.87 F1:0.82 REC:0.79 PRE:0.85 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1921 ACC:0.93 F1:0.90 REC:0.89 PRE:0.90: 100%|█████████| 40/40 [00:05<00:00,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.1921 ACC:0.93 F1:0.90 REC:0.89 PRE:0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.1601 LR:0.00000300: 100%|██████████████████| 344/344 [00:57<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.1601 ACC:0.95 F1:0.93 REC:0.92 PRE:0.93 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1859 ACC:0.93 F1:0.90 REC:0.90 PRE:0.91: 100%|█████████| 40/40 [00:05<00:00,  7.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.1859 ACC:0.93 F1:0.90 REC:0.90 PRE:0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                       | 0/344 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "def main(\n",
    "    model_target,\n",
    "    downstream_task,\n",
    "    attack_strategy,\n",
    "    perturbation_technique,\n",
    "    perturb_ratio,\n",
    "    num_sample,\n",
    "    seed=26092020\n",
    "):\n",
    "    set_seed(seed)\n",
    "\n",
    "    use = USE()\n",
    "\n",
    "    print(\"\\nModel initialization..\")\n",
    "    tokenizer, config, model = init_model(model_target)\n",
    "    \n",
    "    if downstream_task == \"sentiment\":\n",
    "        w2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\n",
    "        print(\"\\nLoading dataset..\")\n",
    "        train_dataset, train_loader = load_dataset_loader(downstream_task, 'train', tokenizer)\n",
    "        valid_dataset, valid_loader = load_dataset_loader(downstream_task, 'valid', tokenizer)\n",
    "        test_dataset, test_loader = load_dataset_loader(downstream_task, 'test', tokenizer)\n",
    "        \n",
    "        text0 = 'lokasi di alun alun masakan padang ini cukup terkenal dengan kepala ikan kakap gule , biasa saya pesan nasi bungkus padang berisikan rendang , ayam pop dan perkedel . porsi banyak dan mengenyangkan'\n",
    "        text1 = 'meski masa kampanye sudah selesai , bukan berati habis pula upaya mengerek tingkat kedipilihan elektabilitas .'\n",
    "        text2 = 'kamar nya sempit tidak ada tempat menyimpan barang malah menambah barang . by the way ini kipas2 mau diletakkan mana . mana uchiwa segede ini pula .'\n",
    "\n",
    "\n",
    "        print(\"\\nTest initial model on sample text..\")\n",
    "        text_logit(text0, model, tokenizer, i2w)\n",
    "        text_logit(text1, model, tokenizer, i2w)\n",
    "        text_logit(text2, model, tokenizer, i2w)\n",
    "        \n",
    "        finetuned_model = fine_tuning_model(model, i2w, train_loader, valid_loader, 5)\n",
    "        \n",
    "        print(\"\\nAttacking text using codemixing...\")\n",
    "        codemixed0 = attack(text0, 0, finetuned_model, tokenizer, perturb_ratio, 'jw', attack_strategy, sim_predictor=use)\n",
    "        codemixed1 = attack(text1, 1, finetuned_model, tokenizer, perturb_ratio, 'en', attack_strategy, sim_predictor=use)\n",
    "        codemixed2 = attack(text2, 2, finetuned_model, tokenizer, perturb_ratio, 'su', attack_strategy, sim_predictor=use)\n",
    "\n",
    "        print(\"\\nCalculating logit on codemixed data...\")\n",
    "        text_logit(codemixed0, finetuned_model, tokenizer, i2w)\n",
    "        text_logit(codemixed1, finetuned_model, tokenizer, i2w)\n",
    "        text_logit(codemixed2, finetuned_model, tokenizer, i2w)\n",
    "        \n",
    "        print(\"\\nCalculating similarity score...\")\n",
    "        print(use.semantic_sim(text0, codemixed0))\n",
    "        print(use.semantic_sim(text1, codemixed1))\n",
    "        print(use.semantic_sim(text2, codemixed2))\n",
    "        \n",
    "    elif downstream_task == \"emotion\":\n",
    "        w2i, i2w = EmotionDetectionDataset.LABEL2INDEX, EmotionDetectionDataset.INDEX2LABEL\n",
    "        train_dataset, train_loader = load_dataset_loader(downstream_task, 'train', tokenizer)\n",
    "        valid_dataset, valid_loader = load_dataset_loader(downstream_task, 'valid', tokenizer)\n",
    "        test_dataset, test_loader = load_dataset_loader(downstream_task, 'test', tokenizer)\n",
    "        \n",
    "        finetuned_model = fine_tuning_model(model, i2w, train_loader, valid_loader, 5)\n",
    "        \n",
    "        # prob_before = \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(\n",
    "        model_target=\"IndoBERT\",\n",
    "        downstream_task=\"sentiment\",\n",
    "        attack_strategy=\"codemixing\",\n",
    "        perturbation_technique=\"adversarial\",\n",
    "        perturb_ratio=0.2,\n",
    "        num_sample=0,\n",
    "        seed=26092020\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def main():\n",
    "# set_seed(26092020)\n",
    "\n",
    "# use = USE()\n",
    "\n",
    "# print(\"\\nModel initialization..\")\n",
    "# tokenizer, config, model = init_model(\"IndoBERT\")\n",
    "# w2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\n",
    "\n",
    "# print(\"\\nLoading dataset..\")\n",
    "# train_dataset, train_loader = load_dataset_loader('sentiment', 'train', tokenizer)\n",
    "# valid_dataset, valid_loader = load_dataset_loader('sentiment', 'valid', tokenizer)\n",
    "# test_dataset, test_loader = load_dataset_loader('sentiment', 'test', tokenizer)\n",
    "\n",
    "# text0 = 'lokasi di alun alun masakan padang ini cukup terkenal dengan kepala ikan kakap gule , biasa saya pesan nasi bungkus padang berisikan rendang , ayam pop dan perkedel . porsi banyak dan mengenyangkan'\n",
    "# text1 = 'meski masa kampanye sudah selesai , bukan berati habis pula upaya mengerek tingkat kedipilihan elektabilitas .'\n",
    "# text2 = 'kamar nya sempit tidak ada tempat menyimpan barang malah menambah barang . by the way ini kipas2 mau diletakkan mana . mana uchiwa segede ini pula .'\n",
    "\n",
    "\n",
    "# print(\"\\nTest initial model on sample text..\")\n",
    "# text_logit(text0, model, tokenizer, i2w)\n",
    "# text_logit(text1, model, tokenizer, i2w)\n",
    "# text_logit(text2, model, tokenizer, i2w)\n",
    "\n",
    "# print(\"\\nModel finetuning...\")\n",
    "# finetuned_model = fine_tuning_model(model, i2w, train_loader, valid_loader, 5)\n",
    "# del model\n",
    "\n",
    "\n",
    "# print(\"\\nTest finetuned model on sample text..\")\n",
    "# text_logit(text0, finetuned_model, tokenizer, i2w)\n",
    "# text_logit(text1, finetuned_model, tokenizer, i2w)\n",
    "# text_logit(text2, finetuned_model, tokenizer, i2w)\n",
    "\n",
    "\n",
    "\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nAttacking text using codemixing...\")\n",
    "# codemixed0 = attack(text0,0, finetuned_model, tokenizer, 0.2, 'jw', \"codemixing\", sim_predictor=use)\n",
    "# codemixed1 = attack(text1,1, finetuned_model, tokenizer, 0.2, 'en', \"codemixing\", sim_predictor=use)\n",
    "# codemixed2 = attack(text2,2, finetuned_model, tokenizer, 0.2, 'su', \"codemixing\", sim_predictor=use)\n",
    "\n",
    "# print(\"\\nCalculating logit on codemixed data...\")\n",
    "# text_logit(codemixed0, finetuned_model, tokenizer, i2w)\n",
    "# text_logit(codemixed1, finetuned_model, tokenizer, i2w)\n",
    "# text_logit(codemixed2, finetuned_model, tokenizer, i2w)\n",
    "\n",
    "# print(\"\\nCalculating similarity score...\")\n",
    "# print(use.semantic_sim(text0, codemixed0))\n",
    "# print(use.semantic_sim(text1, codemixed1))\n",
    "# print(use.semantic_sim(text2, codemixed2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
