{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install icecream\n",
    "# !pip install deep_translator -q\n",
    "# !pip install python-crfsuite -q\n",
    "# !pip install tensorflow-hub==0.7.0 -q\n",
    "# !pip install tensorflow -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e8f79139-a9d1-4165-bc15-afa4cffadffa",
    "_uuid": "bc3ff7e4-d6f3-4dff-adb1-2e008a297636",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words_set = []\n",
    "for w in stopwords.words('indonesian'):\n",
    "    stop_words_set.append(w)\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer, AutoTokenizer, XLMRobertaTokenizer, XLMRobertaConfig, XLMRobertaForSequenceClassification, AutoModelForSequenceClassification\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "import math\n",
    "import re\n",
    "import copy\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "from utils.utils_init_dataset import set_seed, load_dataset_loader\n",
    "from utils.utils_semantic_use import USE\n",
    "from utils.utils_data_utils import DocumentSentimentDataset, DocumentSentimentDataLoader, EmotionDetectionDataset, EmotionDetectionDataLoader\n",
    "from utils.utils_forward_fn import forward_sequence_classification\n",
    "from utils.utils_metrics import document_sentiment_metrics_fn\n",
    "from utils.utils_init_model import text_logit, fine_tuning_model, eval_model, init_model\n",
    "\n",
    "# debugger\n",
    "from icecream import ic\n",
    "\n",
    "# from googletrans import Translator\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "97eaf6ab-5d4c-4551-a406-9cac89835bc9",
    "_uuid": "0f823d48-2e31-4399-b9bd-8e6f65c9dccb",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def codemix_perturbation(words, target_lang, words_perturb):\n",
    "    \"\"\"\n",
    "    'su': 'sundanese'\n",
    "    'jw': 'javanese\n",
    "    'ms': 'malay','\n",
    "    'en': 'english',\n",
    "    \"\"\"\n",
    "    \n",
    "    translator = GoogleTranslator(source='id', target=target_lang)\n",
    "    \n",
    "    supported_langs = [\"su\", \"jw\", \"ms\", \"en\"]\n",
    "    \n",
    "    if target_lang not in supported_langs:\n",
    "        raise ValueError('Language Unavailable')\n",
    "    \n",
    "    new_words = words.copy()\n",
    "    \n",
    "    if len(words_perturb) >= 1:\n",
    "        for perturb_word in words_perturb:\n",
    "            new_words = [translator.translate(word) if word == perturb_word[1] and word.isalpha() else word for word in new_words]\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def synonym_replacement(words, words_perturb):\n",
    "    return None\n",
    "\n",
    "# fungsi untuk mencari kandidat lain ketika sebuah kandidat perturbasi kurang dari sim_score_threshold\n",
    "def swap_minimum_importance_words(words_perturb, top_importance_words):\n",
    "    def get_minimums(word_tups):\n",
    "        arr = []\n",
    "        for wt in word_tups:\n",
    "            if wt[2] == min(top_importance_words, key = lambda t: t[2])[2]:\n",
    "                arr.append(wt)\n",
    "        return arr\n",
    "    minimum_import = get_minimums(top_importance_words)\n",
    "    unlisted = list(set(words_perturb).symmetric_difference(set(top_importance_words)))\n",
    "\n",
    "    len_wp = len(top_importance_words)\n",
    "    len_ul = len(unlisted)\n",
    "    \n",
    "    res = []\n",
    "    for i in range(len_wp):\n",
    "        if top_importance_words[i] in minimum_import:\n",
    "            temp_wp = list(copy.deepcopy(top_importance_words))\n",
    "            temp_wp.pop(i)\n",
    "            swapped_wp = np.array([(temp_wp) for i in range(len_ul)])\n",
    "            for j in range(len(swapped_wp)):\n",
    "                temp_sm = np.vstack((swapped_wp[j], tuple(unlisted[j])))\n",
    "                \n",
    "                res.append(temp_sm.tolist())\n",
    "                \n",
    "    return res\n",
    "\n",
    "def attack(text_ls, true_label, predictor, tokenizer, att_ratio, lang_codemix, attack_strategy, sim_predictor=None, sim_score_threshold=0.5, sim_score_window=15, batch_size=32, \n",
    "           import_score_threshold=-1.):\n",
    "    label_dict = {\n",
    "        'positive': 0, \n",
    "        'neutral': 1, \n",
    "        'negative': 2}\n",
    "    \n",
    "    original_text = text_ls\n",
    "    subwords = tokenizer.encode(text_ls)\n",
    "    subwords = torch.LongTensor(subwords).view(1, -1).to(predictor.device)\n",
    "\n",
    "    logits = predictor(subwords)[0]\n",
    "    orig_label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "    \n",
    "    orig_probs = F.softmax(logits, dim=-1).squeeze()\n",
    "    orig_prob = F.softmax(logits, dim=-1).squeeze()[orig_label].detach().cpu().numpy()\n",
    "        \n",
    "    if true_label != orig_label:\n",
    "        return '', 0, orig_label, orig_label, 0\n",
    "    else:\n",
    "        text_ls = word_tokenize(text_ls)\n",
    "        text_ls = [word for word in text_ls if word.isalnum()]\n",
    "        len_text = len(text_ls)\n",
    "        half_sim_score_window = (sim_score_window - 1) // 2\n",
    "        num_queries = 1\n",
    "        \n",
    "        leave_1_texts = [' '.join(text_ls[:ii] + [tokenizer.mask_token] + text_ls[min(ii + 1, len_text):]) for ii in range(len_text)]\n",
    "                \n",
    "        leave_1_probs = []\n",
    "        leave_1_probs_argmax = []\n",
    "        num_queries += len(leave_1_texts)\n",
    "        for text_leave_1 in leave_1_texts:\n",
    "            subwords_leave_1 = tokenizer.encode(text_leave_1)\n",
    "            subwords_leave_1 = torch.LongTensor(subwords_leave_1).view(1, -1).to(predictor.device)\n",
    "            logits_leave_1 = predictor(subwords_leave_1)[0]\n",
    "            orig_label_leave_1 = torch.topk(logits_leave_1, k=1, dim=-1)[1].squeeze().item()\n",
    "            \n",
    "            leave_1_probs_argmax.append(orig_label_leave_1)\n",
    "            leave_1_probs.append(F.softmax(logits_leave_1, dim=-1).squeeze().detach().cpu().numpy())\n",
    "            \n",
    "        leave_1_probs = torch.tensor(leave_1_probs).to(\"cuda:0\")\n",
    "        \n",
    "        orig_prob_extended=np.empty(len_text)\n",
    "        orig_prob_extended.fill(orig_prob)\n",
    "        orig_prob_extended = torch.tensor(orig_prob_extended).to(\"cuda:0\")\n",
    "        \n",
    "        arr1 = orig_prob_extended - leave_1_probs[:,orig_label] + float(leave_1_probs_argmax != orig_label)\n",
    "        arr2 = (leave_1_probs.max(dim=-1)[0].to(\"cuda:0\") - orig_probs[leave_1_probs_argmax].to(\"cuda:0\"))\n",
    "        \n",
    "        import_scores = arr1*arr2\n",
    "        import_scores = [im * -1 for im in import_scores]\n",
    "        \n",
    "        words_perturb = []\n",
    "        for idx, score in sorted(enumerate(import_scores), key=lambda x: x[1], reverse=True):\n",
    "            try:\n",
    "                if score > import_score_threshold and text_ls[idx] not in stop_words_set:\n",
    "                    words_perturb.append((idx, text_ls[idx], score.item()))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(idx, len(text_ls), import_scores.shape, text_ls, len(leave_1_texts))\n",
    "        \n",
    "        num_perturbation = math.floor(len(words_perturb)*att_ratio)\n",
    "        \n",
    "#       top words perturb berisi list kata terpenting yang tidak akan diswitch ketika first_codemix_sim_score < sim_score_threshold\n",
    "        top_words_perturb = words_perturb[:num_perturbation]\n",
    "        \n",
    "        \n",
    "        if attack_strategy == \"codemixing\":\n",
    "            perturbed_text = codemix_perturbation(text_ls, lang_codemix, words_perturb)\n",
    "        elif attack_strategy == \"synonym_replacement\":\n",
    "            perturbed_text = synonym_replacement(text_ls, words_perturb)\n",
    "        \n",
    "        first_perturbation_sim_score = sim_predictor.semantic_sim(original_text, perturbed_text)\n",
    "                \n",
    "#       cek semantic similarity\n",
    "#       kalo top wordsnya cuma 1 diskip\n",
    "        if len(top_words_perturb) > 1:\n",
    "            words_perturb_candidates = []\n",
    "            if first_perturbation_sim_score < sim_score_threshold:\n",
    "                words_perturb_candidates.append(top_words_perturb)\n",
    "                swapped = swap_minimum_importance_words(words_perturb, top_words_perturb)\n",
    "                for s in swapped:\n",
    "                    words_perturb_candidates.append(s)\n",
    "\n",
    "                words_perturb_candidates = [[tuple(w) for w in wpc] for wpc in words_perturb_candidates]\n",
    "\n",
    "                candidate_comparison = {}\n",
    "                for wpc in words_perturb_candidates:\n",
    "                    if attack_strategy == \"codemixing\":\n",
    "                        perturbed_candidate = codemix_perturbation(text_ls, lang_codemix, words_perturb)\n",
    "                    elif attack_strategy == \"synonym_replacement\":\n",
    "                        perturbed_candidate = synonym_replacement(text_ls, words_perturb)\n",
    "                    \n",
    "                    perturbed_candidate_sim_score = sim_predictor.semantic_sim(original_text, perturbed_candidate)\n",
    "                    candidate_comparison[perturbed_candidate] = (perturbed_candidate_sim_score, wpc[-1][-1])\n",
    "\n",
    "                sorted_candidate_comparison = sorted(candidate_comparison.keys(), key=lambda x: (candidate_comparison[x][0], candidate_comparison[x][1]), reverse=True)\n",
    "                perturbed_text = sorted_candidate_comparison[0]\n",
    "        else:\n",
    "            if first_perturbation_sim_score < sim_score_threshold:\n",
    "                perturbed_text = original_text\n",
    "        \n",
    "        if sim_predictor.semantic_sim(original_text, perturbed_text) < sim_score_threshold:\n",
    "            perturbed_text = original_text\n",
    "        \n",
    "        return perturbed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ca6f4edd-2b8f-4a31-a38a-04f17f5ed503",
    "_uuid": "659d5919-b3ed-45a4-b635-2b7579d0f919",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(\n",
    "    model_target,\n",
    "    downstream_task,\n",
    "    attack_strategy,\n",
    "    perturbation_technique,\n",
    "    perturb_ratio,\n",
    "    num_sample,\n",
    "    seed=26092020\n",
    "):\n",
    "    set_seed(seed)\n",
    "\n",
    "    use = USE()\n",
    "\n",
    "    print(\"\\nModel initialization..\")\n",
    "    tokenizer, config, model = init_model(model_target)\n",
    "    \n",
    "    if downstream_task == \"sentiment\":\n",
    "        w2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\n",
    "        print(\"\\nLoading dataset..\")\n",
    "        train_dataset, train_loader = load_dataset_loader(downstream_task, 'train', tokenizer)\n",
    "        valid_dataset, valid_loader = load_dataset_loader(downstream_task, 'valid', tokenizer)\n",
    "        test_dataset, test_loader = load_dataset_loader(downstream_task, 'test', tokenizer)\n",
    "        \n",
    "        text0 = 'lokasi di alun alun masakan padang ini cukup terkenal dengan kepala ikan kakap gule , biasa saya pesan nasi bungkus padang berisikan rendang , ayam pop dan perkedel . porsi banyak dan mengenyangkan'\n",
    "        text1 = 'meski masa kampanye sudah selesai , bukan berati habis pula upaya mengerek tingkat kedipilihan elektabilitas .'\n",
    "        text2 = 'kamar nya sempit tidak ada tempat menyimpan barang malah menambah barang . by the way ini kipas2 mau diletakkan mana . mana uchiwa segede ini pula .'\n",
    "\n",
    "\n",
    "        print(\"\\nTest initial model on sample text..\")\n",
    "        text_logit(text0, model, tokenizer, i2w)\n",
    "        text_logit(text1, model, tokenizer, i2w)\n",
    "        text_logit(text2, model, tokenizer, i2w)\n",
    "        \n",
    "        finetuned_model = fine_tuning_model(model, i2w, train_loader, valid_loader, 5)\n",
    "        \n",
    "        print(\"\\nAttacking text using codemixing...\")\n",
    "        codemixed0 = attack(text0, 0, finetuned_model, tokenizer, perturb_ratio, 'jw', attack_strategy, sim_predictor=use)\n",
    "        codemixed1 = attack(text1, 1, finetuned_model, tokenizer, perturb_ratio, 'en', attack_strategy, sim_predictor=use)\n",
    "        codemixed2 = attack(text2, 2, finetuned_model, tokenizer, perturb_ratio, 'su', attack_strategy, sim_predictor=use)\n",
    "\n",
    "        print(\"\\nCalculating logit on codemixed data...\")\n",
    "        text_logit(codemixed0, finetuned_model, tokenizer, i2w)\n",
    "        text_logit(codemixed1, finetuned_model, tokenizer, i2w)\n",
    "        text_logit(codemixed2, finetuned_model, tokenizer, i2w)\n",
    "        \n",
    "        print(\"\\nCalculating similarity score...\")\n",
    "        print(use.semantic_sim(text0, codemixed0))\n",
    "        print(use.semantic_sim(text1, codemixed1))\n",
    "        print(use.semantic_sim(text2, codemixed2))\n",
    "        \n",
    "    elif downstream_task == \"emotion\":\n",
    "        w2i, i2w = EmotionDetectionDataset.LABEL2INDEX, EmotionDetectionDataset.INDEX2LABEL\n",
    "        train_dataset, train_loader = load_dataset_loader(downstream_task, 'train', tokenizer)\n",
    "        valid_dataset, valid_loader = load_dataset_loader(downstream_task, 'valid', tokenizer)\n",
    "        test_dataset, test_loader = load_dataset_loader(downstream_task, 'test', tokenizer)\n",
    "        \n",
    "        finetuned_model = fine_tuning_model(model, i2w, train_loader, valid_loader, 5)\n",
    "        \n",
    "        # prob_before = \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(\n",
    "        model_target=\"IndoBERT\",\n",
    "        downstream_task=\"sentiment\",\n",
    "        attack_strategy=\"codemixing\",\n",
    "        perturbation_technique=\"adversarial\",\n",
    "        perturb_ratio=0.2,\n",
    "        num_sample=0,\n",
    "        seed=26092020\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def main():\n",
    "# set_seed(26092020)\n",
    "\n",
    "# use = USE()\n",
    "\n",
    "# print(\"\\nModel initialization..\")\n",
    "# tokenizer, config, model = init_model(\"IndoBERT\")\n",
    "# w2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\n",
    "\n",
    "# print(\"\\nLoading dataset..\")\n",
    "# train_dataset, train_loader = load_dataset_loader('sentiment', 'train', tokenizer)\n",
    "# valid_dataset, valid_loader = load_dataset_loader('sentiment', 'valid', tokenizer)\n",
    "# test_dataset, test_loader = load_dataset_loader('sentiment', 'test', tokenizer)\n",
    "\n",
    "# text0 = 'lokasi di alun alun masakan padang ini cukup terkenal dengan kepala ikan kakap gule , biasa saya pesan nasi bungkus padang berisikan rendang , ayam pop dan perkedel . porsi banyak dan mengenyangkan'\n",
    "# text1 = 'meski masa kampanye sudah selesai , bukan berati habis pula upaya mengerek tingkat kedipilihan elektabilitas .'\n",
    "# text2 = 'kamar nya sempit tidak ada tempat menyimpan barang malah menambah barang . by the way ini kipas2 mau diletakkan mana . mana uchiwa segede ini pula .'\n",
    "\n",
    "\n",
    "# print(\"\\nTest initial model on sample text..\")\n",
    "# text_logit(text0, model, tokenizer, i2w)\n",
    "# text_logit(text1, model, tokenizer, i2w)\n",
    "# text_logit(text2, model, tokenizer, i2w)\n",
    "\n",
    "# print(\"\\nModel finetuning...\")\n",
    "# finetuned_model = fine_tuning_model(model, i2w, train_loader, valid_loader, 5)\n",
    "# del model\n",
    "\n",
    "\n",
    "# print(\"\\nTest finetuned model on sample text..\")\n",
    "# text_logit(text0, finetuned_model, tokenizer, i2w)\n",
    "# text_logit(text1, finetuned_model, tokenizer, i2w)\n",
    "# text_logit(text2, finetuned_model, tokenizer, i2w)\n",
    "\n",
    "\n",
    "\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nAttacking text using codemixing...\")\n",
    "# codemixed0 = attack(text0,0, finetuned_model, tokenizer, 0.2, 'jw', \"codemixing\", sim_predictor=use)\n",
    "# codemixed1 = attack(text1,1, finetuned_model, tokenizer, 0.2, 'en', \"codemixing\", sim_predictor=use)\n",
    "# codemixed2 = attack(text2,2, finetuned_model, tokenizer, 0.2, 'su', \"codemixing\", sim_predictor=use)\n",
    "\n",
    "# print(\"\\nCalculating logit on codemixed data...\")\n",
    "# text_logit(codemixed0, finetuned_model, tokenizer, i2w)\n",
    "# text_logit(codemixed1, finetuned_model, tokenizer, i2w)\n",
    "# text_logit(codemixed2, finetuned_model, tokenizer, i2w)\n",
    "\n",
    "# print(\"\\nCalculating similarity score...\")\n",
    "# print(use.semantic_sim(text0, codemixed0))\n",
    "# print(use.semantic_sim(text1, codemixed1))\n",
    "# print(use.semantic_sim(text2, codemixed2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
