{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install icecream\n",
    "# !pip install deep_translator -q\n",
    "# !pip install python-crfsuite -q\n",
    "# !pip install tensorflow-hub==0.7.0 -q\n",
    "# !pip install tensorflow -q\n",
    "# !pip install --upgrade pip\n",
    "# !pip install --upgrade \"jax[cuda]\"\n",
    "# !pip install jaxlib\n",
    "# !pip install pandarallel\n",
    "# !pip install swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "e8f79139-a9d1-4165-bc15-afa4cffadffa",
    "_uuid": "bc3ff7e4-d6f3-4dff-adb1-2e008a297636",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/m13518040/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/m13518040/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw to /home/m13518040/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/omw.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: conlleval in /home/m13518040/.local/lib/python3.8/site-packages (0.2)\n",
      "WARNING:tensorflow:From /home/m13518040/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "INFO: Pandarallel will run on 80 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import gc\n",
    "import swifter\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "gc.collect()\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  \n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "# import jax.numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw')\n",
    "\n",
    "stop_words_set = []\n",
    "for w in stopwords.words('indonesian'):\n",
    "    stop_words_set.append(w)\n",
    "\n",
    "import math\n",
    "import re\n",
    "import copy\n",
    "\n",
    "from operator import itemgetter\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "from utils.utils_init_dataset import set_seed, load_dataset_loader\n",
    "from utils.utils_semantic_use import USE\n",
    "from utils.utils_data_utils import DocumentSentimentDataset, DocumentSentimentDataLoader, EmotionDetectionDataset, EmotionDetectionDataLoader\n",
    "from utils.utils_metrics import document_sentiment_metrics_fn\n",
    "from utils.utils_init_model import text_logit, fine_tuning_model, eval_model, init_model\n",
    "\n",
    "# debugger\n",
    "from icecream import ic\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cuda:3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "97eaf6ab-5d4c-4551-a406-9cac89835bc9",
    "_uuid": "0f823d48-2e31-4399-b9bd-8e6f65c9dccb",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    \n",
    "    word_lemmas = wordnet.lemmas(word, lang=\"ind\")\n",
    "    \n",
    "    hypernyms = []\n",
    "    for lem in word_lemmas:\n",
    "        hypernyms.append(lem.synset().hypernyms())\n",
    "\n",
    "    if not any(hypernyms):\n",
    "        return [word]\n",
    "    \n",
    "    lemma_corp = []\n",
    "    \n",
    "    for hypernym in hypernyms:\n",
    "        if(len(hypernym) < 1):\n",
    "            continue\n",
    "        else:\n",
    "            lemma_corp.append(hypernym[0].lemmas(lang=\"ind\"))\n",
    "            \n",
    "    lemmas = set()\n",
    "    for list_lemmas in lemma_corp:\n",
    "        if(len(list_lemmas) < 1):\n",
    "            lemmas.add(word)\n",
    "        else:\n",
    "            for l in list_lemmas:\n",
    "                lemmas.add(l.name())\n",
    "    \n",
    "    clean_synonyms = set()\n",
    "    for syn in lemmas.copy():\n",
    "        synonym = syn.replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "        synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "        clean_synonyms.add(synonym) \n",
    "    if word in clean_synonyms:\n",
    "        clean_synonyms.remove(word)\n",
    "    \n",
    "    if len(list(clean_synonyms)) < 1:\n",
    "        return [word]\n",
    "    else:\n",
    "        return list(clean_synonyms)\n",
    "\n",
    "def codemix_perturbation(words, target_lang, words_perturb):\n",
    "    \"\"\"\n",
    "    'su': 'sundanese'\n",
    "    'jw': 'javanese'\n",
    "    'ms': 'malay'\n",
    "    'en': 'english'\n",
    "    \"\"\"\n",
    "    \n",
    "    translator = GoogleTranslator(source=\"id\", target=target_lang)\n",
    "    \n",
    "    supported_langs = [\"su\", \"jw\", \"ms\", \"en\"]\n",
    "    \n",
    "    if target_lang not in supported_langs:\n",
    "        raise ValueError('Language Unavailable')\n",
    "    \n",
    "    new_words = words.copy()\n",
    "    \n",
    "    if len(words_perturb) >= 1:\n",
    "        for perturb_word in words_perturb:\n",
    "            new_words = [translator.translate(word) if word == perturb_word[1] and word.isalpha() else word for word in new_words]\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def synonym_replacement(words, words_perturb):    \n",
    "    new_words = words.copy()\n",
    "       \n",
    "    if len(words_perturb) >= 1:\n",
    "        for perturb_word in words_perturb:\n",
    "            new_words = [get_synonyms(word)[0] if word == perturb_word[1] and word.isalpha() else word for word in new_words]\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "# fungsi untuk mencari kandidat lain ketika sebuah kandidat perturbasi kurang dari sim_score_threshold\n",
    "def swap_minimum_importance_words(words_perturb, top_importance_words):\n",
    "    def get_minimums(word_tups):\n",
    "        arr = []\n",
    "        for wt in word_tups:\n",
    "            if wt[2] == min(top_importance_words, key = lambda t: t[2])[2]:\n",
    "                arr.append(wt)\n",
    "        return arr\n",
    "    minimum_import = get_minimums(top_importance_words)\n",
    "    unlisted = list(set(words_perturb).symmetric_difference(set(top_importance_words)))\n",
    "\n",
    "    len_wp = len(top_importance_words)\n",
    "    len_ul = len(unlisted)\n",
    "    \n",
    "    res = []\n",
    "    for i in range(len_wp):\n",
    "        if top_importance_words[i] in minimum_import:\n",
    "            temp_wp = list(copy.deepcopy(top_importance_words))\n",
    "            temp_wp.pop(i)\n",
    "            swapped_wp = np.array([(temp_wp) for i in range(len_ul)])\n",
    "            for j in range(len(swapped_wp)):\n",
    "                temp_sm = np.vstack((swapped_wp[j], tuple(unlisted[j])))\n",
    "                \n",
    "                res.append(temp_sm.tolist())\n",
    "                \n",
    "    return res\n",
    "\n",
    "def logit_prob(text_ls, predictor, tokenizer):\n",
    "    original_text = text_ls\n",
    "    subwords = tokenizer.encode(text_ls)\n",
    "    subwords = torch.LongTensor(subwords).view(1, -1).to(predictor.device)\n",
    "\n",
    "    logits = predictor(subwords)[0]\n",
    "    orig_label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "    \n",
    "    orig_probs = F.softmax(logits, dim=-1).squeeze()\n",
    "    orig_prob = F.softmax(logits, dim=-1).squeeze()[orig_label].detach().cpu().numpy()\n",
    "    \n",
    "    return orig_label, orig_probs, orig_prob\n",
    "\n",
    "def attack(text_ls,\n",
    "           true_label,\n",
    "           predictor,\n",
    "           tokenizer,\n",
    "           att_ratio,\n",
    "           attack_strategy,\n",
    "           lang_codemix=None,\n",
    "           sim_predictor=None,\n",
    "           sim_score_threshold=0.5,\n",
    "           sim_score_window=15,\n",
    "           batch_size=32, \n",
    "           import_score_threshold=-1.):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    label_dict = {\n",
    "        'positive': 0, \n",
    "        'neutral': 1, \n",
    "        'negative': 2}\n",
    "    \n",
    "    original_text = text_ls\n",
    "    orig_label, orig_probs, orig_prob = logit_prob(text_ls, predictor, tokenizer)\n",
    "        \n",
    "#     SEK SALAAHHHHHH\n",
    "    if true_label != orig_label:\n",
    "        running_time = round(time.time() - start_time, 2)\n",
    "        # perturbed_text, perturbed_semantic_sim, orig_label, orig_prob, perturbed_label, perturbed_prob, running_time\n",
    "        return original_text, 1.000, orig_label, orig_prob, orig_label, orig_prob, running_time\n",
    "    else:\n",
    "        text_ls = word_tokenize(text_ls)\n",
    "        text_ls = [word for word in text_ls if word.isalnum()]\n",
    "        len_text = len(text_ls)\n",
    "        half_sim_score_window = (sim_score_window - 1) // 2\n",
    "        # num_queries = 1\n",
    "        \n",
    "        leave_1_texts = [' '.join(text_ls[:ii] + [tokenizer.mask_token] + text_ls[min(ii + 1, len_text):]) for ii in range(len_text)]\n",
    "                \n",
    "        leave_1_probs = []\n",
    "        leave_1_probs_argmax = []\n",
    "        # num_queries += len(leave_1_texts)\n",
    "        for text_leave_1 in leave_1_texts:\n",
    "            subwords_leave_1 = tokenizer.encode(text_leave_1)\n",
    "            subwords_leave_1 = torch.LongTensor(subwords_leave_1).view(1, -1).to(predictor.device)\n",
    "            logits_leave_1 = predictor(subwords_leave_1)[0]\n",
    "            orig_label_leave_1 = torch.topk(logits_leave_1, k=1, dim=-1)[1].squeeze().item()\n",
    "            \n",
    "            leave_1_probs_argmax.append(orig_label_leave_1)\n",
    "            leave_1_probs.append(F.softmax(logits_leave_1, dim=-1).squeeze().detach().cpu().numpy())\n",
    "            \n",
    "        leave_1_probs = torch.tensor(leave_1_probs).to(\"cuda:1\")\n",
    "        \n",
    "        orig_prob_extended=np.empty(len_text)\n",
    "        orig_prob_extended.fill(orig_prob)\n",
    "        orig_prob_extended = torch.tensor(orig_prob_extended).to(\"cuda:1\")\n",
    "        \n",
    "        arr1 = orig_prob_extended - leave_1_probs[:,orig_label] + float(leave_1_probs_argmax != orig_label)\n",
    "        arr2 = (leave_1_probs.max(dim=-1)[0].to(\"cuda:1\") - orig_probs[leave_1_probs_argmax].to(\"cuda:1\"))\n",
    "        \n",
    "        import_scores = arr1*arr2\n",
    "        import_scores = [im * -1 for im in import_scores]\n",
    "        \n",
    "        words_perturb = []\n",
    "        for idx, score in sorted(enumerate(import_scores), key=lambda x: x[1], reverse=True):\n",
    "            try:\n",
    "                if score > import_score_threshold and text_ls[idx] not in stop_words_set:\n",
    "                    words_perturb.append((idx, text_ls[idx], score.item()))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(idx, len(text_ls), import_scores.shape, text_ls, len(leave_1_texts))\n",
    "        \n",
    "        num_perturbation = math.floor(len(words_perturb)*att_ratio)\n",
    "        \n",
    "#       top words perturb berisi list kata terpenting yang tidak akan diswitch ketika first_codemix_sim_score < sim_score_threshold\n",
    "        top_words_perturb = words_perturb[:num_perturbation]\n",
    "        \n",
    "        \n",
    "        if attack_strategy == \"codemixing\":\n",
    "            perturbed_text = codemix_perturbation(text_ls, lang_codemix, words_perturb)\n",
    "        elif attack_strategy == \"synonym_replacement\":\n",
    "            perturbed_text = synonym_replacement(text_ls, words_perturb)\n",
    "        \n",
    "        first_perturbation_sim_score = sim_predictor.semantic_sim(original_text, perturbed_text)\n",
    "        first_perturbation_sim_score = 0.4        \n",
    "#       cek semantic similarity\n",
    "#       kalo top wordsnya cuma 1 diskip\n",
    "        if len(top_words_perturb) > 1:\n",
    "            words_perturb_candidates = []\n",
    "            if first_perturbation_sim_score < sim_score_threshold:\n",
    "                words_perturb_candidates.append(top_words_perturb)\n",
    "                swapped = swap_minimum_importance_words(words_perturb, top_words_perturb)\n",
    "                for s in swapped:\n",
    "                    words_perturb_candidates.append(s)\n",
    "\n",
    "                words_perturb_candidates = [[tuple(w) for w in wpc] for wpc in words_perturb_candidates]\n",
    "\n",
    "                candidate_comparison = {}\n",
    "                for wpc in words_perturb_candidates:\n",
    "                    if attack_strategy == \"codemixing\":\n",
    "                        perturbed_candidate = codemix_perturbation(text_ls, lang_codemix, words_perturb)\n",
    "                    elif attack_strategy == \"synonym_replacement\":\n",
    "                        perturbed_candidate = synonym_replacement(text_ls, words_perturb)\n",
    "                    \n",
    "                    perturbed_candidate_sim_score = sim_predictor.semantic_sim(original_text, perturbed_candidate)\n",
    "                    candidate_comparison[perturbed_candidate] = (perturbed_candidate_sim_score, wpc[-1][-1])\n",
    "\n",
    "                sorted_candidate_comparison = sorted(candidate_comparison.keys(), key=lambda x: (candidate_comparison[x][0], candidate_comparison[x][1]), reverse=True)\n",
    "                perturbed_text = sorted_candidate_comparison[0]\n",
    "        else:\n",
    "            if first_perturbation_sim_score < sim_score_threshold:\n",
    "                perturbed_text = original_text\n",
    "        \n",
    "        perturbed_semantic_sim = sim_predictor.semantic_sim(original_text, perturbed_text)\n",
    "        if perturbed_semantic_sim < sim_score_threshold:\n",
    "            perturbed_text = original_text\n",
    "            perturbed_semantic_sim = 1.000\n",
    "        \n",
    "        perturbed_label, perturbed_probs, perturbed_prob = logit_prob(perturbed_text, predictor, tokenizer)\n",
    "        \n",
    "        running_time = round(time.time() - start_time, 2)\n",
    "        \n",
    "        return perturbed_text, perturbed_semantic_sim, orig_label, orig_prob, perturbed_label, perturbed_prob, running_time\n",
    "    \n",
    "def load_word_index(downstream_task):\n",
    "    w2i, i2w = None, None\n",
    "    if downstream_task == 'sentiment':\n",
    "        w2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\n",
    "        return w2i, i2w\n",
    "    elif downstream_task == 'emotion':\n",
    "        w2i, i2w = EmotionDetectionDataset.LABEL2INDEX, EmotionDetectionDataset.INDEX2LABEL\n",
    "        return w2i, i2w\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset: https://huggingface.co/datasets/indonlu\n",
    "# saved variables:\n",
    "# %pert\n",
    "# logit prob before v\n",
    "# logit prob after v\n",
    "# prediction before v\n",
    "# prediction after v\n",
    "# running time\n",
    "# semantic sim v\n",
    "# adv training score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "ca6f4edd-2b8f-4a31-a38a-04f17f5ed503",
    "_uuid": "659d5919-b3ed-45a4-b635-2b7579d0f919",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-27 11:21:13.227499: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "(Epoch 1) TRAIN LOSS:1.4313 LR:0.00000300: 100%|██████████████████| 111/111 [00:24<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:1.4313 ACC:0.40 F1:0.34 REC:0.36 PRE:0.41 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:1.2297 ACC:0.54 F1:0.52 REC:0.52 PRE:0.57: 100%|█████████| 14/14 [00:02<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:1.2297 ACC:0.54 F1:0.52 REC:0.52 PRE:0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:1.0873 LR:0.00000300: 100%|██████████████████| 111/111 [00:24<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:1.0873 ACC:0.60 F1:0.58 REC:0.58 PRE:0.60 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.9515 ACC:0.62 F1:0.61 REC:0.62 PRE:0.61: 100%|█████████| 14/14 [00:02<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.9515 ACC:0.62 F1:0.61 REC:0.62 PRE:0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.8165 LR:0.00000300: 100%|██████████████████| 111/111 [00:24<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.8165 ACC:0.71 F1:0.71 REC:0.71 PRE:0.72 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.8281 ACC:0.68 F1:0.67 REC:0.68 PRE:0.69: 100%|█████████| 14/14 [00:02<00:00,  5.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.8281 ACC:0.68 F1:0.67 REC:0.68 PRE:0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.6862 LR:0.00000300: 100%|██████████████████| 111/111 [00:24<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.6862 ACC:0.77 F1:0.77 REC:0.77 PRE:0.78 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7716 ACC:0.71 F1:0.72 REC:0.72 PRE:0.72: 100%|█████████| 14/14 [00:02<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.7716 ACC:0.71 F1:0.72 REC:0.72 PRE:0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.5650 LR:0.00000300: 100%|██████████████████| 111/111 [00:24<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.5650 ACC:0.81 F1:0.81 REC:0.81 PRE:0.82 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7707 ACC:0.70 F1:0.70 REC:0.70 PRE:0.71: 100%|█████████| 14/14 [00:02<00:00,  5.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.7707 ACC:0.70 F1:0.70 REC:0.70 PRE:0.71\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3bb01a182464b5eb553d2dc826b4792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main(\n",
    "    model_target,\n",
    "    downstream_task,\n",
    "    attack_strategy,\n",
    "    perturbation_technique,\n",
    "    perturb_ratio,\n",
    "    finetune_epoch,\n",
    "    num_sample,\n",
    "    result_file,\n",
    "    seed=26092020\n",
    "):\n",
    "    set_seed(seed)\n",
    "    use = USE()\n",
    "\n",
    "    tokenizer, config, model = init_model(model_target, downstream_task)\n",
    "    w2i, i2w = load_word_index(downstream_task)\n",
    "    \n",
    "    train_dataset, train_loader, train_path = load_dataset_loader(downstream_task, 'train', tokenizer)\n",
    "    valid_dataset, valid_loader, valid_path = load_dataset_loader(downstream_task, 'valid', tokenizer)\n",
    "    test_dataset, test_loader, test_path = load_dataset_loader(downstream_task, 'test', tokenizer)\n",
    "\n",
    "    finetuned_model = fine_tuning_model(model, i2w, train_loader, valid_loader, 5)\n",
    "\n",
    "    exp_dataset = valid_dataset.load_dataset(valid_path).head(num_sample)\n",
    "\n",
    "    text,label = None,None\n",
    "    if downstream_task == 'sentiment':\n",
    "        text = 'text'\n",
    "        label = 'sentiment'\n",
    "        exp_dataset[['perturbed_text', 'perturbed_semantic_sim', 'orig_label', 'orig_prob', 'perturbed_label', 'perturbed_prob', 'running_time(s)']] = exp_dataset.swifter.apply(\n",
    "            lambda row: attack(\n",
    "                row.text,\n",
    "                row.sentiment,\n",
    "                finetuned_model,\n",
    "                tokenizer, 0.2,\n",
    "                \"codemixing\",\n",
    "                \"en\",\n",
    "                use), axis=1, result_type='expand'\n",
    "        )\n",
    "    elif downstream_task == 'emotion':\n",
    "        text = 'tweet'\n",
    "        label = 'label'\n",
    "        exp_dataset[['perturbed_text', 'perturbed_semantic_sim', 'orig_label', 'orig_prob', 'perturbed_label', 'perturbed_prob', 'running_time(s)']] = exp_dataset.swifter.apply(\n",
    "            lambda row: attack(\n",
    "                row.tweet,\n",
    "                row.label,\n",
    "                finetuned_model,\n",
    "                tokenizer, 0.2,\n",
    "                \"codemixing\",\n",
    "                \"en\",\n",
    "                use), axis=1, result_type='expand'\n",
    "        )\n",
    "\n",
    "    before_attack = accuracy_score(exp_dataset[label], exp_dataset['orig_label'])\n",
    "    after_attack = accuracy_score(exp_dataset[label], exp_dataset['perturbed_label'])\n",
    "\n",
    "    exp_dataset.loc[exp_dataset.index[0], 'before_attack_acc'] = before_attack\n",
    "    exp_dataset.loc[exp_dataset.index[0], 'after_attack_acc'] = after_attack\n",
    "    exp_dataset.to_csv(os.getcwd() + r'/result/'+result_file+\".csv\", index=False)        \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main(\n",
    "        model_target=\"IndoBERT\", # IndoBERT, XLM-R, mBERT\n",
    "        downstream_task=\"emotion\", # sentiment, emotion\n",
    "        attack_strategy=\"synonym_replacement\", # codemixing, synonym replacement\n",
    "        perturbation_technique=\"adversarial\", # adversarial, random\n",
    "        perturb_ratio=0.2, # 0.2, 0.4, 0.6, 0.8\n",
    "        finetune_epoch=5,\n",
    "        num_sample=2,\n",
    "        result_file=\"test-indobert-emotion-synonym_replacement-adversarial-0.2\",\n",
    "        seed=26092020\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb 27 11:28:59 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.119.04   Driver Version: 450.119.04   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    56W / 300W |  31398MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   41C    P0    58W / 300W |   5400MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   42C    P0    60W / 300W |    920MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    61W / 300W |    920MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   41C    P0    58W / 300W |    920MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   41C    P0    60W / 300W |    920MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   42C    P0    57W / 300W |  24602MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    60W / 300W |    920MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1632568      C   /usr/bin/python3                 8149MiB |\n",
      "|    1   N/A  N/A   1632568      C   /usr/bin/python3                 1391MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n",
      "2022-02-27 11:15:59.592851: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib:/usr/local/cuda/lib\n",
      "2022-02-27 11:15:59.593521: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-02-27 11:16:00.606676: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model initialization..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset..\n",
      "\n",
      "Test initial model on sample text..\n",
      "\n",
      "Model finetuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:1.4313 LR:0.00000300: 100%|██████████████████| 111/111 [00:24<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:1.4313 ACC:0.40 F1:0.34 REC:0.36 PRE:0.41 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:1.2297 ACC:0.54 F1:0.52 REC:0.52 PRE:0.57: 100%|█████████| 14/14 [00:02<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:1.2297 ACC:0.54 F1:0.52 REC:0.52 PRE:0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:1.0873 LR:0.00000300: 100%|██████████████████| 111/111 [00:24<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:1.0873 ACC:0.60 F1:0.58 REC:0.58 PRE:0.60 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.9515 ACC:0.62 F1:0.61 REC:0.62 PRE:0.61: 100%|█████████| 14/14 [00:02<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.9515 ACC:0.62 F1:0.61 REC:0.62 PRE:0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.8165 LR:0.00000300: 100%|██████████████████| 111/111 [00:24<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.8165 ACC:0.71 F1:0.71 REC:0.71 PRE:0.72 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.8281 ACC:0.68 F1:0.67 REC:0.68 PRE:0.69: 100%|█████████| 14/14 [00:02<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.8281 ACC:0.68 F1:0.67 REC:0.68 PRE:0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.6862 LR:0.00000300: 100%|██████████████████| 111/111 [00:24<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.6862 ACC:0.77 F1:0.77 REC:0.77 PRE:0.78 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7716 ACC:0.71 F1:0.72 REC:0.72 PRE:0.72: 100%|█████████| 14/14 [00:02<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.7716 ACC:0.71 F1:0.72 REC:0.72 PRE:0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.5650 LR:0.00000300: 100%|██████████████████| 111/111 [00:24<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.5650 ACC:0.81 F1:0.81 REC:0.81 PRE:0.82 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7707 ACC:0.70 F1:0.70 REC:0.70 PRE:0.71: 100%|█████████| 14/14 [00:02<00:00,  6.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.7707 ACC:0.70 F1:0.70 REC:0.70 PRE:0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# {0: 'sadness', 1: 'anger', 2: 'love', 3: 'fear', 4: 'happy'}\n",
    "set_seed(26092020)\n",
    "\n",
    "use = USE()\n",
    "\n",
    "print(\"\\nModel initialization..\")\n",
    "downstream_task = \"emotion\"\n",
    "tokenizer, config, model = init_model(\"IndoBERT\", downstream_task)\n",
    "w2i, i2w = load_word_index(downstream_task)\n",
    "\n",
    "print(\"\\nLoading dataset..\")\n",
    "train_dataset, train_loader, train_path = load_dataset_loader(downstream_task, 'train', tokenizer)\n",
    "valid_dataset, valid_loader, valid_path = load_dataset_loader(downstream_task, 'valid', tokenizer)\n",
    "test_dataset, test_loader, test_path_ = load_dataset_loader(downstream_task, 'test', tokenizer)\n",
    "\n",
    "text0 = 'lokasi di alun alun masakan padang ini cukup terkenal dengan kepala ikan kakap gule , biasa saya pesan nasi bungkus padang berisikan rendang , ayam pop dan perkedel . porsi banyak dan mengenyangkan'\n",
    "text1 = 'meski masa kampanye sudah selesai , bukan berati habis pula upaya mengerek tingkat kedipilihan elektabilitas .'\n",
    "text2 = 'kamar nya sempit tidak ada tempat menyimpan barang malah menambah barang . by the way ini kipas2 mau diletakkan mana . mana uchiwa segede ini pula .'\n",
    "\n",
    "\n",
    "print(\"\\nTest initial model on sample text..\")\n",
    "text_logit(text0, model, tokenizer, i2w)\n",
    "text_logit(text1, model, tokenizer, i2w)\n",
    "text_logit(text2, model, tokenizer, i2w)\n",
    "\n",
    "print(\"\\nModel finetuning...\")\n",
    "finetuned_model = fine_tuning_model(model, i2w, train_loader, valid_loader, 5)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[USERNAME] jaringannya mati ya? Tidak bisa dibuka mobile jkn. Saya mau ke puskesmes trus piye mau tunjukkan kartu elektoniknya? #kecewa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's like a circle of stupidity. Atlit gak diurusin, duitnya buat bebenah ini itu biar atlit yang dateng dari luar negri bisa nyaman disini tp atlit sendiri kurang gizi. Belum beres bebenah eh salah satu stadion dirusak \"supporter\" karna timnya (atlit) main jelek. What a moron.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  \\\n",
       "0      1   \n",
       "1      1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                    tweet  \n",
       "0                                                                                                                                                 [USERNAME] jaringannya mati ya? Tidak bisa dibuka mobile jkn. Saya mau ke puskesmes trus piye mau tunjukkan kartu elektoniknya? #kecewa  \n",
       "1  It's like a circle of stupidity. Atlit gak diurusin, duitnya buat bebenah ini itu biar atlit yang dateng dari luar negri bisa nyaman disini tp atlit sendiri kurang gizi. Belum beres bebenah eh salah satu stadion dirusak \"supporter\" karna timnya (atlit) main jelek. What a moron.  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dataset = valid_dataset.load_dataset(valid_path).head(2)\n",
    "exp_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1632568/157027359.py:166: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  leave_1_probs = torch.tensor(leave_1_probs).to(\"cuda:1\")\n",
      "INFO:absl:Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: \n",
      "INFO:absl:Unable to initialize backend 'gpu': NOT_FOUND: Could not find registered platform with name: \"cuda\". Available platform names are: Interpreter Host\n",
      "INFO:absl:Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>perturbed_text</th>\n",
       "      <th>perturbed_semantic_sim</th>\n",
       "      <th>orig_label</th>\n",
       "      <th>orig_prob</th>\n",
       "      <th>perturbed_label</th>\n",
       "      <th>perturbed_prob</th>\n",
       "      <th>running_time(s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[USERNAME] jaringannya mati ya? Tidak bisa dibuka mobile jkn. Saya mau ke puskesmes trus piye mau tunjukkan kartu elektoniknya? #kecewa</td>\n",
       "      <td>[USERNAME] jaringannya mati ya? Tidak bisa dibuka mobile jkn. Saya mau ke puskesmes trus piye mau tunjukkan kartu elektoniknya? #kecewa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.83189756</td>\n",
       "      <td>1</td>\n",
       "      <td>0.83189756</td>\n",
       "      <td>63.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's like a circle of stupidity. Atlit gak diurusin, duitnya buat bebenah ini itu biar atlit yang dateng dari luar negri bisa nyaman disini tp atlit sendiri kurang gizi. Belum beres bebenah eh salah satu stadion dirusak \"supporter\" karna timnya (atlit) main jelek. What a moron.</td>\n",
       "      <td>It's like a circle of stupidity. Atlit gak diurusin, duitnya buat bebenah ini itu biar atlit yang dateng dari luar negri bisa nyaman disini tp atlit sendiri kurang gizi. Belum beres bebenah eh salah satu stadion dirusak \"supporter\" karna timnya (atlit) main jelek. What a moron.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41411155</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41411155</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  \\\n",
       "0      1   \n",
       "1      1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                    tweet  \\\n",
       "0                                                                                                                                                 [USERNAME] jaringannya mati ya? Tidak bisa dibuka mobile jkn. Saya mau ke puskesmes trus piye mau tunjukkan kartu elektoniknya? #kecewa   \n",
       "1  It's like a circle of stupidity. Atlit gak diurusin, duitnya buat bebenah ini itu biar atlit yang dateng dari luar negri bisa nyaman disini tp atlit sendiri kurang gizi. Belum beres bebenah eh salah satu stadion dirusak \"supporter\" karna timnya (atlit) main jelek. What a moron.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                           perturbed_text  \\\n",
       "0                                                                                                                                                 [USERNAME] jaringannya mati ya? Tidak bisa dibuka mobile jkn. Saya mau ke puskesmes trus piye mau tunjukkan kartu elektoniknya? #kecewa   \n",
       "1  It's like a circle of stupidity. Atlit gak diurusin, duitnya buat bebenah ini itu biar atlit yang dateng dari luar negri bisa nyaman disini tp atlit sendiri kurang gizi. Belum beres bebenah eh salah satu stadion dirusak \"supporter\" karna timnya (atlit) main jelek. What a moron.   \n",
       "\n",
       "   perturbed_semantic_sim  orig_label   orig_prob  perturbed_label  \\\n",
       "0                     1.0           1  0.83189756                1   \n",
       "1                     1.0           0  0.41411155                0   \n",
       "\n",
       "  perturbed_prob  running_time(s)  \n",
       "0     0.83189756            63.49  \n",
       "1     0.41411155             0.02  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dataset[['perturbed_text', 'perturbed_semantic_sim', 'orig_label', 'orig_prob', 'perturbed_label', 'perturbed_prob', 'running_time(s)']] = exp_dataset.apply(\n",
    "    lambda row: attack(\n",
    "        row.tweet,\n",
    "        row.label,\n",
    "        finetuned_model,\n",
    "        tokenizer, 0.2,\n",
    "        \"codemixing\",\n",
    "        \"en\",\n",
    "        use), axis=1, result_type='expand'\n",
    ")\n",
    "exp_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>perturbed_text</th>\n",
       "      <th>perturbed_semantic_sim</th>\n",
       "      <th>orig_label</th>\n",
       "      <th>orig_prob</th>\n",
       "      <th>perturbed_label</th>\n",
       "      <th>perturbed_prob</th>\n",
       "      <th>running_time(s)</th>\n",
       "      <th>before_attack_acc</th>\n",
       "      <th>after_attack_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[USERNAME] jaringannya mati ya? Tidak bisa dibuka mobile jkn. Saya mau ke puskesmes trus piye mau tunjukkan kartu elektoniknya? #kecewa</td>\n",
       "      <td>[USERNAME] jaringannya mati ya? Tidak bisa dibuka mobile jkn. Saya mau ke puskesmes trus piye mau tunjukkan kartu elektoniknya? #kecewa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.83189756</td>\n",
       "      <td>1</td>\n",
       "      <td>0.83189756</td>\n",
       "      <td>63.49</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's like a circle of stupidity. Atlit gak diurusin, duitnya buat bebenah ini itu biar atlit yang dateng dari luar negri bisa nyaman disini tp atlit sendiri kurang gizi. Belum beres bebenah eh salah satu stadion dirusak \"supporter\" karna timnya (atlit) main jelek. What a moron.</td>\n",
       "      <td>It's like a circle of stupidity. Atlit gak diurusin, duitnya buat bebenah ini itu biar atlit yang dateng dari luar negri bisa nyaman disini tp atlit sendiri kurang gizi. Belum beres bebenah eh salah satu stadion dirusak \"supporter\" karna timnya (atlit) main jelek. What a moron.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41411155</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41411155</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  \\\n",
       "0      1   \n",
       "1      1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                    tweet  \\\n",
       "0                                                                                                                                                 [USERNAME] jaringannya mati ya? Tidak bisa dibuka mobile jkn. Saya mau ke puskesmes trus piye mau tunjukkan kartu elektoniknya? #kecewa   \n",
       "1  It's like a circle of stupidity. Atlit gak diurusin, duitnya buat bebenah ini itu biar atlit yang dateng dari luar negri bisa nyaman disini tp atlit sendiri kurang gizi. Belum beres bebenah eh salah satu stadion dirusak \"supporter\" karna timnya (atlit) main jelek. What a moron.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                           perturbed_text  \\\n",
       "0                                                                                                                                                 [USERNAME] jaringannya mati ya? Tidak bisa dibuka mobile jkn. Saya mau ke puskesmes trus piye mau tunjukkan kartu elektoniknya? #kecewa   \n",
       "1  It's like a circle of stupidity. Atlit gak diurusin, duitnya buat bebenah ini itu biar atlit yang dateng dari luar negri bisa nyaman disini tp atlit sendiri kurang gizi. Belum beres bebenah eh salah satu stadion dirusak \"supporter\" karna timnya (atlit) main jelek. What a moron.   \n",
       "\n",
       "   perturbed_semantic_sim  orig_label   orig_prob  perturbed_label  \\\n",
       "0                     1.0           1  0.83189756                1   \n",
       "1                     1.0           0  0.41411155                0   \n",
       "\n",
       "  perturbed_prob  running_time(s)  before_attack_acc  after_attack_acc  \n",
       "0     0.83189756            63.49                0.5               0.5  \n",
       "1     0.41411155             0.02                NaN               NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exp_dataset = exp_dataset.append([{'before_attack_acc':32, 'after_attack_acc':43}], ignore_index=True)\n",
    "# exp_dataset.loc[exp_dataset.before_attack_acc == 0, \"before_attack_acc\"] = 92\n",
    "# exp_dataset.loc[exp_dataset.after_attack_acc == 0, \"after_attack_acc\"] = 91\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "before_attack = accuracy_score(exp_dataset['label'], exp_dataset['orig_label'])\n",
    "after_attack = accuracy_score(exp_dataset['label'], exp_dataset['perturbed_label'])\n",
    "\n",
    "exp_dataset.loc[exp_dataset.index[0], 'before_attack_acc'] = before_attack\n",
    "exp_dataset.loc[exp_dataset.index[0], 'after_attack_acc'] = after_attack\n",
    "exp_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "set_seed(26092020)\n",
    "\n",
    "use = USE()\n",
    "\n",
    "print(\"\\nModel initialization..\")\n",
    "tokenizer, config, model = init_model(\"IndoBERT\", 'sentiment')\n",
    "w2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\n",
    "\n",
    "print(\"\\nLoading dataset..\")\n",
    "train_dataset, train_loader, train_path = load_dataset_loader('sentiment', 'train', tokenizer)\n",
    "valid_dataset, valid_loader, valid_path = load_dataset_loader('sentiment', 'valid', tokenizer)\n",
    "test_dataset, test_loader, test_path_ = load_dataset_loader('sentiment', 'test', tokenizer)\n",
    "\n",
    "text0 = 'lokasi di alun alun masakan padang ini cukup terkenal dengan kepala ikan kakap gule , biasa saya pesan nasi bungkus padang berisikan rendang , ayam pop dan perkedel . porsi banyak dan mengenyangkan'\n",
    "text1 = 'meski masa kampanye sudah selesai , bukan berati habis pula upaya mengerek tingkat kedipilihan elektabilitas .'\n",
    "text2 = 'kamar nya sempit tidak ada tempat menyimpan barang malah menambah barang . by the way ini kipas2 mau diletakkan mana . mana uchiwa segede ini pula .'\n",
    "\n",
    "\n",
    "print(\"\\nTest initial model on sample text..\")\n",
    "text_logit(text0, model, tokenizer, i2w)\n",
    "text_logit(text1, model, tokenizer, i2w)\n",
    "text_logit(text2, model, tokenizer, i2w)\n",
    "\n",
    "print(\"\\nModel finetuning...\")\n",
    "finetuned_model = fine_tuning_model(model, i2w, train_loader, valid_loader, 5)\n",
    "del model\n",
    "\n",
    "\n",
    "print(\"\\nTest finetuned model on sample text..\")\n",
    "text_logit(text0, finetuned_model, tokenizer, i2w)\n",
    "text_logit(text1, finetuned_model, tokenizer, i2w)\n",
    "text_logit(text2, finetuned_model, tokenizer, i2w)\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[USERNAME] jaringannya mati ya? Tidak bisa dibuka mobile jkn. Saya mau ke puskesmes trus piye mau tunjukkan kartu elektoniknya? #kecewa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's like a circle of stupidity. Atlit gak diurusin, duitnya buat bebenah ini itu biar atlit yang dateng dari luar negri bisa nyaman disini tp atlit sendiri kurang gizi. Belum beres bebenah eh salah satu stadion dirusak \"supporter\" karna timnya (atlit) main jelek. What a moron.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  \\\n",
       "0      1   \n",
       "1      1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                    tweet  \n",
       "0                                                                                                                                                 [USERNAME] jaringannya mati ya? Tidak bisa dibuka mobile jkn. Saya mau ke puskesmes trus piye mau tunjukkan kartu elektoniknya? #kecewa  \n",
       "1  It's like a circle of stupidity. Atlit gak diurusin, duitnya buat bebenah ini itu biar atlit yang dateng dari luar negri bisa nyaman disini tp atlit sendiri kurang gizi. Belum beres bebenah eh salah satu stadion dirusak \"supporter\" karna timnya (atlit) main jelek. What a moron.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dset['val'] = dset['val'].map(perturb_fb, num_proc=40)\n",
    "\n",
    "exp_dataset = valid_dataset.load_dataset(valid_path).head(2)\n",
    "exp_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1632568/1728251893.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m exp_dataset[['perturbed_text', 'perturbed_semantic_sim', 'orig_label', 'orig_prob', 'perturbed_label', 'perturbed_prob', 'running_time(s)']] = exp_dataset.apply(\n\u001b[0m\u001b[1;32m      2\u001b[0m     lambda row: attack(\n\u001b[1;32m      3\u001b[0m         \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mfinetuned_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   8738\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8739\u001b[0m         )\n\u001b[0;32m-> 8740\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8742\u001b[0m     def applymap(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1632568/1728251893.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      1\u001b[0m exp_dataset[['perturbed_text', 'perturbed_semantic_sim', 'orig_label', 'orig_prob', 'perturbed_label', 'perturbed_prob', 'running_time(s)']] = exp_dataset.apply(\n\u001b[1;32m      2\u001b[0m     lambda row: attack(\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mfinetuned_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5485\u001b[0m         ):\n\u001b[1;32m   5486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "exp_dataset[['perturbed_text', 'perturbed_semantic_sim', 'orig_label', 'orig_prob', 'perturbed_label', 'perturbed_prob', 'running_time(s)']] = exp_dataset.apply(\n",
    "    lambda row: attack(\n",
    "        row.text,\n",
    "        row.sentiment,\n",
    "        finetuned_model,\n",
    "        tokenizer, 0.2,\n",
    "        \"codemixing\",\n",
    "        \"en\",\n",
    "        use), axis=1, result_type='expand'\n",
    ")\n",
    "exp_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_dataset = exp_dataset.append([{'before_attack_acc':32, 'after_attack_acc':43}], ignore_index=True)\n",
    "# exp_dataset.loc[exp_dataset.before_attack_acc == 0, \"before_attack_acc\"] = 92\n",
    "# exp_dataset.loc[exp_dataset.after_attack_acc == 0, \"after_attack_acc\"] = 91\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "downstream_task = 'sentiment'\n",
    "\n",
    "before_attack = accuracy_score(exp_dataset[downstream_task], exp_dataset['orig_label'])\n",
    "after_attack = accuracy_score(exp_dataset[downstream_task], exp_dataset['perturbed_label'])\n",
    "\n",
    "exp_dataset.loc[exp_dataset.index[0], 'before_attack_acc'] = before_attack\n",
    "exp_dataset.loc[exp_dataset.index[0], 'after_attack_acc'] = after_attack\n",
    "exp_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'test'\n",
    "exp_dataset.to_csv(os.getcwd() + r'/result/'+filename+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAttacking text using codemixing...\")\n",
    "# perturbed_text, perturbed_semantic_sim, orig_prob, perturbed_prob\n",
    "codemixed0 = attack(text0,0, finetuned_model, tokenizer, 0.2, 'jw', \"codemixing\", sim_predictor=use)\n",
    "codemixed1 = attack(text1,1, finetuned_model, tokenizer, 0.2, 'en', \"codemixing\", sim_predictor=use)\n",
    "codemixed2 = attack(text2,2, finetuned_model, tokenizer, 0.2, 'su', \"codemixing\", sim_predictor=use)\n",
    "\n",
    "print(\"\\nCalculating logit on codemixed data...\")\n",
    "text_logit(codemixed0, finetuned_model, tokenizer, i2w)\n",
    "text_logit(codemixed1, finetuned_model, tokenizer, i2w)\n",
    "text_logit(codemixed2, finetuned_model, tokenizer, i2w)\n",
    "\n",
    "print(\"\\nCalculating similarity score...\")\n",
    "print(use.semantic_sim(text0, codemixed0))\n",
    "print(use.semantic_sim(text1, codemixed1))\n",
    "print(use.semantic_sim(text2, codemixed2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codemixed1 = attack(text1,1, finetuned_model, tokenizer, 0.2, 'en', \"codemixing\", sim_predictor=use)\n",
    "ic(codemixed1)\n",
    "text_logit(codemixed1, finetuned_model, tokenizer, i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attacking text using synonym replacement...\n",
      "\n",
      "Calculating logit on codemixed data...\n",
      "Text: pendudukan di gelombang gelombang minuman keras stadion ini cukup tokoh dengan kewibawaan manusia ikan laut gule biasa saya surat biji bijian menubuhkan stadion berisikan rendang daging pop dan perkedel porsi banyak dan berhasil | Label : positive (97.747%)\n",
      "Text: meski masa kampanye sudah selesai , bukan berati habis pula upaya mengerek tingkat kedipilihan elektabilitas . | Label : neutral (83.620%)\n",
      "Text: bilik nya sempit tidak ada tempat menyarangkan keberadaan malah menceritakan keberadaan by the way ini kipas2 mau diletakkan mana mana uchiwa segede ini pula | Label : negative (98.721%)\n",
      "\n",
      "Calculating similarity score...\n",
      "0.8651537\n",
      "1.0\n",
      "0.9570405\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAttacking text using synonym replacement...\")\n",
    "codemixed0 = attack(text0,0, finetuned_model, tokenizer, 0.2, 'id', \"synonym_replacement\", sim_predictor=use)\n",
    "codemixed1 = attack(text1,1, finetuned_model, tokenizer, 0.2, 'id', \"synonym_replacement\", sim_predictor=use)\n",
    "codemixed2 = attack(text2,2, finetuned_model, tokenizer, 0.2, 'id', \"synonym_replacement\", sim_predictor=use)\n",
    "\n",
    "print(\"\\nCalculating logit on codemixed data...\")\n",
    "text_logit(codemixed0, finetuned_model, tokenizer, i2w)\n",
    "text_logit(codemixed1, finetuned_model, tokenizer, i2w)\n",
    "text_logit(codemixed2, finetuned_model, tokenizer, i2w)\n",
    "\n",
    "print(\"\\nCalculating similarity score...\")\n",
    "print(use.semantic_sim(text0, codemixed0))\n",
    "print(use.semantic_sim(text1, codemixed1))\n",
    "print(use.semantic_sim(text2, codemixed2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attacking text using synonym replacement...\n",
      "\n",
      "Calculating logit on synonym replaced data...\n",
      "Text: pendudukan di gelombang gelombang minuman keras stadion ini cukup tokoh dengan kewibawaan manusia ikan laut gule biasa saya surat biji bijian menubuhkan stadion berisikan rendang daging pop dan perkedel porsi banyak dan berhasil | Label : positive (97.747%)\n",
      "Text: meski masa usaha niaga sudah melengkapi bukan berati memenatkan pula pendudukan membesarkan menyangga kedipilihan elektabilitas | Label : neutral (59.589%)\n",
      "Text: bilik nya sempit tidak ada tempat menyarangkan keberadaan malah menceritakan keberadaan by the way ini kipas2 mau diletakkan mana mana uchiwa segede ini pula | Label : negative (98.721%)\n",
      "\n",
      "Calculating similarity score...\n",
      "0.8651537\n",
      "0.8801627\n",
      "0.9570405\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAttacking text using synonym replacement...\")\n",
    "codemixed0 = attack(text0,0, finetuned_model, tokenizer, 0.4, 'id', \"synonym_replacement\", sim_predictor=use)\n",
    "codemixed1 = attack(text1,1, finetuned_model, tokenizer, 0.4, 'id', \"synonym_replacement\", sim_predictor=use)\n",
    "codemixed2 = attack(text2,2, finetuned_model, tokenizer, 0.4, 'id', \"synonym_replacement\", sim_predictor=use)\n",
    "\n",
    "print(\"\\nCalculating logit on synonym replaced data...\")\n",
    "text_logit(codemixed0, finetuned_model, tokenizer, i2w)\n",
    "text_logit(codemixed1, finetuned_model, tokenizer, i2w)\n",
    "text_logit(codemixed2, finetuned_model, tokenizer, i2w)\n",
    "\n",
    "print(\"\\nCalculating similarity score...\")\n",
    "print(use.semantic_sim(text0, codemixed0))\n",
    "print(use.semantic_sim(text1, codemixed1))\n",
    "print(use.semantic_sim(text2, codemixed2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attacking text using synonym replacement...\n",
      "\n",
      "Calculating logit on synonym replaced data...\n",
      "Text: pendudukan di gelombang gelombang minuman keras stadion ini cukup tokoh dengan kewibawaan manusia ikan laut gule biasa saya surat biji bijian menubuhkan stadion berisikan rendang daging pop dan perkedel porsi banyak dan berhasil | Label : positive (97.747%)\n",
      "Text: meski masa usaha niaga sudah melengkapi bukan berati memenatkan pula pendudukan membesarkan menyangga kedipilihan elektabilitas | Label : neutral (59.589%)\n",
      "Text: bilik nya sempit tidak ada tempat menyarangkan keberadaan malah menceritakan keberadaan by the way ini kipas2 mau diletakkan mana mana uchiwa segede ini pula | Label : negative (98.721%)\n",
      "\n",
      "Calculating similarity score...\n",
      "0.8651537\n",
      "0.8801627\n",
      "0.9570405\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAttacking text using synonym replacement...\")\n",
    "codemixed0 = attack(text0,0, finetuned_model, tokenizer, 0.6, 'id', \"synonym_replacement\", sim_predictor=use)\n",
    "codemixed1 = attack(text1,1, finetuned_model, tokenizer, 0.6, 'id', \"synonym_replacement\", sim_predictor=use)\n",
    "codemixed2 = attack(text2,2, finetuned_model, tokenizer, 0.6, 'id', \"synonym_replacement\", sim_predictor=use)\n",
    "\n",
    "print(\"\\nCalculating logit on synonym replaced data...\")\n",
    "text_logit(codemixed0, finetuned_model, tokenizer, i2w)\n",
    "text_logit(codemixed1, finetuned_model, tokenizer, i2w)\n",
    "text_logit(codemixed2, finetuned_model, tokenizer, i2w)\n",
    "\n",
    "print(\"\\nCalculating similarity score...\")\n",
    "print(use.semantic_sim(text0, codemixed0))\n",
    "print(use.semantic_sim(text1, codemixed1))\n",
    "print(use.semantic_sim(text2, codemixed2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attacking text using synonym replacement...\n",
      "\n",
      "Calculating logit on synonym replaced data...\n",
      "Text: pendudukan di gelombang gelombang minuman keras stadion ini cukup tokoh dengan kewibawaan manusia ikan laut gule biasa saya surat biji bijian menubuhkan stadion berisikan rendang daging pop dan perkedel porsi banyak dan berhasil | Label : positive (97.747%)\n",
      "Text: meski masa usaha niaga sudah melengkapi bukan berati memenatkan pula pendudukan membesarkan menyangga kedipilihan elektabilitas | Label : neutral (59.589%)\n",
      "Text: bilik nya sempit tidak ada tempat menyarangkan keberadaan malah menceritakan keberadaan by the way ini kipas2 mau diletakkan mana mana uchiwa segede ini pula | Label : negative (98.721%)\n",
      "\n",
      "Calculating similarity score...\n",
      "0.8651537\n",
      "0.8801627\n",
      "0.9570405\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAttacking text using synonym replacement...\")\n",
    "codemixed0 = attack(text0,0, finetuned_model, tokenizer, 0.8, 'id', \"synonym_replacement\", sim_predictor=use)\n",
    "codemixed1 = attack(text1,1, finetuned_model, tokenizer, 0.8, 'id', \"synonym_replacement\", sim_predictor=use)\n",
    "codemixed2 = attack(text2,2, finetuned_model, tokenizer, 0.8, 'id', \"synonym_replacement\", sim_predictor=use)\n",
    "\n",
    "print(\"\\nCalculating logit on synonym replaced data...\")\n",
    "text_logit(codemixed0, finetuned_model, tokenizer, i2w)\n",
    "text_logit(codemixed1, finetuned_model, tokenizer, i2w)\n",
    "text_logit(codemixed2, finetuned_model, tokenizer, i2w)\n",
    "\n",
    "print(\"\\nCalculating similarity score...\")\n",
    "print(use.semantic_sim(text0, codemixed0))\n",
    "print(use.semantic_sim(text1, codemixed1))\n",
    "print(use.semantic_sim(text2, codemixed2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
