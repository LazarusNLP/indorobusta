{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install icecream\n",
    "# !pip install deep_translator -q\n",
    "# !pip install python-crfsuite -q\n",
    "# !pip install tensorflow-hub==0.7.0 -q\n",
    "# !pip install tensorflow -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "e8f79139-a9d1-4165-bc15-afa4cffadffa",
    "_uuid": "bc3ff7e4-d6f3-4dff-adb1-2e008a297636",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/m13518040/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/m13518040/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/m13518040/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "Requirement already satisfied: conlleval in /home/m13518040/.local/lib/python3.8/site-packages (0.2)\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words_set = []\n",
    "for w in stopwords.words('indonesian'):\n",
    "    stop_words_set.append(w)\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer, AutoTokenizer, XLMRobertaTokenizer, XLMRobertaConfig, XLMRobertaForSequenceClassification, AutoModelForSequenceClassification\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "import math\n",
    "import re\n",
    "import copy\n",
    "# from sklearn.utils import shuffle\n",
    "\n",
    "# import tensorflow\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "# tf.disable_eager_execution()\n",
    "# import tensorflow_hub as hub\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "from utils.utils_init_dataset import set_seed, count_param, get_lr, metrics_to_string, init_model, load_dataset_loader\n",
    "from utils.utils_semantic_use import USE\n",
    "\n",
    "# debugger\n",
    "from icecream import ic\n",
    "\n",
    "# from googletrans import Translator\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "97eaf6ab-5d4c-4551-a406-9cac89835bc9",
    "_uuid": "0f823d48-2e31-4399-b9bd-8e6f65c9dccb",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def text_logit(text, model, tokenizer, i2w):\n",
    "    subwords = tokenizer.encode(text)\n",
    "    subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "    \n",
    "    logits = model(subwords)[0]\n",
    "    label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "    print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')\n",
    "\n",
    "def fine_tuning_model(base_model, i2w, train_loader, valid_loader, epochs=5):\n",
    "    optimizer = optim.Adam(base_model.parameters(), lr=3e-6)\n",
    "    base_model = base_model.cuda()\n",
    "    \n",
    "    # Train\n",
    "    n_epochs = epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        base_model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "        total_train_loss = 0\n",
    "        list_hyp, list_label = [], []\n",
    "\n",
    "        train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\n",
    "        for i, batch_data in enumerate(train_pbar):\n",
    "            # Forward base_model\n",
    "            loss, batch_hyp, batch_label = forward_sequence_classification(base_model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "\n",
    "            # Update base_model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tr_loss = loss.item()\n",
    "            total_train_loss = total_train_loss + tr_loss\n",
    "\n",
    "            # Calculate metrics\n",
    "            list_hyp += batch_hyp\n",
    "            list_label += batch_label\n",
    "\n",
    "            train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\n",
    "                total_train_loss/(i+1), get_lr(optimizer)))\n",
    "\n",
    "        # Calculate train metric\n",
    "        metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "        print(\"(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}\".format((epoch+1),\n",
    "            total_train_loss/(i+1), metrics_to_string(metrics), get_lr(optimizer)))\n",
    "\n",
    "        # Evaluate on validation\n",
    "        base_model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        total_loss, total_correct, total_labels = 0, 0, 0\n",
    "        list_hyp, list_label = [], []\n",
    "\n",
    "        pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\n",
    "        for i, batch_data in enumerate(pbar):\n",
    "            batch_seq = batch_data[-1]        \n",
    "            loss, batch_hyp, batch_label = forward_sequence_classification(base_model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "\n",
    "            # Calculate total loss\n",
    "            valid_loss = loss.item()\n",
    "            total_loss = total_loss + valid_loss\n",
    "\n",
    "            # Calculate evaluation metrics\n",
    "            list_hyp += batch_hyp\n",
    "            list_label += batch_label\n",
    "            metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "\n",
    "            pbar.set_description(\"VALID LOSS:{:.4f} {}\".format(total_loss/(i+1), metrics_to_string(metrics)))\n",
    "\n",
    "        metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "        print(\"(Epoch {}) VALID LOSS:{:.4f} {}\".format((epoch+1),\n",
    "            total_loss/(i+1), metrics_to_string(metrics)))\n",
    "    return base_model\n",
    "\n",
    "def eval_model(model, test_loader, i2w):\n",
    "    # Evaluate on test\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    total_loss, total_correct, total_labels = 0, 0, 0\n",
    "    list_hyp, list_label = [], []\n",
    "\n",
    "    pbar = tqdm(test_loader, leave=True, total=len(test_loader))\n",
    "    for i, batch_data in enumerate(pbar):\n",
    "        _, batch_hyp, _ = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "        list_hyp += batch_hyp\n",
    "\n",
    "    # Save prediction\n",
    "    df = pd.DataFrame({'label':list_hyp}).reset_index()\n",
    "    df.to_csv('pred.txt', index=False)\n",
    "\n",
    "    print(df)\n",
    "    \n",
    "\n",
    "def codemix_perturbation(words, target_lang, words_perturb):\n",
    "    \"\"\"\n",
    "    'su': 'sundanese'\n",
    "    'jw': 'javanese\n",
    "    'ms': 'malay','\n",
    "    'en': 'english',\n",
    "    \"\"\"\n",
    "    \n",
    "    translator = GoogleTranslator(source='id', target=target_lang)\n",
    "    \n",
    "    supported_langs = [\"su\", \"jw\", \"ms\", \"en\"]\n",
    "    \n",
    "    if target_lang not in supported_langs:\n",
    "        raise ValueError('Language Unavailable')\n",
    "    \n",
    "    new_words = words.copy()\n",
    "    \n",
    "    if len(words_perturb) >= 1:\n",
    "        for perturb_word in words_perturb:\n",
    "            new_words = [translator.translate(word) if word == perturb_word[1] and word.isalpha() else word for word in new_words]\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def synonym_replacement(words, words_perturb):\n",
    "    return None\n",
    "\n",
    "# fungsi untuk mencari kandidat lain ketika sebuah kandidat perturbasi kurang dari sim_score_threshold\n",
    "def swap_minimum_importance_words(words_perturb, top_importance_words):\n",
    "    def get_minimums(word_tups):\n",
    "        arr = []\n",
    "        for wt in word_tups:\n",
    "            if wt[2] == min(top_importance_words, key = lambda t: t[2])[2]:\n",
    "                arr.append(wt)\n",
    "        return arr\n",
    "    minimum_import = get_minimums(top_importance_words)\n",
    "    unlisted = list(set(words_perturb).symmetric_difference(set(top_importance_words)))\n",
    "\n",
    "    len_wp = len(top_importance_words)\n",
    "    len_ul = len(unlisted)\n",
    "    \n",
    "    res = []\n",
    "    for i in range(len_wp):\n",
    "        if top_importance_words[i] in minimum_import:\n",
    "            temp_wp = list(copy.deepcopy(top_importance_words))\n",
    "            temp_wp.pop(i)\n",
    "            swapped_wp = np.array([(temp_wp) for i in range(len_ul)])\n",
    "            for j in range(len(swapped_wp)):\n",
    "                temp_sm = np.vstack((swapped_wp[j], tuple(unlisted[j])))\n",
    "                \n",
    "                res.append(temp_sm.tolist())\n",
    "                \n",
    "    return res\n",
    "\n",
    "def attack(text_ls, true_label, predictor, tokenizer, att_ratio, lang_codemix, attack_strategy, sim_predictor=None, sim_score_threshold=0.5, sim_score_window=15, batch_size=32, \n",
    "           import_score_threshold=-1.):\n",
    "    label_dict = {\n",
    "        'positive': 0, \n",
    "        'neutral': 1, \n",
    "        'negative': 2}\n",
    "    \n",
    "    original_text = text_ls\n",
    "    subwords = tokenizer.encode(text_ls)\n",
    "    subwords = torch.LongTensor(subwords).view(1, -1).to(predictor.device)\n",
    "\n",
    "    logits = predictor(subwords)[0]\n",
    "    orig_label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "    \n",
    "    orig_probs = F.softmax(logits, dim=-1).squeeze()\n",
    "    orig_prob = F.softmax(logits, dim=-1).squeeze()[orig_label].detach().cpu().numpy()\n",
    "        \n",
    "    if true_label != orig_label:\n",
    "        return '', 0, orig_label, orig_label, 0\n",
    "    else:\n",
    "        text_ls = word_tokenize(text_ls)\n",
    "        text_ls = [word for word in text_ls if word.isalnum()]\n",
    "        len_text = len(text_ls)\n",
    "        half_sim_score_window = (sim_score_window - 1) // 2\n",
    "        num_queries = 1\n",
    "        \n",
    "        leave_1_texts = [' '.join(text_ls[:ii] + [tokenizer.mask_token] + text_ls[min(ii + 1, len_text):]) for ii in range(len_text)]\n",
    "                \n",
    "        leave_1_probs = []\n",
    "        leave_1_probs_argmax = []\n",
    "        num_queries += len(leave_1_texts)\n",
    "        for text_leave_1 in leave_1_texts:\n",
    "            subwords_leave_1 = tokenizer.encode(text_leave_1)\n",
    "            subwords_leave_1 = torch.LongTensor(subwords_leave_1).view(1, -1).to(predictor.device)\n",
    "            logits_leave_1 = predictor(subwords_leave_1)[0]\n",
    "            orig_label_leave_1 = torch.topk(logits_leave_1, k=1, dim=-1)[1].squeeze().item()\n",
    "            \n",
    "            leave_1_probs_argmax.append(orig_label_leave_1)\n",
    "            leave_1_probs.append(F.softmax(logits_leave_1, dim=-1).squeeze().detach().cpu().numpy())\n",
    "            \n",
    "        leave_1_probs = torch.tensor(leave_1_probs).to(\"cuda:0\")\n",
    "        \n",
    "        orig_prob_extended=np.empty(len_text)\n",
    "        orig_prob_extended.fill(orig_prob)\n",
    "        orig_prob_extended = torch.tensor(orig_prob_extended).to(\"cuda:0\")\n",
    "        \n",
    "        arr1 = orig_prob_extended - leave_1_probs[:,orig_label] + float(leave_1_probs_argmax != orig_label)\n",
    "        arr2 = (leave_1_probs.max(dim=-1)[0].to(\"cuda:0\") - orig_probs[leave_1_probs_argmax].to(\"cuda:0\"))\n",
    "        \n",
    "        import_scores = arr1*arr2\n",
    "        import_scores = [im * -1 for im in import_scores]\n",
    "        \n",
    "        words_perturb = []\n",
    "        for idx, score in sorted(enumerate(import_scores), key=lambda x: x[1], reverse=True):\n",
    "            try:\n",
    "                if score > import_score_threshold and text_ls[idx] not in stop_words_set:\n",
    "                    words_perturb.append((idx, text_ls[idx], score.item()))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(idx, len(text_ls), import_scores.shape, text_ls, len(leave_1_texts))\n",
    "        \n",
    "        num_perturbation = math.floor(len(words_perturb)*att_ratio)\n",
    "        \n",
    "#       top words perturb berisi list kata terpenting yang tidak akan diswitch ketika first_codemix_sim_score < sim_score_threshold\n",
    "        top_words_perturb = words_perturb[:num_perturbation]\n",
    "        \n",
    "        \n",
    "        if attack_strategy == \"codemixing\":\n",
    "            perturbed_text = codemix_perturbation(text_ls, lang_codemix, words_perturb)\n",
    "        elif attack_strategy == \"synonym_replacement\":\n",
    "            perturbed_text = synonym_replacement(text_ls, words_perturb)\n",
    "        \n",
    "        first_perturbation_sim_score = use.semantic_sim(original_text, perturbed_text)\n",
    "        words_perturb_candidates = []\n",
    "                \n",
    "#       cek semantic similarity\n",
    "#       kalo top wordsnya cuma 1 diskip\n",
    "        if len(top_words_perturb) > 1:\n",
    "            if first_perturbation_sim_score < sim_score_threshold:\n",
    "                words_perturb_candidates.append(top_words_perturb)\n",
    "                swapped = swap_minimum_importance_words(words_perturb, top_words_perturb)\n",
    "                for s in swapped:\n",
    "                    words_perturb_candidates.append(s)\n",
    "\n",
    "                words_perturb_candidates = [[tuple(w) for w in wpc] for wpc in words_perturb_candidates]\n",
    "\n",
    "                candidate_comparison = {}\n",
    "                for wpc in words_perturb_candidates:\n",
    "                    if attack_strategy == \"codemixing\":\n",
    "                        perturbed_candidate = codemix_perturbation(text_ls, lang_codemix, words_perturb)\n",
    "                    elif attack_strategy == \"synonym_replacement\":\n",
    "                        perturbed_candidate = synonym_replacement(text_ls, words_perturb)\n",
    "                    \n",
    "                    perturbed_candidate_sim_score = use.semantic_sim(original_text, perturbed_candidate)\n",
    "                    candidate_comparison[perturbed_candidate] = (perturbed_candidate_sim_score, wpc[-1][-1])\n",
    "\n",
    "                sorted_candidate_comparison = sorted(candidate_comparison.keys(), key=lambda x: (candidate_comparison[x][0], candidate_comparison[x][1]), reverse=True)\n",
    "                perturbed_text = sorted_candidate_comparison[0]\n",
    "        else:\n",
    "            if first_perturbation_sim_score < sim_score_threshold:\n",
    "                perturbed_text = original_text\n",
    "        \n",
    "        if use.semantic_sim(original_text, codemixed_text) < sim_score_threshold:\n",
    "            codemixed_text = original_text\n",
    "        return codemixed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "ca6f4edd-2b8f-4a31-a38a-04f17f5ed503",
    "_uuid": "659d5919-b3ed-45a4-b635-2b7579d0f919",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n",
      "2022-02-20 14:32:53.619559: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib:/usr/local/cuda/lib\n",
      "2022-02-20 14:32:53.620414: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3197107/2773779248.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     main(\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mmodel_target\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"IndoBERT\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mdownstream_task\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3197107/2773779248.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(model_target, downstream_task, attack_strategy, perturbation_technique, perturb_ratio, num_sample, seed)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0muse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nModel initialization..\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Farid/utils/utils_semantic_use.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_growth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "def main(\n",
    "    model_target,\n",
    "    downstream_task,\n",
    "    attack_strategy,\n",
    "    perturbation_technique,\n",
    "    perturb_ratio,\n",
    "    num_sample,\n",
    "    seed=26092020\n",
    "):\n",
    "    set_seed(seed)\n",
    "\n",
    "    use = USE()\n",
    "\n",
    "    print(\"\\nModel initialization..\")\n",
    "    tokenizer, config, model = init_model(model_target)\n",
    "    \n",
    "    if downstream_task == \"sentiment\":\n",
    "        w2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\n",
    "        print(\"\\nLoading dataset..\")\n",
    "        train_dataset, train_loader = load_dataset_loader(downstream_task, 'train', tokenizer)\n",
    "        valid_dataset, valid_loader = load_dataset_loader(downstream_task, 'valid', tokenizer)\n",
    "        test_dataset, test_loader = load_dataset_loader(downstream_task, 'test', tokenizer)\n",
    "        \n",
    "        text0 = 'lokasi di alun alun masakan padang ini cukup terkenal dengan kepala ikan kakap gule , biasa saya pesan nasi bungkus padang berisikan rendang , ayam pop dan perkedel . porsi banyak dan mengenyangkan'\n",
    "        text1 = 'meski masa kampanye sudah selesai , bukan berati habis pula upaya mengerek tingkat kedipilihan elektabilitas .'\n",
    "        text2 = 'kamar nya sempit tidak ada tempat menyimpan barang malah menambah barang . by the way ini kipas2 mau diletakkan mana . mana uchiwa segede ini pula .'\n",
    "\n",
    "\n",
    "        print(\"\\nTest initial model on sample text..\")\n",
    "        text_logit(text0, model, tokenizer, i2w)\n",
    "        text_logit(text1, model, tokenizer, i2w)\n",
    "        text_logit(text2, model, tokenizer, i2w)\n",
    "        \n",
    "        finetuned_model = fine_tuning_model(model, i2w, train_loader, valid_loader, 5)\n",
    "        \n",
    "        print(\"\\nAttacking text using codemixing...\")\n",
    "        codemixed0 = attack(text0, 0, finetuned_model, tokenizer, perturb_ratio, 'jw', use)\n",
    "        codemixed1 = attack(text1, 1, finetuned_model, tokenizer, perturb_ratio, 'en', use)\n",
    "        codemixed2 = attack(text2, 2, finetuned_model, tokenizer, perturb_ratio, 'su', use)\n",
    "\n",
    "        print(\"\\nCalculating logit on codemixed data...\")\n",
    "        text_logit(codemixed0, finetuned_model, tokenizer, i2w)\n",
    "        text_logit(codemixed1, finetuned_model, tokenizer, i2w)\n",
    "        text_logit(codemixed2, finetuned_model, tokenizer, i2w)\n",
    "        \n",
    "        print(\"\\nCalculating similarity score...\")\n",
    "        print(use.semantic_sim(text0, codemixed0))\n",
    "        print(use.semantic_sim(text1, codemixed1))\n",
    "        print(use.semantic_sim(text2, codemixed2))\n",
    "        \n",
    "    elif downstream_task == \"emotion\":\n",
    "        w2i, i2w = EmotionDetectionDataset.LABEL2INDEX, EmotionDetectionDataset.INDEX2LABEL\n",
    "        train_dataset, train_loader = load_dataset_loader(downstream_task, 'train', tokenizer)\n",
    "        valid_dataset, valid_loader = load_dataset_loader(downstream_task, 'valid', tokenizer)\n",
    "        test_dataset, test_loader = load_dataset_loader(downstream_task, 'test', tokenizer)\n",
    "        \n",
    "        finetuned_model = fine_tuning_model(model, i2w, train_loader, valid_loader, 5)\n",
    "        \n",
    "        # prob_before = \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(\n",
    "        model_target=\"IndoBERT\",\n",
    "        downstream_task=\"sentiment\",\n",
    "        attack_strategy=\"codemixing\",\n",
    "        perturbation_technique=\"adversarial\",\n",
    "        perturb_ratio=0.2,\n",
    "        num_sample=0,\n",
    "        seed=26092020\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     set_seed(26092020)\n",
    "\n",
    "#     print(\"\\nModel initialization..\")\n",
    "#     tokenizer, config, model = init_model(\"IndoBERT\")\n",
    "#     w2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\n",
    "\n",
    "#     print(\"\\nLoading dataset..\")\n",
    "#     train_dataset, train_loader = load_dataset_loader('sentiment', 'train', tokenizer)\n",
    "#     valid_dataset, valid_loader = load_dataset_loader('sentiment', 'valid', tokenizer)\n",
    "#     test_dataset, test_loader = load_dataset_loader('sentiment', 'test', tokenizer)\n",
    "\n",
    "#     text0 = 'lokasi di alun alun masakan padang ini cukup terkenal dengan kepala ikan kakap gule , biasa saya pesan nasi bungkus padang berisikan rendang , ayam pop dan perkedel . porsi banyak dan mengenyangkan'\n",
    "#     text1 = 'meski masa kampanye sudah selesai , bukan berati habis pula upaya mengerek tingkat kedipilihan elektabilitas .'\n",
    "#     text2 = 'kamar nya sempit tidak ada tempat menyimpan barang malah menambah barang . by the way ini kipas2 mau diletakkan mana . mana uchiwa segede ini pula .'\n",
    "\n",
    "\n",
    "#     print(\"\\nTest initial model on sample text..\")\n",
    "#     text_logit(text0, model, tokenizer, i2w)\n",
    "#     text_logit(text1, model, tokenizer, i2w)\n",
    "#     text_logit(text2, model, tokenizer, i2w)\n",
    "\n",
    "#     print(\"\\nModel finetuning...\")\n",
    "#     finetuned_model = fine_tuning_model(model, i2w, train_loader, valid_loader, 5)\n",
    "#     del model\n",
    "    \n",
    "    \n",
    "#     print(\"\\nTest finetuned model on sample text..\")\n",
    "#     text_logit(text0, finetuned_model, tokenizer, i2w)\n",
    "#     text_logit(text1, finetuned_model, tokenizer, i2w)\n",
    "#     text_logit(text2, finetuned_model, tokenizer, i2w)\n",
    "\n",
    "#     print(\"\\nAttacking text using codemixing...\")\n",
    "#     codemixed0 = attack(text0,0, finetuned_model, tokenizer, 0.2, 'jw', use)\n",
    "#     codemixed1 = attack(text1,1, finetuned_model, tokenizer, 0.2, 'en', use)\n",
    "#     codemixed2 = attack(text2,2, finetuned_model, tokenizer, 0.2, 'su', use)\n",
    "\n",
    "#     print(\"\\nCalculating logit on codemixed data...\")\n",
    "#     text_logit(codemixed0, finetuned_model, tokenizer, i2w)\n",
    "#     text_logit(codemixed1, finetuned_model, tokenizer, i2w)\n",
    "#     text_logit(codemixed2, finetuned_model, tokenizer, i2w)\n",
    "\n",
    "#     print(\"\\nCalculating similarity score...\")\n",
    "#     print(use.semantic_sim(text0, codemixed0))\n",
    "#     print(use.semantic_sim(text1, codemixed1))\n",
    "#     print(use.semantic_sim(text2, codemixed2))\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nTest finetuned model on sample text..\")\n",
    "# text_logit(text0, finetuned_model, tokenizer, i2w)\n",
    "# text_logit(text1, finetuned_model, tokenizer, i2w)\n",
    "# text_logit(text2, finetuned_model, tokenizer, i2w)\n",
    "\n",
    "# print(\"\\nAttacking text using codemixing...\")\n",
    "# codemixed0 = attack(text0,0, finetuned_model, tokenizer, 0.2, 'jw', use)\n",
    "# codemixed1 = attack(text1,1, finetuned_model, tokenizer, 0.2, 'en', use)\n",
    "# codemixed2 = attack(text2,2, finetuned_model, tokenizer, 0.2, 'su', use)\n",
    "\n",
    "# print(\"\\nCalculating logit on codemixed data...\")\n",
    "# text_logit(codemixed0, finetuned_model, tokenizer, i2w)\n",
    "# text_logit(codemixed1, finetuned_model, tokenizer, i2w)\n",
    "# text_logit(codemixed2, finetuned_model, tokenizer, i2w)\n",
    "\n",
    "# print(\"\\nCalculating similarity score...\")\n",
    "# print(use.semantic_sim(text0, codemixed0))\n",
    "# print(use.semantic_sim(text1, codemixed1))\n",
    "# print(use.semantic_sim(text2, codemixed2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
