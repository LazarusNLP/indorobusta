{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "546ac2b4-f52d-4f77-8b42-4bc237320037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMOD_DIR=/usr/share/lmod/lmod/libexec/\n",
      "TMUX=/tmp/tmux-22086/default,1030980,0\n",
      "SSH_CLIENT=103.107.4.31 63325 22\n",
      "USER=m13518040\n",
      "LMOD_COLORIZE=yes\n",
      "MPLBACKEND=module://matplotlib_inline.backend_inline\n",
      "LMOD_PKG=/usr/share/lmod/lmod\n",
      "LD_LIBRARY_PATH=/usr/local/cuda/lib:/usr/local/cuda/lib\n",
      "HOME=/home/m13518040\n",
      "MOTD_SHOWN=pam\n",
      "OLDPWD=/raid/data/m13518040\n",
      "SSH_TTY=/dev/pts/2\n",
      "PAGER=cat\n",
      "LMOD_sys=Linux\n",
      "LOGNAME=m13518040\n",
      "TERM=xterm-color\n",
      "PATH=/home/m13518040/.local/bin:/usr/local/cuda/bin:/opt/bin/:/usr/local/cuda/bin:/home/m13518040/.local/bin:/usr/local/cuda/bin:/opt/bin/:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n",
      "LMOD_FULL_SETTARG_SUPPORT=no\n",
      "LMOD_PREPEND_BLOCK=normal\n",
      "LANG=en_US.UTF-8\n",
      "MODULEPATH_ROOT=/sw/modules\n",
      "SHELL=/bin/sh\n",
      "LMOD_CMD=/usr/share/lmod/lmod/libexec/lmod\n",
      "BASH_ENV=/usr/share/lmod/lmod/init/bash\n",
      "GIT_PAGER=cat\n",
      "PWD=/raid/data/m13518040/tugas-akhir-repository\n",
      "CLICOLOR=1\n",
      "SSH_CONNECTION=103.107.4.31 63325 167.205.35.247 22\n",
      "XDG_DATA_DIRS=/usr/local/share:/usr/share:/var/lib/snapd/desktop\n",
      "JPY_PARENT_PID=1034665\n",
      "PYDEVD_USE_FRAME_EVAL=NO\n",
      "LMOD_arch=x86_64\n",
      "MANPATH=/usr/share/lmod/lmod/share/man::\n",
      "MODULEPATH=/sw/modules/all\n",
      "TMUX_PANE=%0\n",
      "LMOD_SETTARG_CMD=:\n",
      "MODULESHOME=/usr/share/lmod/lmod\n",
      "CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "LMOD_DIR=/usr/share/lmod/lmod/libexec/\n",
      "TMUX=/tmp/tmux-22086/default,1030980,0\n",
      "SSH_CLIENT=103.107.4.31 63325 22\n",
      "USER=m13518040\n",
      "LMOD_COLORIZE=yes\n",
      "MPLBACKEND=module://matplotlib_inline.backend_inline\n",
      "LMOD_PKG=/usr/share/lmod/lmod\n",
      "LD_LIBRARY_PATH=/usr/local/cuda/lib:/usr/local/cuda/lib\n",
      "HOME=/home/m13518040\n",
      "MOTD_SHOWN=pam\n",
      "OLDPWD=/raid/data/m13518040\n",
      "SSH_TTY=/dev/pts/2\n",
      "PAGER=cat\n",
      "LMOD_sys=Linux\n",
      "LOGNAME=m13518040\n",
      "TERM=xterm-color\n",
      "PATH=/home/m13518040/.local/bin:/usr/local/cuda/bin:/opt/bin/:/usr/local/cuda/bin:/home/m13518040/.local/bin:/usr/local/cuda/bin:/opt/bin/:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n",
      "LMOD_FULL_SETTARG_SUPPORT=no\n",
      "LMOD_PREPEND_BLOCK=normal\n",
      "LANG=en_US.UTF-8\n",
      "MODULEPATH_ROOT=/sw/modules\n",
      "SHELL=/bin/sh\n",
      "LMOD_CMD=/usr/share/lmod/lmod/libexec/lmod\n",
      "BASH_ENV=/usr/share/lmod/lmod/init/bash\n",
      "GIT_PAGER=cat\n",
      "PWD=/raid/data/m13518040/tugas-akhir-repository\n",
      "CLICOLOR=1\n",
      "SSH_CONNECTION=103.107.4.31 63325 167.205.35.247 22\n",
      "XDG_DATA_DIRS=/usr/local/share:/usr/share:/var/lib/snapd/desktop\n",
      "JPY_PARENT_PID=1034665\n",
      "PYDEVD_USE_FRAME_EVAL=NO\n",
      "LMOD_arch=x86_64\n",
      "MANPATH=/usr/share/lmod/lmod/share/man::\n",
      "MODULEPATH=/sw/modules/all\n",
      "TMUX_PANE=%0\n",
      "LMOD_SETTARG_CMD=:\n",
      "MODULESHOME=/usr/share/lmod/lmod\n",
      "CUDA_VISIBLE_DEVICES=4\n"
     ]
    }
   ],
   "source": [
    "# !export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64\n",
    "!env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "!env CUDA_VISIBLE_DEVICES=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15edb277-e4c7-4c1b-998c-32c720b17121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils_init_model import text_logit, fine_tuning_model, eval_model, init_model, logit_prob, load_word_index\n",
    "import time\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# import jax.numpy as jnp\n",
    "# from jax import random\n",
    "\n",
    "import math\n",
    "# from .attack_helper import get_synonyms, codemix_perturbation, synonym_replacement, swap_minimum_importance_words, trans_dict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "stop_words_set = []\n",
    "for w in stopwords.words('indonesian'):\n",
    "    stop_words_set.append(w)\n",
    "\n",
    "from icecream import ic\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "import copy\n",
    "from deep_translator import GoogleTranslator\n",
    "import numpy as np\n",
    "import random\n",
    "# from utils.utils_semantic_use import USE\n",
    "from utils.utils_init_dataset import set_seed, load_dataset_loader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "# tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d543c47-539a-4917-aea4-6e26eea1c708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jax\n",
    "# !pip install --upgrade \"jax[cuda]\" -f https://storage.googleapis.com/jax-releases/jax_releases.html  # Note: wheels only available on linux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cef73d5-b954-43f7-b4b2-4b1d8b46cd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class USE(object):\n",
    "    def __init__(self):\n",
    "        import tensorflow_hub as hub\n",
    "        super(USE, self).__init__()\n",
    "        module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
    "        self.embed = hub.load(module_url)\n",
    "        # print(tf.executing_eagerly())\n",
    "        # self.embed = hub.load( DataManager.load(\"AttackAssist.UniversalSentenceEncoder\") )\n",
    "\n",
    "    def semantic_sim(self, sentA : str, sentB : str) -> float:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sentA: The first sentence.\n",
    "            sentB: The second sentence.\n",
    "\n",
    "        Returns:\n",
    "            Cosine distance between two sentences.\n",
    "        \n",
    "        \"\"\"\n",
    "        if sentA.lower() == sentB.lower():\n",
    "            return 1.000\n",
    "        ret = np.array(self.embed([sentA, sentB]).numpy())\n",
    "        # ret = self.embed([sentA, sentB]).numpy()\n",
    "        # ic(ret)\n",
    "        # .block_until_ready()  \n",
    "        score = ret[0].dot(ret[1]) / (np.linalg.norm(ret[0]) * np.linalg.norm(ret[1]))\n",
    "        if score>1:\n",
    "            return 1.000\n",
    "        return score\n",
    "        # return ret[0].dot(ret[1]) / (np.linalg.norm(ret[0]) * np.linalg.norm(ret[1])\n",
    "\n",
    "    def after_attack(self, input, adversarial_sample):\n",
    "        if adversarial_sample is not None:\n",
    "            return self.calc_score(input[\"x\"], adversarial_sample)\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "# USE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d653914f-8ae1-44af-b961-ba0fbc4dba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class USE(object):\n",
    "#     def __init__(self, module_url=None):\n",
    "#         super(USE, self).__init__()\n",
    "#         module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
    "#         self.embed = hub.load(module_url)\n",
    "        \n",
    "#         def embed(input):\n",
    "#             return model(input)\n",
    "        \n",
    "#         # config = tf.compat.v1.ConfigProto()\n",
    "#         # config.gpu_options.allow_growth = True\n",
    "#         # self.sess = tf.compat.v1.Session(config=config)\n",
    "#         # self.sess.run([tf.compat.v1.global_variables_initializer(), tf.compat.v1.tables_initializer()])\n",
    "\n",
    "#     def semantic_sim(self, sents1, sents2):\n",
    "#         # with tf.Session() as session:\n",
    "#         #     session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "#         #     with tf.device('GPU'):\n",
    "#         #         message_embeddings_ = session.run(self.embed([sents1, sents2]))\n",
    "        \n",
    "#         if sents1.lower() == sents2.lower():\n",
    "#             return 1.000\n",
    "        \n",
    "#         message_embeddings_ = self.embed([sents1, sents2])\n",
    "#         # message_embeddings_ = message_embeddings_.eval(session=self.sess)\n",
    "#         corr = np.tensordot(message_embeddings_, message_embeddings_, axes=(-1,-1))\n",
    "        \n",
    "#         if corr[0][1] > 1:\n",
    "#             return 1.000\n",
    "#         return corr[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3576aabe-dd88-40dd-8ff3-64d225667ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "def get_synonyms(word):\n",
    "    if word in stop_words_set:\n",
    "        return [word]\n",
    "\n",
    "    word_lemmas = wordnet.lemmas(word, lang=\"ind\")\n",
    "    \n",
    "    hypernyms = []\n",
    "    for lem in word_lemmas:\n",
    "        hypernyms.append(lem.synset().hypernyms())\n",
    "\n",
    "    if not any(hypernyms):\n",
    "        return [word]\n",
    "    \n",
    "    lemma_corp = []\n",
    "    \n",
    "    for hypernym in hypernyms:\n",
    "        if(len(hypernym) < 1):\n",
    "            continue\n",
    "        else:\n",
    "            lemma_corp.append(hypernym[0].lemmas(lang=\"ind\"))\n",
    "            \n",
    "    lemmas = set()\n",
    "    for list_lemmas in lemma_corp:\n",
    "        if(len(list_lemmas) < 1):\n",
    "            lemmas.add(word)\n",
    "        else:\n",
    "            for l in list_lemmas:\n",
    "                lemmas.add(l.name())\n",
    "    \n",
    "    clean_synonyms = set()\n",
    "    for syn in lemmas.copy():\n",
    "        synonym = syn.replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "        synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "        clean_synonyms.add(synonym) \n",
    "    if word in clean_synonyms:\n",
    "        clean_synonyms.remove(word)\n",
    "    \n",
    "    if len(list(clean_synonyms)) < 1:\n",
    "        return [word]\n",
    "    else:\n",
    "        return list(clean_synonyms)\n",
    "\n",
    "def codemix_perturbation(words, target_lang, words_perturb):\n",
    "    \"\"\"\n",
    "    'su': 'sundanese'\n",
    "    'jw': 'javanese'\n",
    "    'ms': 'malay'\n",
    "    'en': 'english'\n",
    "    \"\"\"\n",
    "    translator = GoogleTranslator(source=\"id\", target=target_lang)\n",
    "    new_wp = []\n",
    "    for wp in words_perturb:\n",
    "        if wp[1].isalpha():\n",
    "            new_wp.append(wp[1])\n",
    "    \n",
    "    if len(new_wp) == 1:\n",
    "        new_wp_trans = dict(zip(new_wp, translator.translate(new_wp[0])))\n",
    "    elif len(new_wp) == 0:\n",
    "        return ' '.join(new_words), {}\n",
    "        \n",
    "    new_wp_trans = dict(zip(new_wp, translator.translate_batch(new_wp)))\n",
    "    \n",
    "    supported_langs = [\"su\", \"jw\", \"ms\", \"en\", \"fr\", \"it\"]\n",
    "    \n",
    "    if target_lang not in supported_langs:\n",
    "        raise ValueError('Language Unavailable')\n",
    "    \n",
    "    new_words = words.copy()\n",
    "    \n",
    "    if len(words_perturb) >= 1:\n",
    "        for perturb_word in new_wp_trans.keys():\n",
    "            new_words = [new_wp_trans[perturb_word] if word == perturb_word and word.isalpha() else word for word in new_words]\n",
    "\n",
    "    return ' '.join(new_words), new_wp_trans\n",
    "\n",
    "def synonym_replacement(words, words_perturb):    \n",
    "    # new_words = words.copy()\n",
    "    new_wp = []\n",
    "    for wp in words_perturb:\n",
    "        if wp[1].isalpha():\n",
    "            new_wp.append(wp[1])\n",
    "    \n",
    "    if len(new_wp) == 1:\n",
    "        new_wp_trans = dict(zip(new_wp, get_synonyms(new_wp[0])[0]))\n",
    "    elif len(new_wp) == 0:\n",
    "        return ' '.join(new_words), {}\n",
    "    \n",
    "    wp_trans=[]\n",
    "    for wp in new_wp:\n",
    "        wp_trans.append(get_synonyms(wp)[0])\n",
    "    \n",
    "    new_wp_trans = dict(zip(new_wp, wp_trans))\n",
    "    \n",
    "    new_words = words.copy()\n",
    "    \n",
    "    if len(words_perturb) >= 1:\n",
    "        for perturb_word in new_wp_trans.keys():\n",
    "            new_words = [new_wp_trans[perturb_word[1]] if word == perturb_word[1] and word.isalpha() else word for word in new_words]\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "    \n",
    "    return sentence, new_wp_trans\n",
    "\n",
    "# fungsi untuk mencari kandidat lain ketika sebuah kandidat perturbasi kurang dari sim_score_threshold\n",
    "def swap_minimum_importance_words(words_perturb, top_importance_words):\n",
    "    print(\"pointer\")\n",
    "    def get_minimums(word_tups):\n",
    "        arr = []\n",
    "        for wt in word_tups:\n",
    "            if wt[2] == min(top_importance_words, key = lambda t: t[2])[2]:\n",
    "                arr.append(wt)\n",
    "        return arr\n",
    "    \n",
    "    # get list of words with minimum importance score\n",
    "    minimum_import = get_minimums(top_importance_words)\n",
    "    unlisted = list(set(words_perturb).symmetric_difference(set(top_importance_words)))\n",
    "    \n",
    "    ic(unlisted)\n",
    "    \n",
    "    len_wp = len(top_importance_words)\n",
    "    len_ul = len(unlisted)\n",
    "    \n",
    "    res = []\n",
    "    for i in range(len_wp):\n",
    "        if top_importance_words[i] in minimum_import:\n",
    "            temp_wp = list(copy.deepcopy(top_importance_words))\n",
    "            if len(temp_wp) == 0:\n",
    "                break\n",
    "            \n",
    "            swapped_wp = np.array([(temp_wp) for i in range(len_ul)])\n",
    "            \n",
    "            ic(swapped_wp)\n",
    "            for j in range(len(swapped_wp)):\n",
    "                temp_sm = np.vstack((swapped_wp[j], tuple(unlisted[j])))\n",
    "                \n",
    "                res.append(temp_sm.tolist())\n",
    "            temp_wp.pop(i)\n",
    "                \n",
    "    return res\n",
    "\n",
    "def trans_dict(wordlist, perturbation_technique, lang=None):\n",
    "    translated_wordlist = {}\n",
    "    # ic(wordlist)\n",
    "    if perturbation_technique == \"codemixing\":\n",
    "        translator = GoogleTranslator(source=\"id\", target=lang)\n",
    "        \n",
    "        for word in wordlist:\n",
    "            if word.isalpha():\n",
    "                translated_wordlist[word] = translator.translate(word)\n",
    "            else:\n",
    "                translated_wordlist[word] = word\n",
    "    else:\n",
    "        for word in wordlist:\n",
    "            translated_wordlist[word] = get_synonyms(word)[0]\n",
    "            \n",
    "    return translated_wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c0efbfd-7e66-4e0b-8f0f-3e7bb95904fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack(text_ls,\n",
    "           true_label,\n",
    "           predictor,\n",
    "           tokenizer,\n",
    "           att_ratio,\n",
    "           perturbation_technique,\n",
    "           lang_codemix=None,\n",
    "           sim_predictor=None,\n",
    "           sim_score_threshold=0,\n",
    "           sim_score_window=15,\n",
    "           batch_size=32, \n",
    "           import_score_threshold=-1.):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    label_dict = {\n",
    "        'positive': 0, \n",
    "        'neutral': 1, \n",
    "        'negative': 2}\n",
    "    \n",
    "    original_text = text_ls\n",
    "    orig_label, orig_probs, orig_prob = logit_prob(text_ls, predictor, tokenizer)\n",
    "    \n",
    "    if len(original_text.split()) < sim_score_window:\n",
    "        sim_score_threshold = 0.1  \n",
    "    \n",
    "#     SEK SALAAHHHHHH\n",
    "    if true_label != orig_label:\n",
    "        running_time = round(time.time() - start_time, 2)\n",
    "        # perturbed_text, perturbed_semantic_sim, orig_label, orig_probs, perturbed_label, perturbed_probs, translated_words, running_time\n",
    "        orig_probs = np.round(orig_probs.detach().cpu().numpy().tolist(),4)\n",
    "        return original_text, 1.000, orig_label, orig_probs, orig_label, orig_probs,'', running_time\n",
    "    else:\n",
    "        text_ls = original_text.split()\n",
    "        text_ls = [word for word in text_ls if word.isalnum()]\n",
    "        len_text = len(text_ls)\n",
    "        ic(len_text)\n",
    "        half_sim_score_window = (sim_score_window - 1) // 2\n",
    "        # num_queries = 1\n",
    "        \n",
    "        leave_1_texts = [' '.join(text_ls[:ii] + [tokenizer.mask_token] + text_ls[min(ii + 1, len_text):]) for ii in range(len_text)]\n",
    "                \n",
    "        leave_1_probs = []\n",
    "        leave_1_probs_argmax = []\n",
    "        # num_queries += len(leave_1_texts)\n",
    "        \n",
    "# torch.inference mode\n",
    "        for text_leave_1 in leave_1_texts:\n",
    "            orig_label_leave_1, orig_probs_leave_1, orig_prob_leave_1 = logit_prob(text_leave_1, predictor, tokenizer)\n",
    "            leave_1_probs.append(orig_probs_leave_1.detach().cpu().numpy())\n",
    "            leave_1_probs_argmax.append(orig_label_leave_1)\n",
    "        \n",
    "        running_time_a = round(time.time() - start_time, 2)\n",
    "        ic(running_time_a)\n",
    "        \n",
    "        start_time_conv = time.time()\n",
    "        leave_1_probs = torch.tensor(leave_1_probs).to(\"cuda:0\")\n",
    "        end_time_conv = round(time.time() - start_time_conv, 2)\n",
    "        ic(end_time_conv)\n",
    "        \n",
    "        orig_prob_extended=np.empty(len_text)\n",
    "        orig_prob_extended.fill(orig_prob)\n",
    "        orig_prob_extended = torch.tensor(orig_prob_extended).to(\"cuda:0\")\n",
    "        \n",
    "        arr1 = orig_prob_extended - leave_1_probs[:,orig_label] + float(leave_1_probs_argmax != orig_label)\n",
    "        arr2 = (leave_1_probs.max(dim=-1)[0].to(\"cuda:0\") - orig_probs[leave_1_probs_argmax].to(\"cuda:0\"))\n",
    "        \n",
    "        import_scores = arr1*arr2\n",
    "        import_scores = [im * -1 for im in import_scores]\n",
    "        \n",
    "        words_perturb = []\n",
    "        for idx, score in sorted(enumerate(import_scores), key=lambda x: x[1], reverse=True):\n",
    "            try:\n",
    "                if score > import_score_threshold and text_ls[idx] not in stop_words_set:\n",
    "                    words_perturb.append((idx, text_ls[idx], score.item()))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(idx, len(text_ls), import_scores.shape, text_ls, len(leave_1_texts))\n",
    "        \n",
    "        num_perturbation = math.floor(len(words_perturb)*att_ratio)\n",
    "        \n",
    "        if num_perturbation < 1:\n",
    "            num_perturbation = 1\n",
    "        \n",
    "#       top words perturb berisi list kata terpenting yang tidak akan diswitch ketika first_codemix_sim_score < sim_score_threshold\n",
    "        top_words_perturb = words_perturb[:num_perturbation]\n",
    "        trans_word = [twp[1] for twp in top_words_perturb]\n",
    "\n",
    "        if perturbation_technique == \"codemixing\":\n",
    "            perturbed_text,translated_words = codemix_perturbation(text_ls, lang_codemix, top_words_perturb)\n",
    "        elif perturbation_technique == \"synonym_replacement\":\n",
    "            perturbed_text,translated_words = synonym_replacement(text_ls, top_words_perturb)\n",
    "        \n",
    "        start_time_use = time.time()\n",
    "        first_perturbation_sim_score = sim_predictor.semantic_sim(original_text, perturbed_text)\n",
    "        end_time_use = round(time.time() - start_time, 2)\n",
    "        \n",
    "        ic(end_time_use)\n",
    "        \n",
    "        ic(first_perturbation_sim_score)\n",
    "        \n",
    "        words_perturb_candidates = []\n",
    "        if first_perturbation_sim_score < sim_score_threshold:\n",
    "            words_perturb_candidates.append(top_words_perturb)\n",
    "            swapped = swap_minimum_importance_words(words_perturb, top_words_perturb)\n",
    "            for s in swapped:\n",
    "                words_perturb_candidates.append(s)\n",
    "\n",
    "            words_perturb_candidates = [[tuple(w) for w in wpc] for wpc in words_perturb_candidates]\n",
    "\n",
    "            candidate_comparison = {}\n",
    "            for wpc in words_perturb_candidates:\n",
    "                \n",
    "                if perturbation_technique == \"codemixing\":\n",
    "#                     bisa dicache, gaperlu request satu2\n",
    "                    perturbed_candidate,translated_words = codemix_perturbation(text_ls, lang_codemix, wpc)\n",
    "                elif perturbation_technique == \"synonym_replacement\":\n",
    "                    perturbed_candidate,translated_words = synonym_replacement(text_ls, wpc)\n",
    "                    \n",
    "                perturbed_candidate_sim_score = sim_predictor.semantic_sim(original_text, perturbed_candidate)\n",
    "                \n",
    "                candidate_comparison[perturbed_candidate] = (perturbed_candidate_sim_score, wpc[-1][-1], wpc)\n",
    "\n",
    "            sorted_candidate_comparison = sorted(candidate_comparison.keys(), key=lambda x: (float(candidate_comparison[x][0]), float(candidate_comparison[x][1])), reverse=True)\n",
    "\n",
    "            perturbed_text = sorted_candidate_comparison[0]\n",
    "            \n",
    "            top_words_perturb = candidate_comparison[sorted_candidate_comparison[0]][-1]\n",
    "            trans_word = [twp[1] for twp in top_words_perturb]\n",
    "            \n",
    "        perturbed_semantic_sim = sim_predictor.semantic_sim(original_text, perturbed_text)\n",
    "        if perturbed_semantic_sim < sim_score_threshold:\n",
    "            perturbed_text = original_text\n",
    "            perturbed_semantic_sim = 1.000\n",
    "        \n",
    "        perturbed_label, perturbed_probs, perturbed_prob = logit_prob(perturbed_text, predictor, tokenizer)\n",
    "        \n",
    "        \n",
    "        # translated_words = trans_dict(trans_word, perturbation_technique, lang_codemix)\n",
    "        \n",
    "        orig_probs = np.round(orig_probs.detach().cpu().numpy().tolist(),4)\n",
    "        perturbed_probs = np.round(perturbed_probs.detach().cpu().numpy().tolist(),4)\n",
    "        \n",
    "        running_time = round(time.time() - start_time, 2)\n",
    "        \n",
    "        # ic(perturbed_text, perturbed_semantic_sim, orig_label, orig_probs, perturbed_label, perturbed_probs, translated_words, running_time)\n",
    "        return perturbed_text, perturbed_semantic_sim, orig_label, orig_probs, perturbed_label, perturbed_probs, translated_words, running_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efa9058f-8e82-419e-a12f-0e2abf856df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_target=\"IndoBERT\"\n",
    "downstream_task=\"sentiment\"\n",
    "attack_strategy=\"adversarial\"\n",
    "# finetune_epoch=1\n",
    "num_sample=50\n",
    "# exp_name=\n",
    "perturbation_technique=\"codemixing\"\n",
    "perturb_ratio=0.4\n",
    "perturb_lang=\"en\"\n",
    "seed=26092020\n",
    "dataset=\"valid\"\n",
    "\n",
    "set_seed(seed)\n",
    "use = USE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0130c75f-d6be-4709-8b79-6b0137067a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, config, finetuned_model = init_model(model_target, downstream_task, seed)\n",
    "w2i, i2w = load_word_index(downstream_task)\n",
    "\n",
    "train_dataset, train_loader, train_path = load_dataset_loader(downstream_task, 'train', tokenizer)\n",
    "valid_dataset, valid_loader, valid_path = load_dataset_loader(downstream_task, 'valid', tokenizer)\n",
    "test_dataset, test_loader, test_path = load_dataset_loader(downstream_task, 'test', tokenizer)\n",
    "\n",
    "finetuned_model.to(device)\n",
    "# finetuned_model = fine_tuning_model(model, i2w, train_loader, valid_loader, finetune_epoch)\n",
    "\n",
    "if dataset == \"valid\":\n",
    "    exp_dataset = valid_dataset.load_dataset(valid_path)[380:430]\n",
    "elif dataset == \"train\":\n",
    "    exp_dataset = train_dataset.load_dataset(train_path)\n",
    "# exp_dataset = dd.from_pandas(exp_dataset, npartitions=10)\n",
    "# exp_dataset = te.DataFrame.from_pandas(exp_dataset)\n",
    "text,label = None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecfde42-e908-43db-8383-ba3106ec701f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                        | 0/50 [00:00<?, ?it/s]ic| len_text: 5\n",
      "ic| running_time_a: 0.28\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 1.02\n",
      "ic| first_perturbation_sim_score: 1.0\n",
      "  4%|██▌                                                             | 2/50 [00:01<00:29,  1.65it/s]ic| len_text: 13\n",
      "ic| running_time_a: 0.54\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 2.66\n",
      "ic| first_perturbation_sim_score: 0.7537577\n",
      "  6%|███▊                                                            | 3/50 [00:04<01:11,  1.52s/it]ic| len_text: 17\n",
      "ic| running_time_a: 0.59\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 1.46\n",
      "ic| first_perturbation_sim_score: 0.94139946\n",
      "  8%|█████                                                           | 4/50 [00:05<01:13,  1.59s/it]ic| len_text: 12\n",
      "ic| running_time_a: 0.46\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 1.13\n",
      "ic| first_perturbation_sim_score: 0.7428225\n",
      " 10%|██████▍                                                         | 5/50 [00:07<01:07,  1.49s/it]ic| len_text: 26\n",
      "ic| running_time_a: 1.04\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 2.81\n",
      "ic| first_perturbation_sim_score: 0.80816007\n",
      " 12%|███████▋                                                        | 6/50 [00:09<01:26,  1.97s/it]ic| len_text: 22\n",
      "ic| running_time_a: 0.8\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 3.61\n",
      "ic| first_perturbation_sim_score: 0.78777\n",
      " 14%|████████▉                                                       | 7/50 [00:13<01:48,  2.53s/it]ic| len_text: 29\n",
      "ic| running_time_a: 1.14\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 3.11\n",
      "ic| first_perturbation_sim_score: 0.76903087\n",
      " 16%|██████████▏                                                     | 8/50 [00:16<01:56,  2.77s/it]ic| len_text: 1\n",
      "ic| running_time_a: 0.09\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 1.1\n",
      "ic| first_perturbation_sim_score: 0.2863596\n",
      " 18%|███████████▌                                                    | 9/50 [00:18<01:34,  2.30s/it]ic| len_text: 18\n",
      "ic| running_time_a: 0.77\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 3.23\n",
      "ic| first_perturbation_sim_score: 0.94151896\n",
      " 22%|█████████████▊                                                 | 11/50 [00:21<01:19,  2.03s/it]ic| len_text: 15\n",
      "ic| running_time_a: 0.59\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 2.16\n",
      "ic| first_perturbation_sim_score: 0.51143086\n",
      " 24%|███████████████                                                | 12/50 [00:23<01:19,  2.10s/it]ic| len_text: 36\n",
      "ic| running_time_a: 1.42\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 3.01\n",
      "ic| first_perturbation_sim_score: 0.9258507\n",
      " 26%|████████████████▍                                              | 13/50 [00:27<01:28,  2.39s/it]ic| len_text: 22\n",
      "ic| running_time_a: 0.79\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 2.26\n",
      "ic| first_perturbation_sim_score: 0.8358933\n",
      " 28%|█████████████████▋                                             | 14/50 [00:29<01:26,  2.40s/it]ic| len_text: 64\n",
      "ic| running_time_a: 2.47\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 7.07\n",
      "ic| first_perturbation_sim_score: 0.70984495\n",
      " 30%|██████████████████▉                                            | 15/50 [00:36<02:10,  3.74s/it]ic| len_text: 5\n",
      "ic| running_time_a: 0.17\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 1.42\n",
      "ic| first_perturbation_sim_score: 1.0\n",
      " 32%|████████████████████▏                                          | 16/50 [00:38<01:46,  3.13s/it]ic| len_text: 34\n",
      "ic| running_time_a: 1.26\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 5.1\n",
      "ic| first_perturbation_sim_score: 0.9401686\n",
      " 34%|█████████████████████▍                                         | 17/50 [00:43<02:03,  3.75s/it]ic| len_text: 20\n",
      "ic| running_time_a: 0.78\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 3.24\n",
      "ic| first_perturbation_sim_score: 0.83014596\n",
      " 36%|██████████████████████▋                                        | 18/50 [00:47<01:56,  3.65s/it]ic| len_text: 18\n",
      "ic| running_time_a: 0.73\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 3.33\n",
      "ic| first_perturbation_sim_score: 0.8388577\n",
      " 38%|███████████████████████▉                                       | 19/50 [00:50<01:51,  3.60s/it]ic| len_text: 27\n",
      "ic| running_time_a: 1.07\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 3.05\n",
      "ic| first_perturbation_sim_score: 0.83448243\n",
      " 40%|█████████████████████████▏                                     | 20/50 [00:53<01:45,  3.50s/it]ic| len_text: 21\n",
      "ic| running_time_a: 0.66\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 1.59\n",
      "ic| first_perturbation_sim_score: 0.90392625\n",
      " 42%|██████████████████████████▍                                    | 21/50 [00:55<01:26,  2.98s/it]ic| len_text: 55\n",
      "ic| running_time_a: 2.16\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 6.71\n",
      "ic| first_perturbation_sim_score: 0.82226336\n",
      " 44%|███████████████████████████▋                                   | 22/50 [01:02<01:55,  4.14s/it]ic| len_text: 38\n",
      "ic| running_time_a: 1.35\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 4.64\n",
      "ic| first_perturbation_sim_score: 0.85116756\n",
      " 46%|████████████████████████████▉                                  | 23/50 [01:07<01:56,  4.33s/it]ic| len_text: 38\n",
      "ic| running_time_a: 1.44\n",
      "ic| end_time_conv: 0.01\n",
      "ic| end_time_use: 3.86\n",
      "ic| first_perturbation_sim_score: 0.89601016\n",
      " 48%|██████████████████████████████▏                                | 24/50 [01:11<01:49,  4.23s/it]ic| len_text: 20\n",
      "ic| running_time_a: 1.04\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 2.95\n",
      "ic| first_perturbation_sim_score: 0.820675\n",
      " 50%|███████████████████████████████▌                               | 25/50 [01:14<01:37,  3.89s/it]ic| len_text: 15\n",
      "ic| running_time_a: 0.64\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 1.63\n",
      "ic| first_perturbation_sim_score: 0.7986703\n",
      " 52%|████████████████████████████████▊                              | 26/50 [01:16<01:18,  3.26s/it]ic| len_text: 46\n",
      "ic| running_time_a: 1.81\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 4.23\n",
      "ic| first_perturbation_sim_score: 0.86029136\n",
      " 54%|██████████████████████████████████                             | 27/50 [01:20<01:23,  3.62s/it]ic| len_text: 32\n",
      "ic| running_time_a: 1.4\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 3.27\n",
      "ic| first_perturbation_sim_score: 0.9503614\n",
      " 56%|███████████████████████████████████▎                           | 28/50 [01:23<01:17,  3.54s/it]ic| len_text: 32\n",
      "ic| running_time_a: 1.2\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 3.06\n",
      "ic| first_perturbation_sim_score: 0.9245075\n",
      " 58%|████████████████████████████████████▌                          | 29/50 [01:27<01:12,  3.45s/it]ic| len_text: 32\n",
      "ic| running_time_a: 1.18\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 3.05\n",
      "ic| first_perturbation_sim_score: 0.9386984\n",
      " 60%|█████████████████████████████████████▊                         | 30/50 [01:30<01:07,  3.37s/it]ic| len_text: 35\n",
      "ic| running_time_a: 1.32\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 4.15\n",
      "ic| first_perturbation_sim_score: 0.883024\n",
      " 64%|████████████████████████████████████████▎                      | 32/50 [01:34<00:50,  2.81s/it]ic| len_text: 49\n",
      "ic| running_time_a: 2.07\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 3.92\n",
      "ic| first_perturbation_sim_score: 0.9822756\n",
      " 66%|█████████████████████████████████████████▌                     | 33/50 [01:38<00:53,  3.12s/it]ic| len_text: 29\n",
      "ic| running_time_a: 0.82\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 2.77\n",
      "ic| first_perturbation_sim_score: 0.91663724\n",
      " 70%|████████████████████████████████████████████                   | 35/50 [01:41<00:36,  2.44s/it]ic| len_text: 25\n",
      "ic| running_time_a: 0.97\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 2.39\n",
      "ic| first_perturbation_sim_score: 0.89728945\n",
      " 72%|█████████████████████████████████████████████▎                 | 36/50 [01:44<00:34,  2.46s/it]ic| len_text: 22\n",
      "ic| running_time_a: 0.87\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 2.14\n",
      "ic| first_perturbation_sim_score: 0.84611785\n",
      " 74%|██████████████████████████████████████████████▌                | 37/50 [01:46<00:31,  2.41s/it]ic| len_text: 18\n",
      "ic| running_time_a: 0.61\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 1.88\n",
      "ic| first_perturbation_sim_score: 0.85740626\n",
      " 76%|███████████████████████████████████████████████▉               | 38/50 [01:48<00:27,  2.31s/it]ic| len_text: 59\n",
      "ic| running_time_a: 1.99\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 7.39\n",
      "ic| first_perturbation_sim_score: 0.76940155\n",
      " 78%|█████████████████████████████████████████████████▏             | 39/50 [01:55<00:41,  3.73s/it]ic| len_text: 65\n",
      "ic| running_time_a: 2.61\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 6.62\n",
      "ic| first_perturbation_sim_score: 0.88010395\n",
      " 80%|██████████████████████████████████████████████████▍            | 40/50 [02:02<00:45,  4.57s/it]ic| len_text: 24\n",
      "ic| running_time_a: 0.97\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 3.09\n",
      "ic| first_perturbation_sim_score: 0.7763792\n",
      " 82%|███████████████████████████████████████████████████▋           | 41/50 [02:05<00:37,  4.20s/it]ic| len_text: 23\n",
      "ic| running_time_a: 1.1\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 2.67\n",
      "ic| first_perturbation_sim_score: 0.72316724\n",
      " 84%|████████████████████████████████████████████████████▉          | 42/50 [02:08<00:30,  3.80s/it]ic| len_text: 14\n",
      "ic| running_time_a: 0.69\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 1.21\n",
      "ic| first_perturbation_sim_score: 0.88281155\n",
      " 86%|██████████████████████████████████████████████████████▏        | 43/50 [02:10<00:21,  3.09s/it]ic| len_text: 61\n",
      "ic| running_time_a: 2.47\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 8.25\n",
      "ic| first_perturbation_sim_score: 0.8760915\n",
      " 88%|███████████████████████████████████████████████████████▍       | 44/50 [02:18<00:28,  4.67s/it]ic| len_text: 30\n",
      "ic| running_time_a: 1.32\n",
      "ic| end_time_conv: 0.0\n"
     ]
    }
   ],
   "source": [
    "if downstream_task == 'sentiment':\n",
    "    text = 'text'\n",
    "    label = 'sentiment'\n",
    "    exp_dataset[['perturbed_text', 'perturbed_semantic_sim', 'pred_label', 'pred_proba', 'perturbed_label', 'perturbed_proba', 'translated_word(s)', 'running_time(s)']] = exp_dataset.progress_apply(\n",
    "        lambda row: attack(\n",
    "            text_ls = row.text,\n",
    "            true_label = row.sentiment,\n",
    "            predictor = finetuned_model,\n",
    "            tokenizer = tokenizer, \n",
    "            att_ratio = perturb_ratio,\n",
    "            perturbation_technique = perturbation_technique,\n",
    "            lang_codemix = perturb_lang,\n",
    "            sim_predictor = use), axis=1, result_type='expand'\n",
    "    )\n",
    "elif downstream_task == 'emotion':\n",
    "    text = 'tweet'\n",
    "    label = 'label'\n",
    "    exp_dataset[['perturbed_text', 'perturbed_semantic_sim', 'pred_label', 'pred_proba', 'perturbed_label', 'perturbed_proba', 'translated_word(s)', 'running_time(s)']] = exp_dataset.progress_apply(\n",
    "        lambda row: attack(\n",
    "            text_ls = row.tweet.values,\n",
    "            true_label = row.label.values,\n",
    "            predictor = finetuned_model,\n",
    "            tokenizer = tokenizer, \n",
    "            att_ratio = perturb_ratio,\n",
    "            perturbation_technique = perturbation_technique,\n",
    "            lang_codemix = perturb_lang,\n",
    "            sim_predictor = use), axis=1, result_type='expand'\n",
    "    )\n",
    "\n",
    "before_attack = accuracy_score(exp_dataset[label], exp_dataset['pred_label'])\n",
    "after_attack = accuracy_score(exp_dataset[label], exp_dataset['perturbed_label'])\n",
    "\n",
    "exp_dataset.loc[exp_dataset.index[0], 'before_attack_acc'] = before_attack\n",
    "exp_dataset.loc[exp_dataset.index[0], 'after_attack_acc'] = after_attack\n",
    "exp_dataset.loc[exp_dataset.index[0], 'avg_semantic_sim'] = exp_dataset[\"perturbed_semantic_sim\"].mean()\n",
    "exp_dataset.loc[exp_dataset.index[0], 'avg_running_time(s)'] = exp_dataset[\"running_time(s)\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe121c30-5649-4277-8805-0a7f3c59d6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>perturbed_text</th>\n",
       "      <th>perturbed_semantic_sim</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>pred_proba</th>\n",
       "      <th>perturbed_label</th>\n",
       "      <th>perturbed_proba</th>\n",
       "      <th>translated_word(s)</th>\n",
       "      <th>running_time(s)</th>\n",
       "      <th>before_attack_acc</th>\n",
       "      <th>after_attack_acc</th>\n",
       "      <th>avg_semantic_sim</th>\n",
       "      <th>avg_running_time(s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>mau marah sama koneksi indosat</td>\n",
       "      <td>2</td>\n",
       "      <td>mau marah sama koneksi Indosat</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0005, 0.0015, 0.998]</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0005, 0.0015, 0.998]</td>\n",
       "      <td>{'indosat': 'Indosat'}</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.851869</td>\n",
       "      <td>3.4364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>pemancing peliharaan , nanti kasus nya kek ang...</td>\n",
       "      <td>2</td>\n",
       "      <td>pemancing peliharaan nanti kasus his cake angg...</td>\n",
       "      <td>0.753758</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0007, 0.0079, 0.9914]</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0023, 0.3216, 0.6761]</td>\n",
       "      <td>{'kek': 'cake', 'nya': 'his', 'fpi': 'fpi'}</td>\n",
       "      <td>2.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>anies itu tidak ada beda nya sama pemimpin-pem...</td>\n",
       "      <td>2</td>\n",
       "      <td>anies itu tidak ada beda nya sama hypocrite se...</td>\n",
       "      <td>0.941399</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0135, 0.0053, 0.9812]</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.123, 0.0149, 0.8621]</td>\n",
       "      <td>{'aus': 'wear out', 'munafiq': 'hypocrite'}</td>\n",
       "      <td>1.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>sebelum mencoblos , sudah seharusnya kita memp...</td>\n",
       "      <td>1</td>\n",
       "      <td>sebelum to vote sudah seharusnya kita mempelaj...</td>\n",
       "      <td>0.742823</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.003, 0.9809, 0.0162]</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0258, 0.9323, 0.0419]</td>\n",
       "      <td>{'kandidat': 'candidate', 'mencoblos': 'to vote'}</td>\n",
       "      <td>1.28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>menikmati salah satu teh otentik khas jawa bar...</td>\n",
       "      <td>0</td>\n",
       "      <td>menikmati salah satu tea otentik khas jawa wes...</td>\n",
       "      <td>0.808160</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9963, 0.0028, 0.0008]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9921, 0.0069, 0.001]</td>\n",
       "      <td>{'nya': 'his', 'enak': 'nice', 'juara': 'champ...</td>\n",
       "      <td>2.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>hasil quick count pilkada kota makassar yang d...</td>\n",
       "      <td>1</td>\n",
       "      <td>hasil quick count pilkada kota Makassar yang d...</td>\n",
       "      <td>0.787770</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0036, 0.9958, 0.0006]</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0101, 0.989, 0.0009]</td>\n",
       "      <td>{'tunggal': 'single', 'calon': 'candidate', 'k...</td>\n",
       "      <td>3.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>makan di burangrang kafe vip lantai 2 , makana...</td>\n",
       "      <td>0</td>\n",
       "      <td>makan di burangrang kafe vip floor 2 eat his n...</td>\n",
       "      <td>0.769031</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9982, 0.0014, 0.0004]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9937, 0.0056, 0.0007]</td>\n",
       "      <td>{'enak': 'nice', 'nya': 'his', 'suka': 'like',...</td>\n",
       "      <td>3.28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>lucu</td>\n",
       "      <td>0</td>\n",
       "      <td>funny</td>\n",
       "      <td>0.286360</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9945, 0.0015, 0.004]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9941, 0.0045, 0.0014]</td>\n",
       "      <td>{'lucu': 'funny'}</td>\n",
       "      <td>1.27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>mari terus upayakan perubahan , perubahan ke a...</td>\n",
       "      <td>0</td>\n",
       "      <td>mari terus upayakan perubahan , perubahan ke a...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0672, 0.9294, 0.0034]</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0672, 0.9294, 0.0034]</td>\n",
       "      <td></td>\n",
       "      <td>0.03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>kita pakai biznet dah mahal juga banyak masala...</td>\n",
       "      <td>2</td>\n",
       "      <td>kita pakai biznet bye expensive juga banyak ma...</td>\n",
       "      <td>0.941519</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0535, 0.0301, 0.9165]</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.2063, 0.284, 0.5097]</td>\n",
       "      <td>{'mahal': 'expensive', 'biznet': 'biznet', 'da...</td>\n",
       "      <td>3.39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>semua makanan nya enak . pelayanan nya benar-b...</td>\n",
       "      <td>0</td>\n",
       "      <td>semua makanan his nice pelayanan his excellent...</td>\n",
       "      <td>0.511431</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9981, 0.0011, 0.0007]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.99, 0.0062, 0.0039]</td>\n",
       "      <td>{'sunda': 'Sunda', 'enak': 'nice', 'nya': 'his...</td>\n",
       "      <td>2.31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>liburan yang menyenangkan dengan banyak aktifi...</td>\n",
       "      <td>0</td>\n",
       "      <td>liburan yang pleasant dengan banyak aktifitas ...</td>\n",
       "      <td>0.925851</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9968, 0.0024, 0.0007]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9956, 0.003, 0.0013]</td>\n",
       "      <td>{'menyenangkan': 'pleasant', 'makanan': 'food'...</td>\n",
       "      <td>3.19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>tempat nya asik banget , dingin juga , dan mak...</td>\n",
       "      <td>0</td>\n",
       "      <td>tempat his asik banget dingin juga dan makanan...</td>\n",
       "      <td>0.835893</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9984, 0.0011, 0.0004]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9977, 0.0015, 0.0008]</td>\n",
       "      <td>{'baju': 'dress', 'toko': 'shop', 'enak': 'nic...</td>\n",
       "      <td>2.41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>setelah menunggu , akhirnya dipanggil juga . t...</td>\n",
       "      <td>0</td>\n",
       "      <td>setelah waiting akhirnya called juga tempat hi...</td>\n",
       "      <td>0.709845</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.619, 0.0095, 0.3715]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5153, 0.009, 0.4757]</td>\n",
       "      <td>{'nya': 'his', 'oke': 'okay', 'bakso': 'meatba...</td>\n",
       "      <td>7.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>siapapun pasangan pdip ku burkan</td>\n",
       "      <td>2</td>\n",
       "      <td>siapapun pasangan pdip ku burkan</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0218, 0.0425, 0.9357]</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0218, 0.0425, 0.9357]</td>\n",
       "      <td>{'pdip': 'pdip'}</td>\n",
       "      <td>1.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>pilkada kaltim sekarang ini jorok banget , say...</td>\n",
       "      <td>2</td>\n",
       "      <td>election kaltim sekarang ini dirty very saya a...</td>\n",
       "      <td>0.940169</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0009, 0.0016, 0.9976]</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0098, 0.7538, 0.2363]</td>\n",
       "      <td>{'jorok': 'dirty', 'banget': 'very', 'pilkada'...</td>\n",
       "      <td>5.26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>infrastruktur dikebut sana - sini adalah usaha...</td>\n",
       "      <td>2</td>\n",
       "      <td>infrastructure accelerated sana sini adalah us...</td>\n",
       "      <td>0.830146</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.016, 0.0017, 0.9823]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9711, 0.0094, 0.0195]</td>\n",
       "      <td>{'ambruk': 'collapse', 'infrastruktur': 'infra...</td>\n",
       "      <td>3.41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>koruptor merajalela banyak menteri nya tertang...</td>\n",
       "      <td>2</td>\n",
       "      <td>koruptor rampant banyak menteri his tertangkap...</td>\n",
       "      <td>0.838858</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0006, 0.0016, 0.9977]</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0009, 0.0032, 0.996]</td>\n",
       "      <td>{'bengong': 'dumbfounded', 'nya': 'his', 'cina...</td>\n",
       "      <td>3.47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>ketua kamar dagang dan industri alias kadin ja...</td>\n",
       "      <td>1</td>\n",
       "      <td>ketua kamar dagang dan industri alias kadin ja...</td>\n",
       "      <td>0.834482</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0054, 0.9931, 0.0015]</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.014, 0.984, 0.002]</td>\n",
       "      <td>{'kamil': 'Kamil', 'ridwan': 'ridwan', 'calon'...</td>\n",
       "      <td>3.26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>suasana yang nyaman untuk makan bersama keluar...</td>\n",
       "      <td>0</td>\n",
       "      <td>suasana yang comfortable untuk makan bersama k...</td>\n",
       "      <td>0.903926</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9791, 0.002, 0.0189]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9718, 0.002, 0.0263]</td>\n",
       "      <td>{'nyaman': 'comfortable', 'terjangkau': 'affor...</td>\n",
       "      <td>1.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  sentiment  \\\n",
       "380                     mau marah sama koneksi indosat          2   \n",
       "381  pemancing peliharaan , nanti kasus nya kek ang...          2   \n",
       "382  anies itu tidak ada beda nya sama pemimpin-pem...          2   \n",
       "383  sebelum mencoblos , sudah seharusnya kita memp...          1   \n",
       "384  menikmati salah satu teh otentik khas jawa bar...          0   \n",
       "385  hasil quick count pilkada kota makassar yang d...          1   \n",
       "386  makan di burangrang kafe vip lantai 2 , makana...          0   \n",
       "387                                               lucu          0   \n",
       "388  mari terus upayakan perubahan , perubahan ke a...          0   \n",
       "389  kita pakai biznet dah mahal juga banyak masala...          2   \n",
       "390  semua makanan nya enak . pelayanan nya benar-b...          0   \n",
       "391  liburan yang menyenangkan dengan banyak aktifi...          0   \n",
       "392  tempat nya asik banget , dingin juga , dan mak...          0   \n",
       "393  setelah menunggu , akhirnya dipanggil juga . t...          0   \n",
       "394                   siapapun pasangan pdip ku burkan          2   \n",
       "395  pilkada kaltim sekarang ini jorok banget , say...          2   \n",
       "396  infrastruktur dikebut sana - sini adalah usaha...          2   \n",
       "397  koruptor merajalela banyak menteri nya tertang...          2   \n",
       "398  ketua kamar dagang dan industri alias kadin ja...          1   \n",
       "399  suasana yang nyaman untuk makan bersama keluar...          0   \n",
       "\n",
       "                                        perturbed_text  \\\n",
       "380                     mau marah sama koneksi Indosat   \n",
       "381  pemancing peliharaan nanti kasus his cake angg...   \n",
       "382  anies itu tidak ada beda nya sama hypocrite se...   \n",
       "383  sebelum to vote sudah seharusnya kita mempelaj...   \n",
       "384  menikmati salah satu tea otentik khas jawa wes...   \n",
       "385  hasil quick count pilkada kota Makassar yang d...   \n",
       "386  makan di burangrang kafe vip floor 2 eat his n...   \n",
       "387                                              funny   \n",
       "388  mari terus upayakan perubahan , perubahan ke a...   \n",
       "389  kita pakai biznet bye expensive juga banyak ma...   \n",
       "390  semua makanan his nice pelayanan his excellent...   \n",
       "391  liburan yang pleasant dengan banyak aktifitas ...   \n",
       "392  tempat his asik banget dingin juga dan makanan...   \n",
       "393  setelah waiting akhirnya called juga tempat hi...   \n",
       "394                   siapapun pasangan pdip ku burkan   \n",
       "395  election kaltim sekarang ini dirty very saya a...   \n",
       "396  infrastructure accelerated sana sini adalah us...   \n",
       "397  koruptor rampant banyak menteri his tertangkap...   \n",
       "398  ketua kamar dagang dan industri alias kadin ja...   \n",
       "399  suasana yang comfortable untuk makan bersama k...   \n",
       "\n",
       "     perturbed_semantic_sim  pred_label                pred_proba  \\\n",
       "380                1.000000           2   [0.0005, 0.0015, 0.998]   \n",
       "381                0.753758           2  [0.0007, 0.0079, 0.9914]   \n",
       "382                0.941399           2  [0.0135, 0.0053, 0.9812]   \n",
       "383                0.742823           1   [0.003, 0.9809, 0.0162]   \n",
       "384                0.808160           0  [0.9963, 0.0028, 0.0008]   \n",
       "385                0.787770           1  [0.0036, 0.9958, 0.0006]   \n",
       "386                0.769031           0  [0.9982, 0.0014, 0.0004]   \n",
       "387                0.286360           0   [0.9945, 0.0015, 0.004]   \n",
       "388                1.000000           1  [0.0672, 0.9294, 0.0034]   \n",
       "389                0.941519           2  [0.0535, 0.0301, 0.9165]   \n",
       "390                0.511431           0  [0.9981, 0.0011, 0.0007]   \n",
       "391                0.925851           0  [0.9968, 0.0024, 0.0007]   \n",
       "392                0.835893           0  [0.9984, 0.0011, 0.0004]   \n",
       "393                0.709845           0   [0.619, 0.0095, 0.3715]   \n",
       "394                1.000000           2  [0.0218, 0.0425, 0.9357]   \n",
       "395                0.940169           2  [0.0009, 0.0016, 0.9976]   \n",
       "396                0.830146           2   [0.016, 0.0017, 0.9823]   \n",
       "397                0.838858           2  [0.0006, 0.0016, 0.9977]   \n",
       "398                0.834482           1  [0.0054, 0.9931, 0.0015]   \n",
       "399                0.903926           0   [0.9791, 0.002, 0.0189]   \n",
       "\n",
       "     perturbed_label           perturbed_proba  \\\n",
       "380                2   [0.0005, 0.0015, 0.998]   \n",
       "381                2  [0.0023, 0.3216, 0.6761]   \n",
       "382                2   [0.123, 0.0149, 0.8621]   \n",
       "383                1  [0.0258, 0.9323, 0.0419]   \n",
       "384                0   [0.9921, 0.0069, 0.001]   \n",
       "385                1   [0.0101, 0.989, 0.0009]   \n",
       "386                0  [0.9937, 0.0056, 0.0007]   \n",
       "387                0  [0.9941, 0.0045, 0.0014]   \n",
       "388                1  [0.0672, 0.9294, 0.0034]   \n",
       "389                2   [0.2063, 0.284, 0.5097]   \n",
       "390                0    [0.99, 0.0062, 0.0039]   \n",
       "391                0   [0.9956, 0.003, 0.0013]   \n",
       "392                0  [0.9977, 0.0015, 0.0008]   \n",
       "393                0   [0.5153, 0.009, 0.4757]   \n",
       "394                2  [0.0218, 0.0425, 0.9357]   \n",
       "395                1  [0.0098, 0.7538, 0.2363]   \n",
       "396                0  [0.9711, 0.0094, 0.0195]   \n",
       "397                2   [0.0009, 0.0032, 0.996]   \n",
       "398                1     [0.014, 0.984, 0.002]   \n",
       "399                0   [0.9718, 0.002, 0.0263]   \n",
       "\n",
       "                                    translated_word(s)  running_time(s)  \\\n",
       "380                             {'indosat': 'Indosat'}             1.21   \n",
       "381        {'kek': 'cake', 'nya': 'his', 'fpi': 'fpi'}             2.80   \n",
       "382        {'aus': 'wear out', 'munafiq': 'hypocrite'}             1.70   \n",
       "383  {'kandidat': 'candidate', 'mencoblos': 'to vote'}             1.28   \n",
       "384  {'nya': 'his', 'enak': 'nice', 'juara': 'champ...             2.92   \n",
       "385  {'tunggal': 'single', 'calon': 'candidate', 'k...             3.70   \n",
       "386  {'enak': 'nice', 'nya': 'his', 'suka': 'like',...             3.28   \n",
       "387                                  {'lucu': 'funny'}             1.27   \n",
       "388                                                                0.03   \n",
       "389  {'mahal': 'expensive', 'biznet': 'biznet', 'da...             3.39   \n",
       "390  {'sunda': 'Sunda', 'enak': 'nice', 'nya': 'his...             2.31   \n",
       "391  {'menyenangkan': 'pleasant', 'makanan': 'food'...             3.19   \n",
       "392  {'baju': 'dress', 'toko': 'shop', 'enak': 'nic...             2.41   \n",
       "393  {'nya': 'his', 'oke': 'okay', 'bakso': 'meatba...             7.16   \n",
       "394                                   {'pdip': 'pdip'}             1.60   \n",
       "395  {'jorok': 'dirty', 'banget': 'very', 'pilkada'...             5.26   \n",
       "396  {'ambruk': 'collapse', 'infrastruktur': 'infra...             3.41   \n",
       "397  {'bengong': 'dumbfounded', 'nya': 'his', 'cina...             3.47   \n",
       "398  {'kamil': 'Kamil', 'ridwan': 'ridwan', 'calon'...             3.26   \n",
       "399  {'nyaman': 'comfortable', 'terjangkau': 'affor...             1.74   \n",
       "\n",
       "     before_attack_acc  after_attack_acc  avg_semantic_sim  \\\n",
       "380               0.94              0.88          0.851869   \n",
       "381                NaN               NaN               NaN   \n",
       "382                NaN               NaN               NaN   \n",
       "383                NaN               NaN               NaN   \n",
       "384                NaN               NaN               NaN   \n",
       "385                NaN               NaN               NaN   \n",
       "386                NaN               NaN               NaN   \n",
       "387                NaN               NaN               NaN   \n",
       "388                NaN               NaN               NaN   \n",
       "389                NaN               NaN               NaN   \n",
       "390                NaN               NaN               NaN   \n",
       "391                NaN               NaN               NaN   \n",
       "392                NaN               NaN               NaN   \n",
       "393                NaN               NaN               NaN   \n",
       "394                NaN               NaN               NaN   \n",
       "395                NaN               NaN               NaN   \n",
       "396                NaN               NaN               NaN   \n",
       "397                NaN               NaN               NaN   \n",
       "398                NaN               NaN               NaN   \n",
       "399                NaN               NaN               NaN   \n",
       "\n",
       "     avg_running_time(s)  \n",
       "380               3.4364  \n",
       "381                  NaN  \n",
       "382                  NaN  \n",
       "383                  NaN  \n",
       "384                  NaN  \n",
       "385                  NaN  \n",
       "386                  NaN  \n",
       "387                  NaN  \n",
       "388                  NaN  \n",
       "389                  NaN  \n",
       "390                  NaN  \n",
       "391                  NaN  \n",
       "392                  NaN  \n",
       "393                  NaN  \n",
       "394                  NaN  \n",
       "395                  NaN  \n",
       "396                  NaN  \n",
       "397                  NaN  \n",
       "398                  NaN  \n",
       "399                  NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dataset.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83b16bb-b416-4519-909b-ad6211215ee6",
   "metadata": {},
   "source": [
    "### Fungsi Awal banget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "913a33b3-e1fa-401e-9e3e-20eec418fa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len_text: 17\n",
      "ic| running_time_a: 0.55\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 0.65\n",
      "ic| first_perturbation_sim_score: 0.994461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('anies itu tidak ada beda nya sama munafiq sebentar lagi juga dia pasti aus itu sama kekuasaan',\n",
       " 0.994461,\n",
       " 2,\n",
       " array([0.0135, 0.0053, 0.9812]),\n",
       " 2,\n",
       " array([0.0055, 0.0034, 0.9911]),\n",
       " {'aus': 'aus', 'munafiq': 'munafiq'},\n",
       " 0.73)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack(\n",
    "    text_ls = exp_dataset.iloc[[2]].text.item(),\n",
    "    true_label = exp_dataset.iloc[[2]].sentiment.item(),\n",
    "    predictor = finetuned_model,\n",
    "    tokenizer = tokenizer, \n",
    "    att_ratio = perturb_ratio,\n",
    "    perturbation_technique = perturbation_technique,\n",
    "    lang_codemix = perturb_lang,\n",
    "    sim_predictor = use\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdf9577-ef43-407b-b707-252c0ea1853f",
   "metadata": {},
   "source": [
    "### Coba optimize di USE nya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0752ac79-d4fc-4f0c-be49-d1117ac29329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len_text: 68\n",
      "ic| running_time_a: 2.27\n",
      "ic| end_time_conv: 0.0\n",
      "ic| end_time_use: 2.54\n",
      "ic| first_perturbation_sim_score: 0.99954677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('bao sendiri arti nya bakpau dan di sini banyak rasa model bao nya yang di sini beli 3 rasa bao yaitu bao coklat ayam bumbu merah sama durian untuk bao coklat sama durian nya enak banget tapi untuk bao ayam nya biasa saja dimsum mencoba bakau sama siomay sioma nya di sini siomay ayam buat rasa sih enak selain ada bao dan dimsum juga tersedia makanan berat lainnya',\n",
       " 0.99954677,\n",
       " 0,\n",
       " array([0.9969, 0.0012, 0.0019]),\n",
       " 0,\n",
       " array([0.9936, 0.0018, 0.0045]),\n",
       " {'tersedia': 'insan',\n",
       "  'mencoba': 'bergerak',\n",
       "  'siomay': 'siomay',\n",
       "  'enak': 'enak',\n",
       "  'banget': 'banget',\n",
       "  'bao': 'bao',\n",
       "  'sioma': 'sioma',\n",
       "  'bakpau': 'bakpau',\n",
       "  'merah': 'kesehatan',\n",
       "  'nya': 'nya'},\n",
       " 2.66)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack(\n",
    "    text_ls = exp_dataset.iloc[[48]].text.item(),\n",
    "    true_label = 0,\n",
    "    predictor = finetuned_model,\n",
    "    tokenizer = tokenizer, \n",
    "    att_ratio = perturb_ratio,\n",
    "    perturbation_technique = perturbation_technique,\n",
    "    lang_codemix = perturb_lang,\n",
    "    sim_predictor = use\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83533730-b3c7-471a-a06f-6233e86e8d2e",
   "metadata": {},
   "source": [
    "### Coba Optimize step 2+codemix perturbation pake batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1475f7-9a37-4239-90ac-47db1fa26e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack(\n",
    "    text_ls = exp_dataset.iloc[[48]].text.item(),\n",
    "    true_label = 0,\n",
    "    predictor = finetuned_model,\n",
    "    tokenizer = tokenizer, \n",
    "    att_ratio = perturb_ratio,\n",
    "    perturbation_technique = perturbation_technique,\n",
    "    lang_codemix = perturb_lang,\n",
    "    sim_predictor = use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f70e96-30c9-4fd1-b2ae-1d3c61bdd7d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4100b193-fdc3-4d59-bc11-ab02fa4611f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def codemix_perturbation(words, target_lang, words_perturb):\n",
    "    \"\"\"\n",
    "    'su': 'sundanese'\n",
    "    'jw': 'javanese'\n",
    "    'ms': 'malay'\n",
    "    'en': 'english'\n",
    "    \"\"\"\n",
    "    translator = GoogleTranslator(source=\"id\", target=target_lang)\n",
    "    new_wp = []\n",
    "    for wp in words_perturb:\n",
    "        if wp[1].isalpha():\n",
    "            new_wp.append(wp[1])\n",
    "    \n",
    "    if len(new_wp) == 1:\n",
    "        new_wp_trans = dict(zip(new_wp, translator.translate(new_wp[0])))\n",
    "    elif len(new_wp) == 0:\n",
    "        return ' '.join(new_words), {}\n",
    "        \n",
    "    new_wp_trans = dict(zip(new_wp, translator.translate_batch(new_wp)))\n",
    "    \n",
    "    supported_langs = [\"su\", \"jw\", \"ms\", \"en\", \"fr\", \"it\"]\n",
    "    \n",
    "    if target_lang not in supported_langs:\n",
    "        raise ValueError('Language Unavailable')\n",
    "    \n",
    "    new_words = words.copy()\n",
    "    \n",
    "    if len(words_perturb) >= 1:\n",
    "        for perturb_word in new_wp_trans.keys():\n",
    "            new_words = [new_wp_trans[perturb_word] if word == perturb_word and word.isalpha() else word for word in new_words]\n",
    "\n",
    "    return ' '.join(new_words), new_wp_trans\n",
    "\n",
    "def synonym_replacement(words, words_perturb):    \n",
    "    # new_words = words.copy()\n",
    "    new_wp = []\n",
    "    for wp in words_perturb:\n",
    "        if wp[1].isalpha():\n",
    "            new_wp.append(wp[1])\n",
    "    \n",
    "    if len(new_wp) == 1:\n",
    "        new_wp_trans = dict(zip(new_wp, get_synonyms(new_wp[0])[0]))\n",
    "    elif len(new_wp) == 0:\n",
    "        return ' '.join(new_words), {}\n",
    "    \n",
    "    wp_trans=[]\n",
    "    for wp in new_wp:\n",
    "        wp_trans.append(get_synonyms(wp)[0])\n",
    "        print(wp,wp_trans)\n",
    "    \n",
    "    new_wp_trans = dict(zip(new_wp, wp_trans))\n",
    "    \n",
    "    new_words = words.copy()\n",
    "    \n",
    "    if len(words_perturb) >= 1:\n",
    "        for perturb_word in new_wp_trans.keys():\n",
    "            new_words = [new_wp_trans[perturb_word[1]] if word == perturb_word[1] and word.isalpha() else word for word in new_words]\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "    \n",
    "    return sentence, new_wp_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ddac99a0-2976-48c0-8c91-20e9c86de269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['diragukan', 'enak', 'burgundy', 'makanan', 'dago', 'romantis', 'taman', 'rerumputan', 'malam']\n",
      "diragukan ['diragukan']\n",
      "enak ['diragukan', 'enak']\n",
      "burgundy ['diragukan', 'enak', 'burgundy']\n",
      "makanan ['diragukan', 'enak', 'burgundy', 'nutrisi']\n",
      "dago ['diragukan', 'enak', 'burgundy', 'nutrisi', 'dago']\n",
      "romantis ['diragukan', 'enak', 'burgundy', 'nutrisi', 'dago', 'cita']\n",
      "taman ['diragukan', 'enak', 'burgundy', 'nutrisi', 'dago', 'cita', 'stadion']\n",
      "rerumputan ['diragukan', 'enak', 'burgundy', 'nutrisi', 'dago', 'cita', 'stadion', 'bendang']\n",
      "malam ['diragukan', 'enak', 'burgundy', 'nutrisi', 'dago', 'cita', 'stadion', 'bendang', 'rentang waktu']\n",
      "{'diragukan': 'diragukan', 'enak': 'enak', 'burgundy': 'burgundy', 'makanan': 'nutrisi', 'dago': 'dago', 'romantis': 'cita', 'taman': 'stadion', 'rerumputan': 'bendang', 'malam': 'rentang waktu'}\n",
      "buat kalian yang cuma tahu tempat makan malam romantis yang ada di dago , boleh dicoba deh makan di burgundy sini , di sini sih menurut saya tempat yang enak banget buat ngadem selain tempat nya yang hening kita juga bisa melihat rerumputan yang hijau meski cuma di taman nya , hehehe . tapi balik lagi buat saya sih makanan , haha . tidak usah diragukan .\n",
      "0.03\n"
     ]
    }
   ],
   "source": [
    "word = 'buat kalian yang cuma tahu tempat makan malam romantis yang ada di dago , boleh dicoba deh makan di burgundy sini , di sini sih menurut saya tempat yang enak banget buat ngadem selain tempat nya yang hening kita juga bisa melihat rerumputan yang hijau meski cuma di taman nya , hehehe . tapi balik lagi buat saya sih makanan , haha . tidak usah diragukan .'\n",
    "words = word.split()\n",
    "wp = [(59, 'diragukan', 0.001480978060399707), (27, 'enak', 0.0006103660272316347), (18, 'burgundy', 0.0005351185233486433), (55, 'makanan', 0.000532135112141674), (12, 'dago', 0.0004499789485095107), (8, 'romantis', 0.0003965279976370084), (46, 'taman', 0.00039581217539819136), (40, 'rerumputan', 0.00039318750260974866), (7, 'malam', 0.000384060906586825)]\n",
    "target_lang = \"en\"\n",
    "\n",
    "start_time = time.time()\n",
    "print(synonym_replacement(words, wp))\n",
    "print(round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92985e6-016d-4609-bb85-70034bbeb2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbert-sentiment-codemixing-jw-adv-0.6-valid\n",
    "# 279/1260\n",
    "850/1260\n",
    "new_wp_trans = dict(zip(new_wp, translator.translate(new_wp[0])))\n",
    "IndexError: list index out of range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00c2493-4560-4d0a-b00a-ac4ec680cb3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c95ec8-1c18-42da-8abc-8bc0bdab7b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4e3f4a9-9263-40c8-bc14-523ab596c096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len_text: 43\n",
      "ic| running_time_a: 1.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['air', 'gemericik', 'lembang', 'asri', 'pekan', 'harga', 'kampung', 'daun', 'terletak', 'restoran', 'enak', 'reservasi']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| end_time_use: 6.41\n",
      "ic| first_perturbation_sim_score: DeviceArray(0.997835, dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('kampung daun terletak di lembang restoran ini akan penuh di akhir pekan bila belum melakukan leladen tidak disarankan untuk datang langsung ke restoran ini rasa masakan yang disajikan cukup enak dengan harga yang terjangkau tempat makan juga sangat asri karena terdengar gemericik air',\n",
       " DeviceArray(0.997835, dtype=float32),\n",
       " 0,\n",
       " array([9.983e-01, 1.000e-03, 7.000e-04]),\n",
       " 0,\n",
       " array([9.978e-01, 1.300e-03, 9.000e-04]),\n",
       " {'air': 'banyu',\n",
       "  'gemericik': 'gurgling',\n",
       "  'lembang': 'lembah',\n",
       "  'asri': 'ayu',\n",
       "  'pekan': 'minggu',\n",
       "  'harga': 'regane',\n",
       "  'kampung': 'desa',\n",
       "  'daun': 'godhong',\n",
       "  'terletak': 'dumunung',\n",
       "  'restoran': 'restoran',\n",
       "  'enak': 'becik',\n",
       "  'reservasi': 'leladen'},\n",
       " 10.7)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack(\n",
    "    text_ls = exp_dataset.iloc[[5]].text.item(),\n",
    "    true_label = exp_dataset.iloc[[5]].sentiment.item(),\n",
    "    predictor = finetuned_model,\n",
    "    tokenizer = tokenizer, \n",
    "    att_ratio = 0.6,\n",
    "    perturbation_technique = perturbation_technique,\n",
    "    lang_codemix = \"jw\",\n",
    "    sim_predictor = use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8d3139-25ad-4e6e-b504-d88b2a388cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24548f59-5e35-4aad-9b84-0ae0a81c5d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9583c461-28d2-4c46-b2ed-0dc9f27a2479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>batagor somay nya memang luar biasa enak , tap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  sentiment\n",
       "852  batagor somay nya memang luar biasa enak , tap...          0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dataset.iloc[[7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aacafc0-3376-4cd2-884b-dc98c7967b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01636e7-36cb-4cf4-aee6-da4316774ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs:\", len(physical_devices))\n",
    "# !pip list cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55317fa7-d77c-40a1-9ecd-a145385008e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['diragukan', 'enak', 'burgundy', 'makanan', 'dago', 'romantis', 'taman', 'rerumputan', 'malam']\n",
    "\n",
    "start_time = time.time()\n",
    "translated_1 = []\n",
    "translator = GoogleTranslator(source=\"id\", target=\"en\")\n",
    "for txt in texts:\n",
    "    translated_1.append(translator.translate(txt))\n",
    "print(round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd92533-750e-4274-a722-04d0f0c89141",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "translated_2 = translator.translate_batch(texts)\n",
    "print(round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469821c-a347-4a04-88df-c9fb8ad61798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !. myenv/bin/activate\n",
    "# !pip uninstall tensorflow -y\n",
    "# !pip uninstall tensorflow-gpu -y\n",
    "\n",
    "!pip install tensorflow --ignore-installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad62f6-736d-4323-a8c7-7b011b8004e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64\n",
    "!echo $LD_LIBRARY_PATH"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
