{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "546ac2b4-f52d-4f77-8b42-4bc237320037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMOD_DIR=/usr/share/lmod/lmod/libexec/\n",
      "TMUX=/tmp/tmux-22086/default,1030980,0\n",
      "SSH_CLIENT=103.107.4.31 63325 22\n",
      "USER=m13518040\n",
      "LMOD_COLORIZE=yes\n",
      "MPLBACKEND=module://matplotlib_inline.backend_inline\n",
      "LMOD_PKG=/usr/share/lmod/lmod\n",
      "LD_LIBRARY_PATH=/usr/local/cuda/lib:/usr/local/cuda/lib\n",
      "HOME=/home/m13518040\n",
      "MOTD_SHOWN=pam\n",
      "OLDPWD=/raid/data/m13518040\n",
      "SSH_TTY=/dev/pts/2\n",
      "PAGER=cat\n",
      "TF_CPP_MIN_LOG_LEVEL=1\n",
      "LMOD_sys=Linux\n",
      "TF2_BEHAVIOR=1\n",
      "LOGNAME=m13518040\n",
      "TERM=xterm-color\n",
      "CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "PATH=/home/m13518040/.local/bin:/usr/local/cuda/bin:/opt/bin/:/usr/local/cuda/bin:/home/m13518040/.local/bin:/usr/local/cuda/bin:/opt/bin/:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n",
      "LMOD_FULL_SETTARG_SUPPORT=no\n",
      "LMOD_PREPEND_BLOCK=normal\n",
      "KMP_DUPLICATE_LIB_OK=True\n",
      "KMP_INIT_AT_FORK=FALSE\n",
      "LANG=en_US.UTF-8\n",
      "MODULEPATH_ROOT=/sw/modules\n",
      "SHELL=/bin/sh\n",
      "LMOD_CMD=/usr/share/lmod/lmod/libexec/lmod\n",
      "BASH_ENV=/usr/share/lmod/lmod/init/bash\n",
      "GIT_PAGER=cat\n",
      "PWD=/raid/data/m13518040/tugas-akhir-repository\n",
      "CLICOLOR=1\n",
      "SSH_CONNECTION=103.107.4.31 63325 167.205.35.247 22\n",
      "XDG_DATA_DIRS=/usr/local/share:/usr/share:/var/lib/snapd/desktop\n",
      "JPY_PARENT_PID=1034665\n",
      "PYDEVD_USE_FRAME_EVAL=NO\n",
      "LMOD_arch=x86_64\n",
      "MANPATH=/usr/share/lmod/lmod/share/man::\n",
      "MODULEPATH=/sw/modules/all\n",
      "TMUX_PANE=%0\n",
      "LMOD_SETTARG_CMD=:\n",
      "MODULESHOME=/usr/share/lmod/lmod\n",
      "LMOD_DIR=/usr/share/lmod/lmod/libexec/\n",
      "TMUX=/tmp/tmux-22086/default,1030980,0\n",
      "SSH_CLIENT=103.107.4.31 63325 22\n",
      "USER=m13518040\n",
      "LMOD_COLORIZE=yes\n",
      "MPLBACKEND=module://matplotlib_inline.backend_inline\n",
      "LMOD_PKG=/usr/share/lmod/lmod\n",
      "LD_LIBRARY_PATH=/usr/local/cuda/lib:/usr/local/cuda/lib\n",
      "HOME=/home/m13518040\n",
      "MOTD_SHOWN=pam\n",
      "OLDPWD=/raid/data/m13518040\n",
      "SSH_TTY=/dev/pts/2\n",
      "PAGER=cat\n",
      "TF_CPP_MIN_LOG_LEVEL=1\n",
      "LMOD_sys=Linux\n",
      "TF2_BEHAVIOR=1\n",
      "LOGNAME=m13518040\n",
      "TERM=xterm-color\n",
      "CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "PATH=/home/m13518040/.local/bin:/usr/local/cuda/bin:/opt/bin/:/usr/local/cuda/bin:/home/m13518040/.local/bin:/usr/local/cuda/bin:/opt/bin/:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n",
      "LMOD_FULL_SETTARG_SUPPORT=no\n",
      "LMOD_PREPEND_BLOCK=normal\n",
      "KMP_DUPLICATE_LIB_OK=True\n",
      "KMP_INIT_AT_FORK=FALSE\n",
      "LANG=en_US.UTF-8\n",
      "MODULEPATH_ROOT=/sw/modules\n",
      "SHELL=/bin/sh\n",
      "LMOD_CMD=/usr/share/lmod/lmod/libexec/lmod\n",
      "BASH_ENV=/usr/share/lmod/lmod/init/bash\n",
      "GIT_PAGER=cat\n",
      "PWD=/raid/data/m13518040/tugas-akhir-repository\n",
      "CLICOLOR=1\n",
      "SSH_CONNECTION=103.107.4.31 63325 167.205.35.247 22\n",
      "XDG_DATA_DIRS=/usr/local/share:/usr/share:/var/lib/snapd/desktop\n",
      "JPY_PARENT_PID=1034665\n",
      "PYDEVD_USE_FRAME_EVAL=NO\n",
      "LMOD_arch=x86_64\n",
      "MANPATH=/usr/share/lmod/lmod/share/man::\n",
      "MODULEPATH=/sw/modules/all\n",
      "TMUX_PANE=%0\n",
      "LMOD_SETTARG_CMD=:\n",
      "MODULESHOME=/usr/share/lmod/lmod\n",
      "CUDA_VISIBLE_DEVICES=4\n"
     ]
    }
   ],
   "source": [
    "# !export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64\n",
    "!env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "!env CUDA_VISIBLE_DEVICES=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "15edb277-e4c7-4c1b-998c-32c720b17121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils_init_model import text_logit, fine_tuning_model, eval_model, init_model, load_word_index\n",
    "import time\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# import jax.numpy as jnp\n",
    "# from jax import random\n",
    "\n",
    "import math\n",
    "# from .attack_helper import get_synonyms, codemix_perturbation, synonym_replacement, swap_minimum_importance_words, trans_dict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "stop_words_set = []\n",
    "for w in stopwords.words('indonesian'):\n",
    "    stop_words_set.append(w)\n",
    "\n",
    "from icecream import ic\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "import copy\n",
    "from deep_translator import GoogleTranslator\n",
    "import numpy as np\n",
    "import random\n",
    "# from utils.utils_semantic_use import USE\n",
    "from utils.utils_init_dataset import set_seed, load_dataset_loader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "# tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6d543c47-539a-4917-aea4-6e26eea1c708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jax\n",
    "# !pip install --upgrade \"jax[cuda]\" -f https://storage.googleapis.com/jax-releases/jax_releases.html  # Note: wheels only available on linux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7cef73d5-b954-43f7-b4b2-4b1d8b46cd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class USE(object):\n",
    "    def __init__(self):\n",
    "        import tensorflow_hub as hub\n",
    "        super(USE, self).__init__()\n",
    "        module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
    "        self.embed = hub.load(module_url)\n",
    "\n",
    "    def semantic_sim(self, sentA : str, sentB : str) -> float:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sentA: The first sentence.\n",
    "            sentB: The second sentence.\n",
    "\n",
    "        Returns:\n",
    "            Cosine distance between two sentences.\n",
    "        \n",
    "        \"\"\"\n",
    "        if sentA.lower() == sentB.lower():\n",
    "            return 1.000\n",
    "        ret = np.array(self.embed([sentA, sentB]).numpy())\n",
    "        score = ret[0].dot(ret[1]) / (np.linalg.norm(ret[0]) * np.linalg.norm(ret[1]))\n",
    "        if score>1:\n",
    "            return 1.000\n",
    "        return score\n",
    "\n",
    "    def after_attack(self, input, adversarial_sample):\n",
    "        if adversarial_sample is not None:\n",
    "            return self.calc_score(input[\"x\"], adversarial_sample)\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "# USE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d653914f-8ae1-44af-b961-ba0fbc4dba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class USE(object):\n",
    "#     def __init__(self, module_url=None):\n",
    "#         super(USE, self).__init__()\n",
    "#         module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
    "#         self.embed = hub.load(module_url)\n",
    "        \n",
    "#         def embed(input):\n",
    "#             return model(input)\n",
    "        \n",
    "#         # config = tf.compat.v1.ConfigProto()\n",
    "#         # config.gpu_options.allow_growth = True\n",
    "#         # self.sess = tf.compat.v1.Session(config=config)\n",
    "#         # self.sess.run([tf.compat.v1.global_variables_initializer(), tf.compat.v1.tables_initializer()])\n",
    "\n",
    "#     def semantic_sim(self, sents1, sents2):\n",
    "#         # with tf.Session() as session:\n",
    "#         #     session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "#         #     with tf.device('GPU'):\n",
    "#         #         message_embeddings_ = session.run(self.embed([sents1, sents2]))\n",
    "        \n",
    "#         if sents1.lower() == sents2.lower():\n",
    "#             return 1.000\n",
    "        \n",
    "#         message_embeddings_ = self.embed([sents1, sents2])\n",
    "#         # message_embeddings_ = message_embeddings_.eval(session=self.sess)\n",
    "#         corr = np.tensordot(message_embeddings_, message_embeddings_, axes=(-1,-1))\n",
    "        \n",
    "#         if corr[0][1] > 1:\n",
    "#             return 1.000\n",
    "#         return corr[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3576aabe-dd88-40dd-8ff3-64d225667ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "def get_synonyms(word):\n",
    "    if word in stop_words_set:\n",
    "        return [word]\n",
    "\n",
    "    word_lemmas = wordnet.lemmas(word, lang=\"ind\")\n",
    "    \n",
    "    hypernyms = []\n",
    "    for lem in word_lemmas:\n",
    "        hypernyms.append(lem.synset().hypernyms())\n",
    "\n",
    "    if not any(hypernyms):\n",
    "        return [word]\n",
    "    \n",
    "    lemma_corp = []\n",
    "    \n",
    "    for hypernym in hypernyms:\n",
    "        if(len(hypernym) < 1):\n",
    "            continue\n",
    "        else:\n",
    "            lemma_corp.append(hypernym[0].lemmas(lang=\"ind\"))\n",
    "            \n",
    "    lemmas = set()\n",
    "    for list_lemmas in lemma_corp:\n",
    "        if(len(list_lemmas) < 1):\n",
    "            lemmas.add(word)\n",
    "        else:\n",
    "            for l in list_lemmas:\n",
    "                lemmas.add(l.name())\n",
    "    \n",
    "    clean_synonyms = set()\n",
    "    for syn in lemmas.copy():\n",
    "        synonym = syn.replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "        synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "        clean_synonyms.add(synonym) \n",
    "    if word in clean_synonyms:\n",
    "        clean_synonyms.remove(word)\n",
    "    \n",
    "    if len(list(clean_synonyms)) < 1:\n",
    "        return [word]\n",
    "    else:\n",
    "        return list(clean_synonyms)\n",
    "\n",
    "def codemix_perturbation(words, target_lang, words_perturb):\n",
    "    \"\"\"\n",
    "    'su': 'sundanese'\n",
    "    'jw': 'javanese'\n",
    "    'ms': 'malay'\n",
    "    'en': 'english'\n",
    "    \"\"\"\n",
    "    translator = GoogleTranslator(source=\"id\", target=target_lang)\n",
    "    new_wp = []\n",
    "    for wp in words_perturb:\n",
    "        if wp[1].isalpha():\n",
    "            new_wp.append(wp[1])\n",
    "    \n",
    "    if len(new_wp) == 1:\n",
    "        new_wp_trans = dict(zip(new_wp, translator.translate(new_wp[0])))\n",
    "    elif len(new_wp) == 0:\n",
    "        return ' '.join(new_words), {}\n",
    "        \n",
    "    new_wp_trans = dict(zip(new_wp, translator.translate_batch(new_wp)))\n",
    "    \n",
    "    supported_langs = [\"su\", \"jw\", \"ms\", \"en\", \"fr\", \"it\"]\n",
    "    \n",
    "    if target_lang not in supported_langs:\n",
    "        raise ValueError('Language Unavailable')\n",
    "    \n",
    "    new_words = words.copy()\n",
    "    \n",
    "    if len(words_perturb) >= 1:\n",
    "        for perturb_word in new_wp_trans.keys():\n",
    "            new_words = [new_wp_trans[perturb_word] if word == perturb_word and word.isalpha() else word for word in new_words]\n",
    "\n",
    "    return ' '.join(new_words), new_wp_trans\n",
    "\n",
    "def synonym_replacement(words, words_perturb):    \n",
    "    # new_words = words.copy()\n",
    "    new_wp = []\n",
    "    for wp in words_perturb:\n",
    "        if wp[1].isalpha():\n",
    "            new_wp.append(wp[1])\n",
    "    \n",
    "    if len(new_wp) == 1:\n",
    "        new_wp_trans = dict(zip(new_wp, get_synonyms(new_wp[0])[0]))\n",
    "    elif len(new_wp) == 0:\n",
    "        return ' '.join(new_words), {}\n",
    "    \n",
    "    wp_trans=[]\n",
    "    for wp in new_wp:\n",
    "        wp_trans.append(get_synonyms(wp)[0])\n",
    "    \n",
    "    new_wp_trans = dict(zip(new_wp, wp_trans))\n",
    "    \n",
    "    new_words = words.copy()\n",
    "    \n",
    "    if len(words_perturb) >= 1:\n",
    "        for perturb_word in new_wp_trans.keys():\n",
    "            new_words = [new_wp_trans[perturb_word[1]] if word == perturb_word[1] and word.isalpha() else word for word in new_words]\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "    \n",
    "    return sentence, new_wp_trans\n",
    "\n",
    "# fungsi untuk mencari kandidat lain ketika sebuah kandidat perturbasi kurang dari sim_score_threshold\n",
    "def swap_minimum_importance_words(words_perturb, top_importance_words):\n",
    "    print(\"pointer\")\n",
    "    def get_minimums(word_tups):\n",
    "        arr = []\n",
    "        for wt in word_tups:\n",
    "            if wt[2] == min(top_importance_words, key = lambda t: t[2])[2]:\n",
    "                arr.append(wt)\n",
    "        return arr\n",
    "    \n",
    "    # get list of words with minimum importance score\n",
    "    minimum_import = get_minimums(top_importance_words)\n",
    "    unlisted = list(set(words_perturb).symmetric_difference(set(top_importance_words)))\n",
    "    \n",
    "    ic(unlisted)\n",
    "    \n",
    "    len_wp = len(top_importance_words)\n",
    "    len_ul = len(unlisted)\n",
    "    \n",
    "    res = []\n",
    "    for i in range(len_wp):\n",
    "        if top_importance_words[i] in minimum_import:\n",
    "            temp_wp = list(copy.deepcopy(top_importance_words))\n",
    "            if len(temp_wp) == 0:\n",
    "                break\n",
    "            \n",
    "            swapped_wp = np.array([(temp_wp) for i in range(len_ul)])\n",
    "            \n",
    "            ic(swapped_wp)\n",
    "            for j in range(len(swapped_wp)):\n",
    "                temp_sm = np.vstack((swapped_wp[j], tuple(unlisted[j])))\n",
    "                \n",
    "                res.append(temp_sm.tolist())\n",
    "            temp_wp.pop(i)\n",
    "                \n",
    "    return res\n",
    "\n",
    "def trans_dict(wordlist, perturbation_technique, lang=None):\n",
    "    translated_wordlist = {}\n",
    "    # ic(wordlist)\n",
    "    if perturbation_technique == \"codemixing\":\n",
    "        translator = GoogleTranslator(source=\"id\", target=lang)\n",
    "        \n",
    "        for word in wordlist:\n",
    "            if word.isalpha():\n",
    "                translated_wordlist[word] = translator.translate(word)\n",
    "            else:\n",
    "                translated_wordlist[word] = word\n",
    "    else:\n",
    "        for word in wordlist:\n",
    "            translated_wordlist[word] = get_synonyms(word)[0]\n",
    "            \n",
    "    return translated_wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5c0efbfd-7e66-4e0b-8f0f-3e7bb95904fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_prob(text_ls, predictor, tokenizer):\n",
    "    with torch.inference_mode():\n",
    "        subwords = np.array(tokenizer(text_ls)['input_ids'])\n",
    "        ic(text_ls)\n",
    "        ic(subwords)\n",
    "        ic(subwords.ndim)\n",
    "        # ic(subwords)\n",
    "        if subwords.ndim <= 1:\n",
    "            subwords = torch.from_numpy(subwords).view(1, -1).to(predictor.device)\n",
    "            logits = predictor(subwords)[0]\n",
    "            orig_label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "            orig_probs = F.softmax(logits, dim=-1).squeeze()\n",
    "            orig_prob = F.softmax(logits, dim=-1).squeeze()[orig_label].detach().cpu().numpy()\n",
    "        else:\n",
    "            subwords = torch.from_numpy(subwords).to(predictor.device)\n",
    "            logits = finetuned_model(subwords)[0]\n",
    "            orig_label = [t.squeeze().item() for t in torch.topk(logits, k=1, dim=-1)[1]]\n",
    "            orig_probs = F.softmax(logits, dim=-1).squeeze()\n",
    "            orig_prob = [F.softmax(logits, dim=-1).squeeze()[idx][label].detach().cpu().numpy() for idx, label in enumerate(orig_label)]\n",
    "    \n",
    "    return orig_label, orig_probs, orig_prob\n",
    "\n",
    "def attack(text_ls,\n",
    "           true_label,\n",
    "           predictor,\n",
    "           tokenizer,\n",
    "           att_ratio,\n",
    "           perturbation_technique,\n",
    "           lang_codemix=None,\n",
    "           sim_predictor=None,\n",
    "           sim_score_threshold=0,\n",
    "           sim_score_window=15,\n",
    "           batch_size=32, \n",
    "           import_score_threshold=-1.):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    label_dict = {\n",
    "        'positive': 0, \n",
    "        'neutral': 1, \n",
    "        'negative': 2}\n",
    "    \n",
    "    original_text = text_ls\n",
    "    orig_label, orig_probs, orig_prob = logit_prob(text_ls, predictor, tokenizer)\n",
    "    \n",
    "    if len(original_text.split()) < sim_score_window:\n",
    "        sim_score_threshold = 0.1  \n",
    "    \n",
    "#     SEK SALAAHHHHHH\n",
    "    if true_label != orig_label:\n",
    "        running_time = round(time.time() - start_time, 2)\n",
    "        # perturbed_text, perturbed_semantic_sim, orig_label, orig_probs, perturbed_label, perturbed_probs, translated_words, running_time\n",
    "        orig_probs = np.round(orig_probs.detach().cpu().numpy().tolist(),4)\n",
    "        return original_text, 1.000, orig_label, orig_probs, orig_label, orig_probs,'', running_time\n",
    "    else:\n",
    "        text_ls = original_text.split()\n",
    "        text_ls = [word for word in text_ls if word.isalnum()]\n",
    "        len_text = len(text_ls)\n",
    "        ic(len_text)\n",
    "        half_sim_score_window = (sim_score_window - 1) // 2\n",
    "        # num_queries = 1\n",
    "        \n",
    "        leave_1_texts = [' '.join(text_ls[:ii] + [tokenizer.mask_token] + text_ls[min(ii + 1, len_text):]) for ii in range(len_text)]\n",
    "                \n",
    "        leave_1_probs = []\n",
    "        leave_1_probs_argmax = []\n",
    "        # num_queries += len(leave_1_texts)\n",
    "        \n",
    "# torch.inference mode\n",
    "        start_time_logit = time.time()\n",
    "        leave_1_probs_argmax, leave_1_probs, _ = logit_prob(leave_1_texts, predictor, tokenizer)\n",
    "        \n",
    "        \n",
    "        end_time_logit = round(time.time() - start_time_logit, 4)\n",
    "        ic(end_time_logit)\n",
    "        \n",
    "\n",
    "        \n",
    "        start_time_arrops = time.time()\n",
    "        # leave_1_probs = torch.tensor(np.array(leave_1_probs)).to(\"cuda:0\")\n",
    "        leave_1_probs = torch.tensor(np.array(leave_1_probs.detach().cpu().numpy())).to(\"cuda:0\")\n",
    "        ic(leave_1_probs)\n",
    "        \n",
    "        orig_prob_extended=np.empty(len_text)\n",
    "        orig_prob_extended.fill(orig_prob)\n",
    "        ic(orig_prob_extended)\n",
    "        orig_prob_extended = torch.tensor(orig_prob_extended).to(\"cuda:0\")\n",
    "        \n",
    "        arr1 = orig_prob_extended - leave_1_probs[:,orig_label] + float(leave_1_probs_argmax != orig_label)\n",
    "        arr2 = (leave_1_probs.max(dim=-1)[0].to(\"cuda:0\") - orig_probs[leave_1_probs_argmax].to(\"cuda:0\"))\n",
    "        \n",
    "        import_scores = arr1*arr2\n",
    "        import_scores = [im * -1 for im in import_scores]\n",
    "        \n",
    "        words_perturb = []\n",
    "        for idx, score in sorted(enumerate(import_scores), key=lambda x: x[1], reverse=True):\n",
    "            try:\n",
    "                if score > import_score_threshold and text_ls[idx] not in stop_words_set:\n",
    "                    words_perturb.append((idx, text_ls[idx], score.item()))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(idx, len(text_ls), import_scores.shape, text_ls, len(leave_1_texts))\n",
    "        \n",
    "        num_perturbation = math.floor(len(words_perturb)*att_ratio)\n",
    "        \n",
    "        end_time_arrops = round(time.time() - start_time_arrops, 4)\n",
    "        ic(end_time_arrops)\n",
    "        \n",
    "        if num_perturbation < 1:\n",
    "            num_perturbation = 1\n",
    "        \n",
    "#       top words perturb berisi list kata terpenting yang tidak akan diswitch ketika first_codemix_sim_score < sim_score_threshold\n",
    "        top_words_perturb = words_perturb[:num_perturbation]\n",
    "        trans_word = [twp[1] for twp in top_words_perturb]\n",
    "        \n",
    "        running_time_a = round(time.time() - start_time, 4)\n",
    "        ic(running_time_a)\n",
    "        \n",
    "        start_time_pert = time.time()\n",
    "        if perturbation_technique == \"codemixing\":\n",
    "            perturbed_text,translated_words = codemix_perturbation(text_ls, lang_codemix, top_words_perturb)\n",
    "        elif perturbation_technique == \"synonym_replacement\":\n",
    "            perturbed_text,translated_words = synonym_replacement(text_ls, top_words_perturb)\n",
    "        end_time_pert = round(time.time() - start_time_pert, 4)\n",
    "        ic(end_time_pert)\n",
    "        \n",
    "        \n",
    "        start_time_use = time.time()\n",
    "        first_perturbation_sim_score = sim_predictor.semantic_sim(original_text, perturbed_text)\n",
    "        end_time_use = round(time.time() - start_time_use, 4)\n",
    "        \n",
    "        ic(end_time_use)\n",
    "        \n",
    "        ic(first_perturbation_sim_score)\n",
    "        \n",
    "        words_perturb_candidates = []\n",
    "        if first_perturbation_sim_score < sim_score_threshold:\n",
    "            words_perturb_candidates.append(top_words_perturb)\n",
    "            swapped = swap_minimum_importance_words(words_perturb, top_words_perturb)\n",
    "            for s in swapped:\n",
    "                words_perturb_candidates.append(s)\n",
    "\n",
    "            words_perturb_candidates = [[tuple(w) for w in wpc] for wpc in words_perturb_candidates]\n",
    "\n",
    "            candidate_comparison = {}\n",
    "            for wpc in words_perturb_candidates:\n",
    "                \n",
    "                if perturbation_technique == \"codemixing\":\n",
    "#                     bisa dicache, gaperlu request satu2\n",
    "                    perturbed_candidate,translated_words = codemix_perturbation(text_ls, lang_codemix, wpc)\n",
    "                elif perturbation_technique == \"synonym_replacement\":\n",
    "                    perturbed_candidate,translated_words = synonym_replacement(text_ls, wpc)\n",
    "                    \n",
    "                perturbed_candidate_sim_score = sim_predictor.semantic_sim(original_text, perturbed_candidate)\n",
    "                \n",
    "                candidate_comparison[perturbed_candidate] = (perturbed_candidate_sim_score, wpc[-1][-1], wpc)\n",
    "\n",
    "            sorted_candidate_comparison = sorted(candidate_comparison.keys(), key=lambda x: (float(candidate_comparison[x][0]), float(candidate_comparison[x][1])), reverse=True)\n",
    "\n",
    "            perturbed_text = sorted_candidate_comparison[0]\n",
    "            \n",
    "            top_words_perturb = candidate_comparison[sorted_candidate_comparison[0]][-1]\n",
    "            trans_word = [twp[1] for twp in top_words_perturb]\n",
    "            \n",
    "        perturbed_semantic_sim = sim_predictor.semantic_sim(original_text, perturbed_text)\n",
    "        if perturbed_semantic_sim < sim_score_threshold:\n",
    "            perturbed_text = original_text\n",
    "            perturbed_semantic_sim = 1.000\n",
    "        \n",
    "        perturbed_label, perturbed_probs, perturbed_prob = logit_prob(perturbed_text, predictor, tokenizer)\n",
    "        \n",
    "        \n",
    "        # translated_words = trans_dict(trans_word, perturbation_technique, lang_codemix)\n",
    "        \n",
    "        orig_probs = np.round(orig_probs.detach().cpu().numpy().tolist(),4)\n",
    "        perturbed_probs = np.round(perturbed_probs.detach().cpu().numpy().tolist(),4)\n",
    "        \n",
    "        running_time = round(time.time() - start_time, 4)\n",
    "        ic(running_time)\n",
    "        \n",
    "        # ic(perturbed_text, perturbed_semantic_sim, orig_label, orig_probs, perturbed_label, perturbed_probs, translated_words, running_time)\n",
    "        return perturbed_text, perturbed_semantic_sim, orig_label, orig_probs, perturbed_label, perturbed_probs, translated_words, running_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "efa9058f-8e82-419e-a12f-0e2abf856df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_target=\"IndoBERT\"\n",
    "downstream_task=\"sentiment\"\n",
    "attack_strategy=\"adversarial\"\n",
    "# finetune_epoch=1\n",
    "num_sample=50\n",
    "# exp_name=\n",
    "perturbation_technique=\"codemixing\"\n",
    "perturb_ratio=0.4\n",
    "perturb_lang=\"en\"\n",
    "seed=26092020\n",
    "dataset=\"valid\"\n",
    "\n",
    "set_seed(seed)\n",
    "use = USE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0130c75f-d6be-4709-8b79-6b0137067a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, config, finetuned_model = init_model(model_target, downstream_task, seed)\n",
    "w2i, i2w = load_word_index(downstream_task)\n",
    "\n",
    "train_dataset, train_loader, train_path = load_dataset_loader(downstream_task, 'train', tokenizer)\n",
    "valid_dataset, valid_loader, valid_path = load_dataset_loader(downstream_task, 'valid', tokenizer)\n",
    "test_dataset, test_loader, test_path = load_dataset_loader(downstream_task, 'test', tokenizer)\n",
    "\n",
    "finetuned_model.to(device)\n",
    "# finetuned_model = fine_tuning_model(model, i2w, train_loader, valid_loader, finetune_epoch)\n",
    "\n",
    "if dataset == \"valid\":\n",
    "    exp_dataset = valid_dataset.load_dataset(valid_path)[380:430]\n",
    "elif dataset == \"train\":\n",
    "    exp_dataset = train_dataset.load_dataset(train_path)\n",
    "# exp_dataset = dd.from_pandas(exp_dataset, npartitions=10)\n",
    "# exp_dataset = te.DataFrame.from_pandas(exp_dataset)\n",
    "text,label = None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1ecfde42-e908-43db-8383-ba3106ec701f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                        | 0/50 [00:00<?, ?it/s]ic| text_ls: 'mau marah sama koneksi indosat'\n",
      "ic| subwords: array([   2,  422, 4700,  500, 5622, 7884,    3])\n",
      "ic| subwords.ndim: 1\n",
      "ic| len_text: 5\n",
      "ic| text_ls: ['[MASK] marah sama koneksi indosat',\n",
      "              'mau [MASK] sama koneksi indosat',\n",
      "              'mau marah [MASK] koneksi indosat',\n",
      "              'mau marah sama [MASK] indosat',\n",
      "              'mau marah sama koneksi [MASK]']\n",
      "ic| subwords: array([[   2,    4, 4700,  500, 5622, 7884,    3],\n",
      "                     [   2,  422,    4,  500, 5622, 7884,    3],\n",
      "                     [   2,  422, 4700,    4, 5622, 7884,    3],\n",
      "                     [   2,  422, 4700,  500,    4, 7884,    3],\n",
      "                     [   2,  422, 4700,  500, 5622,    4,    3]])\n",
      "ic| subwords.ndim: 2\n",
      "ic| end_time_logit: 0.1402\n",
      "ic| leave_1_probs: tensor([[5.8145e-04, 1.2208e-03, 9.9820e-01],\n",
      "                           [3.1326e-03, 9.9333e-01, 3.5423e-03],\n",
      "                           [6.4712e-04, 1.3099e-03, 9.9804e-01],\n",
      "                           [5.1722e-04, 1.5441e-03, 9.9794e-01],\n",
      "                           [9.0873e-04, 1.1874e-03, 9.9790e-01]], device='cuda:0')\n",
      "ic| orig_prob_extended: array([0.99799299, 0.99799299, 0.99799299, 0.99799299, 0.99799299])\n",
      "ic| end_time_arrops: 0.3141\n",
      "ic| running_time_a: 0.9685\n",
      "ic| end_time_pert: 0.3418\n",
      "ic| end_time_use: 0.0\n",
      "ic| first_perturbation_sim_score: 1.0\n",
      "ic| text_ls: 'mau marah sama koneksi Indosat'\n",
      "ic| subwords: array([   2,  422, 4700,  500, 5622, 7884,    3])\n",
      "ic| subwords.ndim: 1\n",
      "ic| running_time: 1.6915\n",
      "  4%|██▌                                                             | 2/50 [00:01<00:42,  1.13it/s]ic| text_ls: ('pemancing peliharaan , nanti kasus nya kek anggota fpi ikut membongkar '\n",
      "              'obattan malah ditangkap .')\n",
      "ic| subwords: array([    2,  3941,  3054,  9338, 30468,  2036,  1578,  1107,   945,\n",
      "                      1225, 17550,  1820, 16065,   925,  5363,  2592,  7177, 30470,\n",
      "                         3])\n",
      "ic| subwords.ndim: 1\n",
      "ic| len_text: 13\n",
      "/tmp/ipykernel_3091670/2524955306.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  subwords = np.array(tokenizer(text_ls)['input_ids'])\n",
      "ic| text_ls: ['[MASK] peliharaan nanti kasus nya kek anggota fpi ikut membongkar obattan '\n",
      "              'malah ditangkap',\n",
      "              'pemancing [MASK] nanti kasus nya kek anggota fpi ikut membongkar obattan '\n",
      "              'malah ditangkap',\n",
      "              'pemancing peliharaan [MASK] kasus nya kek anggota fpi ikut membongkar '\n",
      "              'obattan malah ditangkap',\n",
      "              'pemancing peliharaan nanti [MASK] nya kek anggota fpi ikut membongkar '\n",
      "              'obattan malah ditangkap',\n",
      "              'pemancing peliharaan nanti kasus [MASK] kek anggota fpi ikut membongkar '\n",
      "              'obattan malah ditangkap',\n",
      "              'pemancing peliharaan nanti kasus nya [MASK] anggota fpi ikut membongkar '\n",
      "              'obattan malah ditangkap',\n",
      "              'pemancing peliharaan nanti kasus nya kek [MASK] fpi ikut membongkar obattan '\n",
      "              'malah ditangkap',\n",
      "              'pemancing peliharaan nanti kasus nya kek anggota [MASK] ikut membongkar '\n",
      "              'obattan malah ditangkap',\n",
      "              'pemancing peliharaan nanti kasus nya kek anggota fpi [MASK] membongkar '\n",
      "              'obattan malah ditangkap',\n",
      "              'pemancing peliharaan nanti kasus nya kek anggota fpi ikut [MASK] obattan '\n",
      "              'malah ditangkap',\n",
      "              'pemancing peliharaan nanti kasus nya kek anggota fpi ikut membongkar [MASK] '\n",
      "              'malah ditangkap',\n",
      "              'pemancing peliharaan nanti kasus nya kek anggota fpi ikut membongkar obattan '\n",
      "              '[MASK] ditangkap',\n",
      "              'pemancing peliharaan nanti kasus nya kek anggota fpi ikut membongkar obattan '\n",
      "              'malah [MASK]']\n",
      "ic| subwords: array([list([2, 4, 9338, 2036, 1578, 1107, 945, 1225, 17550, 1820, 16065, 925, 5363, 2592, 7177, 3]),\n",
      "                     list([2, 3941, 3054, 4, 2036, 1578, 1107, 945, 1225, 17550, 1820, 16065, 925, 5363, 2592, 7177, 3]),\n",
      "                     list([2, 3941, 3054, 9338, 4, 1578, 1107, 945, 1225, 17550, 1820, 16065, 925, 5363, 2592, 7177, 3]),\n",
      "                     list([2, 3941, 3054, 9338, 2036, 4, 1107, 945, 1225, 17550, 1820, 16065, 925, 5363, 2592, 7177, 3]),\n",
      "                     list([2, 3941, 3054, 9338, 2036, 1578, 4, 945, 1225, 17550, 1820, 16065, 925, 5363, 2592, 7177, 3]),\n",
      "                     list([2, 3941, 3054, 9338, 2036, 1578, 1107, 4, 1225, 17550, 1820, 16065, 925, 5363, 2592, 7177, 3]),\n",
      "                     list([2, 3941, 3054, 9338, 2036, 1578, 1107, 945, 4, 17550, 1820, 16065, 925, 5363, 2592, 7177, 3]),\n",
      "                     list([2, 3941, 3054, 9338, 2036, 1578, 1107, 945, 1225, 4, 1820, 16065, 925, 5363, 2592, 7177, 3]),\n",
      "                     list([2, 3941, 3054, 9338, 2036, 1578, 1107, 945, 1225, 17550, 4, 16065, 925, 5363, 2592, 7177, 3]),\n",
      "                     list([2, 3941, 3054, 9338, 2036, 1578, 1107, 945, 1225, 17550, 1820, 4, 925, 5363, 2592, 7177, 3]),\n",
      "                     list([2, 3941, 3054, 9338, 2036, 1578, 1107, 945, 1225, 17550, 1820, 16065, 4, 2592, 7177, 3]),\n",
      "                     list([2, 3941, 3054, 9338, 2036, 1578, 1107, 945, 1225, 17550, 1820, 16065, 925, 5363, 4, 7177, 3]),\n",
      "                     list([2, 3941, 3054, 9338, 2036, 1578, 1107, 945, 1225, 17550, 1820, 16065, 925, 5363, 2592, 4, 3])],\n",
      "                    dtype=object)\n",
      "ic| subwords.ndim: 1\n",
      "  4%|██▌                                                             | 2/50 [00:02<01:04,  1.34s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [70]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m     exp_dataset[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperturbed_text\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperturbed_semantic_sim\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_label\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_proba\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperturbed_label\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperturbed_proba\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslated_word(s)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrunning_time(s)\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[43mexp_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext_ls\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrue_label\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentiment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfinetuned_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43matt_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mperturb_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43mperturbation_technique\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mperturbation_technique\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlang_codemix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mperturb_lang\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43msim_predictor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muse\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexpand\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m downstream_task \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     16\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/std.py:814\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    816\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py:8833\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   8822\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m   8824\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m   8825\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   8826\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   8831\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   8832\u001b[0m )\n\u001b[0;32m-> 8833\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/apply.py:727\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[0;32m--> 727\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/apply.py:851\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/apply.py:867\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    868\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    869\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    870\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    871\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/std.py:809\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[1;32m    808\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 809\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [70]\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      2\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m     exp_dataset[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperturbed_text\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperturbed_semantic_sim\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_label\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_proba\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperturbed_label\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperturbed_proba\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslated_word(s)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrunning_time(s)\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m exp_dataset\u001b[38;5;241m.\u001b[39mprogress_apply(\n\u001b[0;32m----> 5\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext_ls\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrue_label\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentiment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfinetuned_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43matt_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mperturb_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43mperturbation_technique\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mperturbation_technique\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlang_codemix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mperturb_lang\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43msim_predictor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muse\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, result_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpand\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m downstream_task \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     16\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Input \u001b[0;32mIn [69]\u001b[0m, in \u001b[0;36mattack\u001b[0;34m(text_ls, true_label, predictor, tokenizer, att_ratio, perturbation_technique, lang_codemix, sim_predictor, sim_score_threshold, sim_score_window, batch_size, import_score_threshold)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;66;03m# num_queries += len(leave_1_texts)\u001b[39;00m\n\u001b[1;32m     68\u001b[0m         \n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# torch.inference mode\u001b[39;00m\n\u001b[1;32m     70\u001b[0m         start_time_logit \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 71\u001b[0m         leave_1_probs_argmax, leave_1_probs, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlogit_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleave_1_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m         end_time_logit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time_logit, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     75\u001b[0m         ic(end_time_logit)\n",
      "Input \u001b[0;32mIn [69]\u001b[0m, in \u001b[0;36mlogit_prob\u001b[0;34m(text_ls, predictor, tokenizer)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# ic(subwords)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m subwords\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m----> 9\u001b[0m     subwords \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubwords\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(predictor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     10\u001b[0m     logits \u001b[38;5;241m=\u001b[39m predictor(subwords)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     11\u001b[0m     orig_label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(logits, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "if downstream_task == 'sentiment':\n",
    "    text = 'text'\n",
    "    label = 'sentiment'\n",
    "    exp_dataset[['perturbed_text', 'perturbed_semantic_sim', 'pred_label', 'pred_proba', 'perturbed_label', 'perturbed_proba', 'translated_word(s)', 'running_time(s)']] = exp_dataset.progress_apply(\n",
    "        lambda row: attack(\n",
    "            text_ls = row.text,\n",
    "            true_label = row.sentiment,\n",
    "            predictor = finetuned_model,\n",
    "            tokenizer = tokenizer, \n",
    "            att_ratio = perturb_ratio,\n",
    "            perturbation_technique = perturbation_technique,\n",
    "            lang_codemix = perturb_lang,\n",
    "            sim_predictor = use), axis=1, result_type='expand'\n",
    "    )\n",
    "elif downstream_task == 'emotion':\n",
    "    text = 'tweet'\n",
    "    label = 'label'\n",
    "    exp_dataset[['perturbed_text', 'perturbed_semantic_sim', 'pred_label', 'pred_proba', 'perturbed_label', 'perturbed_proba', 'translated_word(s)', 'running_time(s)']] = exp_dataset.progress_apply(\n",
    "        lambda row: attack(\n",
    "            text_ls = row.tweet.values,\n",
    "            true_label = row.label.values,\n",
    "            predictor = finetuned_model,\n",
    "            tokenizer = tokenizer, \n",
    "            att_ratio = perturb_ratio,\n",
    "            perturbation_technique = perturbation_technique,\n",
    "            lang_codemix = perturb_lang,\n",
    "            sim_predictor = use), axis=1, result_type='expand'\n",
    "    )\n",
    "\n",
    "before_attack = accuracy_score(exp_dataset[label], exp_dataset['pred_label'])\n",
    "after_attack = accuracy_score(exp_dataset[label], exp_dataset['perturbed_label'])\n",
    "\n",
    "exp_dataset.loc[exp_dataset.index[0], 'before_attack_acc'] = before_attack\n",
    "exp_dataset.loc[exp_dataset.index[0], 'after_attack_acc'] = after_attack\n",
    "exp_dataset.loc[exp_dataset.index[0], 'avg_semantic_sim'] = exp_dataset[\"perturbed_semantic_sim\"].mean()\n",
    "exp_dataset.loc[exp_dataset.index[0], 'avg_running_time(s)'] = exp_dataset[\"running_time(s)\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe121c30-5649-4277-8805-0a7f3c59d6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83b16bb-b416-4519-909b-ad6211215ee6",
   "metadata": {},
   "source": [
    "### After Logit Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "913a33b3-e1fa-401e-9e3e-20eec418fa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len_text: 5\n",
      "ic| end_time_logit: 0.0158\n",
      "ic| leave_1_probs: tensor([[5.8145e-04, 1.2208e-03, 9.9820e-01],\n",
      "                           [3.1326e-03, 9.9333e-01, 3.5423e-03],\n",
      "                           [6.4712e-04, 1.3099e-03, 9.9804e-01],\n",
      "                           [5.1722e-04, 1.5441e-03, 9.9794e-01],\n",
      "                           [9.0873e-04, 1.1874e-03, 9.9790e-01]], device='cuda:0')\n",
      "ic| orig_prob_extended: array([0.99799299, 0.99799299, 0.99799299, 0.99799299, 0.99799299])\n",
      "ic| end_time_arrops: 0.2159\n",
      "ic| running_time_a: 0.3671\n",
      "ic| end_time_pert: 0.2674\n",
      "ic| end_time_use: 0.0\n",
      "ic| first_perturbation_sim_score: 1.0\n",
      "ic| running_time: 0.8207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('mau marah sama koneksi Indosat',\n",
       " 1.0,\n",
       " 2,\n",
       " array([5.00e-04, 1.50e-03, 9.98e-01]),\n",
       " 2,\n",
       " array([5.00e-04, 1.50e-03, 9.98e-01]),\n",
       " {'indosat': 'Indosat'},\n",
       " 0.8207)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack(\n",
    "    text_ls = exp_dataset.iloc[[0]].text.item(),\n",
    "    true_label = exp_dataset.iloc[[2]].sentiment.item(),\n",
    "    predictor = finetuned_model,\n",
    "    tokenizer = tokenizer, \n",
    "    att_ratio = perturb_ratio,\n",
    "    perturbation_technique = perturbation_technique,\n",
    "    lang_codemix = perturb_lang,\n",
    "    sim_predictor = use\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdf9577-ef43-407b-b707-252c0ea1853f",
   "metadata": {},
   "source": [
    "### Coba optimize di USE nya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0752ac79-d4fc-4f0c-be49-d1117ac29329",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack(\n",
    "    text_ls = exp_dataset.iloc[[2]].text.item(),\n",
    "    true_label = exp_dataset.iloc[[2]].sentiment.item(),\n",
    "    predictor = finetuned_model,\n",
    "    tokenizer = tokenizer, \n",
    "    att_ratio = perturb_ratio,\n",
    "    perturbation_technique = perturbation_technique,\n",
    "    lang_codemix = perturb_lang,\n",
    "    sim_predictor = use\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83533730-b3c7-471a-a06f-6233e86e8d2e",
   "metadata": {},
   "source": [
    "### Coba Optimize step 2+codemix perturbation pake batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1475f7-9a37-4239-90ac-47db1fa26e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack(\n",
    "    text_ls = exp_dataset.iloc[[48]].text.item(),\n",
    "    true_label = 0,\n",
    "    predictor = finetuned_model,\n",
    "    tokenizer = tokenizer, \n",
    "    att_ratio = perturb_ratio,\n",
    "    perturbation_technique = perturbation_technique,\n",
    "    lang_codemix = perturb_lang,\n",
    "    sim_predictor = use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f70e96-30c9-4fd1-b2ae-1d3c61bdd7d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4100b193-fdc3-4d59-bc11-ab02fa4611f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def codemix_perturbation(words, target_lang, words_perturb):\n",
    "    \"\"\"\n",
    "    'su': 'sundanese'\n",
    "    'jw': 'javanese'\n",
    "    'ms': 'malay'\n",
    "    'en': 'english'\n",
    "    \"\"\"\n",
    "    translator = GoogleTranslator(source=\"id\", target=target_lang)\n",
    "    new_wp = []\n",
    "    for wp in words_perturb:\n",
    "        if wp[1].isalpha():\n",
    "            new_wp.append(wp[1])\n",
    "    \n",
    "    if len(new_wp) == 1:\n",
    "        new_wp_trans = dict(zip(new_wp, translator.translate(new_wp[0])))\n",
    "    elif len(new_wp) == 0:\n",
    "        return ' '.join(new_words), {}\n",
    "        \n",
    "    new_wp_trans = dict(zip(new_wp, translator.translate_batch(new_wp)))\n",
    "    \n",
    "    supported_langs = [\"su\", \"jw\", \"ms\", \"en\", \"fr\", \"it\"]\n",
    "    \n",
    "    if target_lang not in supported_langs:\n",
    "        raise ValueError('Language Unavailable')\n",
    "    \n",
    "    new_words = words.copy()\n",
    "    \n",
    "    if len(words_perturb) >= 1:\n",
    "        for perturb_word in new_wp_trans.keys():\n",
    "            new_words = [new_wp_trans[perturb_word] if word == perturb_word and word.isalpha() else word for word in new_words]\n",
    "\n",
    "    return ' '.join(new_words), new_wp_trans\n",
    "\n",
    "def synonym_replacement(words, words_perturb):    \n",
    "    # new_words = words.copy()\n",
    "    new_wp = []\n",
    "    for wp in words_perturb:\n",
    "        if wp[1].isalpha():\n",
    "            new_wp.append(wp[1])\n",
    "    \n",
    "    if len(new_wp) == 1:\n",
    "        new_wp_trans = dict(zip(new_wp, get_synonyms(new_wp[0])[0]))\n",
    "    elif len(new_wp) == 0:\n",
    "        return ' '.join(new_words), {}\n",
    "    \n",
    "    wp_trans=[]\n",
    "    for wp in new_wp:\n",
    "        wp_trans.append(get_synonyms(wp)[0])\n",
    "        print(wp,wp_trans)\n",
    "    \n",
    "    new_wp_trans = dict(zip(new_wp, wp_trans))\n",
    "    \n",
    "    new_words = words.copy()\n",
    "    \n",
    "    if len(words_perturb) >= 1:\n",
    "        for perturb_word in new_wp_trans.keys():\n",
    "            new_words = [new_wp_trans[perturb_word[1]] if word == perturb_word[1] and word.isalpha() else word for word in new_words]\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "    \n",
    "    return sentence, new_wp_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddac99a0-2976-48c0-8c91-20e9c86de269",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'buat kalian yang cuma tahu tempat makan malam romantis yang ada di dago , boleh dicoba deh makan di burgundy sini , di sini sih menurut saya tempat yang enak banget buat ngadem selain tempat nya yang hening kita juga bisa melihat rerumputan yang hijau meski cuma di taman nya , hehehe . tapi balik lagi buat saya sih makanan , haha . tidak usah diragukan .'\n",
    "words = word.split()\n",
    "wp = [(59, 'diragukan', 0.001480978060399707), (27, 'enak', 0.0006103660272316347), (18, 'burgundy', 0.0005351185233486433), (55, 'makanan', 0.000532135112141674), (12, 'dago', 0.0004499789485095107), (8, 'romantis', 0.0003965279976370084), (46, 'taman', 0.00039581217539819136), (40, 'rerumputan', 0.00039318750260974866), (7, 'malam', 0.000384060906586825)]\n",
    "target_lang = \"en\"\n",
    "\n",
    "start_time = time.time()\n",
    "print(synonym_replacement(words, wp))\n",
    "print(round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92985e6-016d-4609-bb85-70034bbeb2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbert-sentiment-codemixing-jw-adv-0.6-valid\n",
    "# 279/1260\n",
    "850/1260\n",
    "new_wp_trans = dict(zip(new_wp, translator.translate(new_wp[0])))\n",
    "IndexError: list index out of range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00c2493-4560-4d0a-b00a-ac4ec680cb3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c95ec8-1c18-42da-8abc-8bc0bdab7b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e3f4a9-9263-40c8-bc14-523ab596c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack(\n",
    "    text_ls = exp_dataset.iloc[[5]].text.item(),\n",
    "    true_label = exp_dataset.iloc[[5]].sentiment.item(),\n",
    "    predictor = finetuned_model,\n",
    "    tokenizer = tokenizer, \n",
    "    att_ratio = 0.6,\n",
    "    perturbation_technique = perturbation_technique,\n",
    "    lang_codemix = \"jw\",\n",
    "    sim_predictor = use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8d3139-25ad-4e6e-b504-d88b2a388cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24548f59-5e35-4aad-9b84-0ae0a81c5d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9583c461-28d2-4c46-b2ed-0dc9f27a2479",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dataset.iloc[[7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aacafc0-3376-4cd2-884b-dc98c7967b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01636e7-36cb-4cf4-aee6-da4316774ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs:\", len(physical_devices))\n",
    "# !pip list cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55317fa7-d77c-40a1-9ecd-a145385008e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['diragukan', 'enak', 'burgundy', 'makanan', 'dago', 'romantis', 'taman', 'rerumputan', 'malam']\n",
    "\n",
    "start_time = time.time()\n",
    "translated_1 = []\n",
    "translator = GoogleTranslator(source=\"id\", target=\"en\")\n",
    "for txt in texts:\n",
    "    translated_1.append(translator.translate(txt))\n",
    "print(round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd92533-750e-4274-a722-04d0f0c89141",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "translated_2 = translator.translate_batch(texts)\n",
    "print(round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469821c-a347-4a04-88df-c9fb8ad61798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !. myenv/bin/activate\n",
    "# !pip uninstall tensorflow -y\n",
    "# !pip uninstall tensorflow-gpu -y\n",
    "\n",
    "!pip install tensorflow --ignore-installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "318575e4-3cd4-4076-a50c-3f83f6badc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.17.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "442ae727-69da-4521-8280-fea661f194a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_prob(text_ls, predictor, tokenizer):\n",
    "    # original_text = text_ls\n",
    "    # print(text_ls)\n",
    "    subwords = tokenizer.encode(text_ls)\n",
    "    subwords = torch.LongTensor(subwords).view(1, -1).to(predictor.device)\n",
    "\n",
    "    logits = predictor(subwords)[0]\n",
    "    orig_label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "    \n",
    "    orig_probs = F.softmax(logits, dim=-1).squeeze()\n",
    "    orig_prob = F.softmax(logits, dim=-1).squeeze()[orig_label].detach().cpu().numpy()\n",
    "    \n",
    "    return orig_label, orig_probs, orig_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad62f6-736d-4323-a8c7-7b011b8004e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64\n",
    "!echo $LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ccd6a637-af2c-4af6-ac25-53081b0922d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_prob(text_ls, predictor, tokenizer):\n",
    "    with torch.inference_mode():\n",
    "        subwords = np.array(tokenizer(text_ls)['input_ids'])\n",
    "        # ic(subwords)\n",
    "        if subwords.ndim <= 1:\n",
    "            subwords = torch.tensor(subwords).view(1, -1).to(predictor.device)\n",
    "            logits = predictor(subwords)[0]\n",
    "            orig_label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "            orig_probs = F.softmax(logits, dim=-1).squeeze()\n",
    "            orig_prob = F.softmax(logits, dim=-1).squeeze()[orig_label].detach().cpu().numpy()\n",
    "        else:\n",
    "            subwords = torch.tensor(subwords).to(predictor.device)\n",
    "            logits = finetuned_model(subwords)[0]\n",
    "            orig_label = [t.squeeze().item() for t in torch.topk(logits, k=1, dim=-1)[1]]\n",
    "            orig_probs = F.softmax(logits, dim=-1).squeeze()\n",
    "            orig_prob = [F.softmax(logits, dim=-1).squeeze()[idx][label].detach().cpu().numpy() for idx, label in enumerate(orig_label)]\n",
    "    \n",
    "    return orig_label, orig_probs, orig_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b2ddd7-1a1c-44b5-9637-e04dab2b3855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,    4, 4700,  500, 5622, 7884,    3],\n",
       "        [   2,  422,    4,  500, 5622, 7884,    3],\n",
       "        [   2,  422, 4700,    4, 5622, 7884,    3],\n",
       "        [   2,  422, 4700,  500,    4, 7884,    3],\n",
       "        [   2,  422, 4700,  500, 5622,    4,    3]], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subwords = np.array([\n",
    "    [2, 4, 4700, 500, 5622, 7884, 3],\n",
    "    [2, 422, 4, 500, 5622, 7884, 3],\n",
    "    [2, 422, 4700, 4, 5622, 7884, 3],\n",
    "    [2, 422, 4700, 500, 4, 7884, 3],\n",
    "    [2, 422, 4700, 500, 5622, 4, 3]\n",
    "])\n",
    "subwords = torch.tensor(subwords).to(finetuned_model.device)\n",
    "subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5647c3cf-f1dc-4535-850a-13463bbc3232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6abd9cb2-0c47-44d7-9755-dc348596bae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "subwords = np.array([2, 4, 4700, 500, 5622, 7884, 3])\n",
    "print(subwords.ndim)\n",
    "subwords = torch.tensor(subwords).view(1,-1).to(finetuned_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7780051c-29bc-4ed4-8032-7e5194c1d357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,    4, 4700,  500, 5622, 7884,    3]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6b0de845-7c22-4276-a5c2-7850b131fd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| subwords: tensor([[   2,    4, 4700,  500, 5622, 7884,    3],\n",
      "                      [   2,  422,    4,  500, 5622, 7884,    3],\n",
      "                      [   2,  422, 4700,    4, 5622, 7884,    3],\n",
      "                      [   2,  422, 4700,  500,    4, 7884,    3],\n",
      "                      [   2,  422, 4700,  500, 5622,    4,    3]], device='cuda:0')\n",
      "ic| logits: tensor([[-2.7573, -2.0155,  4.6909],\n",
      "                    [-2.1742,  3.5850, -2.0513],\n",
      "                    [-2.7181, -2.0129,  4.6229],\n",
      "                    [-2.9572, -1.8635,  4.6078],\n",
      "                    [-2.4672, -2.1997,  4.5342]], device='cuda:0',\n",
      "                   grad_fn=<AddmmBackward0>)\n",
      "ic| orig_label: [2, 1, 2, 2, 2]\n",
      "ic| orig_probs: tensor([[5.8145e-04, 1.2208e-03, 9.9820e-01],\n",
      "                        [3.1326e-03, 9.9333e-01, 3.5423e-03],\n",
      "                        [6.4712e-04, 1.3099e-03, 9.9804e-01],\n",
      "                        [5.1722e-04, 1.5441e-03, 9.9794e-01],\n",
      "                        [9.0873e-04, 1.1874e-03, 9.9790e-01]], device='cuda:0',\n",
      "                       grad_fn=<SqueezeBackward0>)\n",
      "ic| orig_prob: [array(0.99819773, dtype=float32),\n",
      "                array(0.9933252, dtype=float32),\n",
      "                array(0.99804294, dtype=float32),\n",
      "                array(0.9979386, dtype=float32),\n",
      "                array(0.9979038, dtype=float32)]\n",
      "ic| end_time_use: 0.5198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5198"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time_use = time.time()\n",
    "subwords = np.array([\n",
    "    [2, 4, 4700, 500, 5622, 7884, 3],\n",
    "    [2, 422, 4, 500, 5622, 7884, 3],\n",
    "    [2, 422, 4700, 4, 5622, 7884, 3],\n",
    "    [2, 422, 4700, 500, 4, 7884, 3],\n",
    "    [2, 422, 4700, 500, 5622, 4, 3]\n",
    "])\n",
    "subwords = torch.tensor(subwords).to(finetuned_model.device)\n",
    "ic(subwords)\n",
    "logits = finetuned_model(subwords)[0]\n",
    "ic(logits)\n",
    "orig_label = [t.squeeze().item() for t in torch.topk(logits, k=1, dim=-1)[1]]\n",
    "ic(orig_label)\n",
    "orig_probs = F.softmax(logits, dim=-1).squeeze()\n",
    "ic(orig_probs)\n",
    "orig_prob = [F.softmax(logits, dim=-1).squeeze()[idx][label].detach().cpu().numpy() for idx, label in enumerate(orig_label)]\n",
    "ic(orig_prob)\n",
    "end_time_use = round(time.time() - start_time_use, 4)\n",
    "ic(end_time_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ca52ab1d-96c8-4039-a654-b3017c86f7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0.99819773\n",
      "1\n",
      "0.9933252\n",
      "2\n",
      "0.99804294\n",
      "2\n",
      "0.9979386\n",
      "2\n",
      "0.9979038\n",
      "[array(0.99819773, dtype=float32), array(0.9933252, dtype=float32), array(0.99804294, dtype=float32), array(0.9979386, dtype=float32), array(0.9979038, dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# orig_label\n",
    "# orig_probs = F.softmax(logits, dim=-1).squeeze()\n",
    "# orig_probs\n",
    "for idx, label in enumerate(orig_label):\n",
    "    print(label)\n",
    "    print(F.softmax(logits, dim=-1).squeeze()[idx][label].detach().cpu().numpy())\n",
    "orig_prob = [F.softmax(logits, dim=-1).squeeze()[idx][label].detach().cpu().numpy() for idx, label in enumerate(orig_label)]\n",
    "# orig_prob = F.softmax(logits, dim=-1).squeeze()\n",
    "print(orig_prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
