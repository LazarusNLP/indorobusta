{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "546ac2b4-f52d-4f77-8b42-4bc237320037",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15edb277-e4c7-4c1b-998c-32c720b17121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/m13518040/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.utils_init_model import text_logit, fine_tuning_model, eval_model, init_model, logit_prob, load_word_index\n",
    "import time\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "import math\n",
    "# from .attack_helper import get_synonyms, codemix_perturbation, synonym_replacement, swap_minimum_importance_words, trans_dict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "stop_words_set = []\n",
    "for w in stopwords.words('indonesian'):\n",
    "    stop_words_set.append(w)\n",
    "\n",
    "from icecream import ic\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import copy\n",
    "from deep_translator import GoogleTranslator\n",
    "import numpy as np\n",
    "import random\n",
    "# from utils.utils_semantic_use import USE\n",
    "from utils.utils_init_dataset import set_seed, load_dataset_loader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "# tf.enable_eager_execution()\n",
    "# import tensorflow.compat.v1 as tf\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d543c47-539a-4917-aea4-6e26eea1c708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jax\n",
    "# !pip install --upgrade \"jax[cuda]\" -f https://storage.googleapis.com/jax-releases/jax_releases.html  # Note: wheels only available on linux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cef73d5-b954-43f7-b4b2-4b1d8b46cd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class USE(object):\n",
    "    def __init__(self):\n",
    "        import tensorflow_hub as hub\n",
    "        super(USE, self).__init__()\n",
    "        module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
    "        self.embed = hub.load(module_url)\n",
    "        # print(tf.executing_eagerly())\n",
    "        # self.embed = hub.load( DataManager.load(\"AttackAssist.UniversalSentenceEncoder\") )\n",
    "\n",
    "    def semantic_sim(self, sentA : str, sentB : str) -> float:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sentA: The first sentence.\n",
    "            sentB: The second sentence.\n",
    "\n",
    "        Returns:\n",
    "            Cosine distance between two sentences.\n",
    "        \n",
    "        \"\"\"\n",
    "        ret = self.embed([sentA, sentB]).numpy()\n",
    "        return ret[0].dot(ret[1]) / (np.linalg.norm(ret[0]) * np.linalg.norm(ret[1]))\n",
    "\n",
    "    def after_attack(self, input, adversarial_sample):\n",
    "        if adversarial_sample is not None:\n",
    "            return self.calc_score(input[\"x\"], adversarial_sample)\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "# USE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d653914f-8ae1-44af-b961-ba0fbc4dba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class USE(object):\n",
    "#     def __init__(self, module_url=None):\n",
    "#         super(USE, self).__init__()\n",
    "#         module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
    "#         self.embed = hub.load(module_url)\n",
    "        \n",
    "#         def embed(input):\n",
    "#             return model(input)\n",
    "        \n",
    "#         # config = tf.compat.v1.ConfigProto()\n",
    "#         # config.gpu_options.allow_growth = True\n",
    "#         # self.sess = tf.compat.v1.Session(config=config)\n",
    "#         # self.sess.run([tf.compat.v1.global_variables_initializer(), tf.compat.v1.tables_initializer()])\n",
    "\n",
    "#     def semantic_sim(self, sents1, sents2):\n",
    "#         # with tf.Session() as session:\n",
    "#         #     session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "#         #     with tf.device('GPU'):\n",
    "#         #         message_embeddings_ = session.run(self.embed([sents1, sents2]))\n",
    "        \n",
    "#         if sents1.lower() == sents2.lower():\n",
    "#             return 1.000\n",
    "        \n",
    "#         message_embeddings_ = self.embed([sents1, sents2])\n",
    "#         # message_embeddings_ = message_embeddings_.eval(session=self.sess)\n",
    "#         corr = np.tensordot(message_embeddings_, message_embeddings_, axes=(-1,-1))\n",
    "        \n",
    "#         if corr[0][1] > 1:\n",
    "#             return 1.000\n",
    "#         return corr[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3576aabe-dd88-40dd-8ff3-64d225667ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "def get_synonyms(word):\n",
    "    if word in stop_words_set:\n",
    "        return [word]\n",
    "\n",
    "    word_lemmas = wordnet.lemmas(word, lang=\"ind\")\n",
    "    \n",
    "    hypernyms = []\n",
    "    for lem in word_lemmas:\n",
    "        hypernyms.append(lem.synset().hypernyms())\n",
    "\n",
    "    if not any(hypernyms):\n",
    "        return [word]\n",
    "    \n",
    "    lemma_corp = []\n",
    "    \n",
    "    for hypernym in hypernyms:\n",
    "        if(len(hypernym) < 1):\n",
    "            continue\n",
    "        else:\n",
    "            lemma_corp.append(hypernym[0].lemmas(lang=\"ind\"))\n",
    "            \n",
    "    lemmas = set()\n",
    "    for list_lemmas in lemma_corp:\n",
    "        if(len(list_lemmas) < 1):\n",
    "            lemmas.add(word)\n",
    "        else:\n",
    "            for l in list_lemmas:\n",
    "                lemmas.add(l.name())\n",
    "    \n",
    "    clean_synonyms = set()\n",
    "    for syn in lemmas.copy():\n",
    "        synonym = syn.replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "        synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "        clean_synonyms.add(synonym) \n",
    "    if word in clean_synonyms:\n",
    "        clean_synonyms.remove(word)\n",
    "    \n",
    "    if len(list(clean_synonyms)) < 1:\n",
    "        return [word]\n",
    "    else:\n",
    "        return list(clean_synonyms)\n",
    "\n",
    "def codemix_perturbation(words, target_lang, words_perturb):\n",
    "    \"\"\"\n",
    "    'su': 'sundanese'\n",
    "    'jw': 'javanese'\n",
    "    'ms': 'malay'\n",
    "    'en': 'english'\n",
    "    \"\"\"\n",
    "    translator = GoogleTranslator(source=\"id\", target=target_lang)\n",
    "    \n",
    "    new_wp = []\n",
    "    for wp in words_perturb:\n",
    "        new_wp.append(wp[1])\n",
    "    \n",
    "    new_wp_trans = dict(zip(new_wp, translator.translate_batch(new_wp)))\n",
    "    \n",
    "    supported_langs = [\"su\", \"jw\", \"ms\", \"en\", \"fr\", \"it\"]\n",
    "    \n",
    "    if target_lang not in supported_langs:\n",
    "        raise ValueError('Language Unavailable')\n",
    "    \n",
    "    if len(words_perturb) >= 1:\n",
    "        for perturb_word in new_wp_trans.keys():\n",
    "            new_words = [new_wp_trans[perturb_word] if word == perturb_word and word.isalpha() else word for word in words]\n",
    "\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def synonym_replacement(words, words_perturb):    \n",
    "    # new_words = words.copy()\n",
    "       \n",
    "    if len(words_perturb) >= 1:\n",
    "        for perturb_word in words_perturb:\n",
    "            new_words = [get_synonyms(word)[0] if word == perturb_word[1] and word.isalpha() else word for word in words]\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "# fungsi untuk mencari kandidat lain ketika sebuah kandidat perturbasi kurang dari sim_score_threshold\n",
    "def swap_minimum_importance_words(words_perturb, top_importance_words):\n",
    "    print(\"pointer\")\n",
    "    def get_minimums(word_tups):\n",
    "        arr = []\n",
    "        for wt in word_tups:\n",
    "            if wt[2] == min(top_importance_words, key = lambda t: t[2])[2]:\n",
    "                arr.append(wt)\n",
    "        return arr\n",
    "    \n",
    "    # get list of words with minimum importance score\n",
    "    minimum_import = get_minimums(top_importance_words)\n",
    "    unlisted = list(set(words_perturb).symmetric_difference(set(top_importance_words)))\n",
    "    \n",
    "    ic(unlisted)\n",
    "    \n",
    "    len_wp = len(top_importance_words)\n",
    "    len_ul = len(unlisted)\n",
    "    \n",
    "    res = []\n",
    "    for i in range(len_wp):\n",
    "        if top_importance_words[i] in minimum_import:\n",
    "            temp_wp = list(copy.deepcopy(top_importance_words))\n",
    "            if len(temp_wp) == 0:\n",
    "                break\n",
    "            \n",
    "            swapped_wp = np.array([(temp_wp) for i in range(len_ul)])\n",
    "            \n",
    "            ic(swapped_wp)\n",
    "            for j in range(len(swapped_wp)):\n",
    "                temp_sm = np.vstack((swapped_wp[j], tuple(unlisted[j])))\n",
    "                \n",
    "                res.append(temp_sm.tolist())\n",
    "            temp_wp.pop(i)\n",
    "                \n",
    "    return res\n",
    "\n",
    "def trans_dict(wordlist, perturbation_technique, lang=None):\n",
    "    translated_wordlist = {}\n",
    "    # ic(wordlist)\n",
    "    if perturbation_technique == \"codemixing\":\n",
    "        translator = GoogleTranslator(source=\"id\", target=lang)\n",
    "        \n",
    "        for word in wordlist:\n",
    "            if word.isalpha():\n",
    "                translated_wordlist[word] = translator.translate(word)\n",
    "            else:\n",
    "                translated_wordlist[word] = word\n",
    "    else:\n",
    "        for word in wordlist:\n",
    "            translated_wordlist[word] = get_synonyms(word)[0]\n",
    "            \n",
    "    return translated_wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c0efbfd-7e66-4e0b-8f0f-3e7bb95904fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack(text_ls,\n",
    "           true_label,\n",
    "           predictor,\n",
    "           tokenizer,\n",
    "           att_ratio,\n",
    "           perturbation_technique,\n",
    "           lang_codemix=None,\n",
    "           sim_predictor=None,\n",
    "           sim_score_threshold=0,\n",
    "           sim_score_window=15,\n",
    "           batch_size=32, \n",
    "           import_score_threshold=-1.):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    label_dict = {\n",
    "        'positive': 0, \n",
    "        'neutral': 1, \n",
    "        'negative': 2}\n",
    "    \n",
    "    original_text = text_ls\n",
    "    orig_label, orig_probs, orig_prob = logit_prob(text_ls, predictor, tokenizer)\n",
    "    \n",
    "    if len(original_text.split()) < sim_score_window:\n",
    "        sim_score_threshold = 0.1  \n",
    "    \n",
    "#     SEK SALAAHHHHHH\n",
    "    if true_label != orig_label:\n",
    "        running_time = round(time.time() - start_time, 2)\n",
    "        # perturbed_text, perturbed_semantic_sim, orig_label, orig_probs, perturbed_label, perturbed_probs, translated_words, running_time\n",
    "        orig_probs = np.round(orig_probs.detach().cpu().numpy().tolist(),4)\n",
    "        return original_text, 1.000, orig_label, orig_probs, orig_label, orig_probs,'', running_time\n",
    "    else:\n",
    "        text_ls = original_text.split()\n",
    "        text_ls = [word for word in text_ls if word.isalnum()]\n",
    "        len_text = len(text_ls)\n",
    "        ic(len_text)\n",
    "        half_sim_score_window = (sim_score_window - 1) // 2\n",
    "        # num_queries = 1\n",
    "        \n",
    "        leave_1_texts = [' '.join(text_ls[:ii] + [tokenizer.mask_token] + text_ls[min(ii + 1, len_text):]) for ii in range(len_text)]\n",
    "                \n",
    "        leave_1_probs = []\n",
    "        leave_1_probs_argmax = []\n",
    "        # num_queries += len(leave_1_texts)\n",
    "        \n",
    "# torch.inference mode\n",
    "        for text_leave_1 in leave_1_texts:\n",
    "            orig_label_leave_1, orig_probs_leave_1, orig_prob_leave_1 = logit_prob(text_leave_1, predictor, tokenizer)\n",
    "            leave_1_probs.append(orig_probs_leave_1.detach().cpu().numpy())\n",
    "            leave_1_probs_argmax.append(orig_label_leave_1)\n",
    "        \n",
    "        running_time_a = round(time.time() - start_time, 2)\n",
    "        ic(running_time_a)\n",
    "        \n",
    "        leave_1_probs = torch.tensor(leave_1_probs).to(\"cuda:0\")\n",
    "        \n",
    "        orig_prob_extended=np.empty(len_text)\n",
    "        orig_prob_extended.fill(orig_prob)\n",
    "        orig_prob_extended = torch.tensor(orig_prob_extended).to(\"cuda:0\")\n",
    "        \n",
    "        arr1 = orig_prob_extended - leave_1_probs[:,orig_label] + float(leave_1_probs_argmax != orig_label)\n",
    "        arr2 = (leave_1_probs.max(dim=-1)[0].to(\"cuda:0\") - orig_probs[leave_1_probs_argmax].to(\"cuda:0\"))\n",
    "        \n",
    "        import_scores = arr1*arr2\n",
    "        import_scores = [im * -1 for im in import_scores]\n",
    "        \n",
    "        words_perturb = []\n",
    "        for idx, score in sorted(enumerate(import_scores), key=lambda x: x[1], reverse=True):\n",
    "            try:\n",
    "                if score > import_score_threshold and text_ls[idx] not in stop_words_set:\n",
    "                    words_perturb.append((idx, text_ls[idx], score.item()))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(idx, len(text_ls), import_scores.shape, text_ls, len(leave_1_texts))\n",
    "        \n",
    "        num_perturbation = math.floor(len(words_perturb)*att_ratio)\n",
    "        \n",
    "        if num_perturbation < 1:\n",
    "            num_perturbation = 1\n",
    "        \n",
    "#       top words perturb berisi list kata terpenting yang tidak akan diswitch ketika first_codemix_sim_score < sim_score_threshold\n",
    "        top_words_perturb = words_perturb[:num_perturbation]\n",
    "        trans_word = [twp[1] for twp in top_words_perturb]\n",
    "\n",
    "        if perturbation_technique == \"codemixing\":\n",
    "            perturbed_text = codemix_perturbation(text_ls, lang_codemix, top_words_perturb)\n",
    "        elif perturbation_technique == \"synonym_replacement\":\n",
    "            perturbed_text = synonym_replacement(text_ls, top_words_perturb)\n",
    "        \n",
    "        start_time_use = time.time()\n",
    "        first_perturbation_sim_score = sim_predictor.semantic_sim(original_text, perturbed_text)\n",
    "        end_time_use = round(time.time() - start_time, 2)\n",
    "        \n",
    "        ic(end_time_use)\n",
    "        \n",
    "        ic(first_perturbation_sim_score)\n",
    "        \n",
    "        words_perturb_candidates = []\n",
    "        if first_perturbation_sim_score < sim_score_threshold:\n",
    "            words_perturb_candidates.append(top_words_perturb)\n",
    "            swapped = swap_minimum_importance_words(words_perturb, top_words_perturb)\n",
    "            for s in swapped:\n",
    "                words_perturb_candidates.append(s)\n",
    "\n",
    "            words_perturb_candidates = [[tuple(w) for w in wpc] for wpc in words_perturb_candidates]\n",
    "\n",
    "            candidate_comparison = {}\n",
    "            for wpc in words_perturb_candidates:\n",
    "                \n",
    "                if perturbation_technique == \"codemixing\":\n",
    "#                     bisa dicache, gaperlu request satu2\n",
    "                    perturbed_candidate = codemix_perturbation(text_ls, lang_codemix, wpc)\n",
    "                elif perturbation_technique == \"synonym_replacement\":\n",
    "                    perturbed_candidate = synonym_replacement(text_ls, wpc)\n",
    "                    \n",
    "                perturbed_candidate_sim_score = sim_predictor.semantic_sim(original_text, perturbed_candidate)\n",
    "                \n",
    "                candidate_comparison[perturbed_candidate] = (perturbed_candidate_sim_score, wpc[-1][-1], wpc)\n",
    "\n",
    "            sorted_candidate_comparison = sorted(candidate_comparison.keys(), key=lambda x: (float(candidate_comparison[x][0]), float(candidate_comparison[x][1])), reverse=True)\n",
    "\n",
    "            perturbed_text = sorted_candidate_comparison[0]\n",
    "            \n",
    "            top_words_perturb = candidate_comparison[sorted_candidate_comparison[0]][-1]\n",
    "            trans_word = [twp[1] for twp in top_words_perturb]\n",
    "            \n",
    "        perturbed_semantic_sim = sim_predictor.semantic_sim(original_text, perturbed_text)\n",
    "        if perturbed_semantic_sim < sim_score_threshold:\n",
    "            perturbed_text = original_text\n",
    "            perturbed_semantic_sim = 1.000\n",
    "        \n",
    "        perturbed_label, perturbed_probs, perturbed_prob = logit_prob(perturbed_text, predictor, tokenizer)\n",
    "        \n",
    "        \n",
    "        translated_words = trans_dict(trans_word, perturbation_technique, lang_codemix)\n",
    "        \n",
    "        orig_probs = np.round(orig_probs.detach().cpu().numpy().tolist(),4)\n",
    "        perturbed_probs = np.round(perturbed_probs.detach().cpu().numpy().tolist(),4)\n",
    "        \n",
    "        running_time = round(time.time() - start_time, 2)\n",
    "        \n",
    "        \n",
    "        return perturbed_text, perturbed_semantic_sim, orig_label, orig_probs, perturbed_label, perturbed_probs, translated_words, running_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efa9058f-8e82-419e-a12f-0e2abf856df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 10:24:35.049047: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib:/usr/local/cuda/lib\n",
      "2022-04-12 10:24:35.050411: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "model_target=\"IndoBERT\"\n",
    "downstream_task=\"sentiment\"\n",
    "attack_strategy=\"adversarial\"\n",
    "# finetune_epoch=1\n",
    "num_sample=50\n",
    "# exp_name=\n",
    "perturbation_technique=\"codemixing\"\n",
    "perturb_ratio=0.4\n",
    "perturb_lang=\"en\"\n",
    "seed=26092020\n",
    "dataset=\"valid\"\n",
    "\n",
    "set_seed(seed)\n",
    "use = USE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0130c75f-d6be-4709-8b79-6b0137067a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, config, finetuned_model = init_model(model_target, downstream_task, seed)\n",
    "w2i, i2w = load_word_index(downstream_task)\n",
    "\n",
    "train_dataset, train_loader, train_path = load_dataset_loader(downstream_task, 'train', tokenizer)\n",
    "valid_dataset, valid_loader, valid_path = load_dataset_loader(downstream_task, 'valid', tokenizer)\n",
    "test_dataset, test_loader, test_path = load_dataset_loader(downstream_task, 'test', tokenizer)\n",
    "\n",
    "finetuned_model.to(device)\n",
    "# finetuned_model = fine_tuning_model(model, i2w, train_loader, valid_loader, finetune_epoch)\n",
    "\n",
    "if dataset == \"valid\":\n",
    "    exp_dataset = valid_dataset.load_dataset(valid_path)[381:381+num_sample]\n",
    "elif dataset == \"train\":\n",
    "    exp_dataset = train_dataset.load_dataset(train_path)\n",
    "# exp_dataset = dd.from_pandas(exp_dataset, npartitions=10)\n",
    "# exp_dataset = te.DataFrame.from_pandas(exp_dataset)\n",
    "text,label = None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ecfde42-e908-43db-8383-ba3106ec701f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                        | 0/50 [00:00<?, ?it/s]ic| len_text: 13\n",
      "ic| running_time_a: 0.46\n",
      "ic| end_time_use: 1.25\n",
      "ic| first_perturbation_sim_score: 1.0000001\n",
      "  4%|██▌                                                             | 2/50 [00:02<00:50,  1.05s/it]ic| len_text: 17\n",
      "ic| running_time_a: 0.63\n",
      "ic| end_time_use: 3.1\n",
      "ic| first_perturbation_sim_score: 0.9733958\n",
      "  6%|███▊                                                            | 3/50 [00:05<01:43,  2.21s/it]ic| len_text: 12\n",
      "ic| running_time_a: 0.32\n",
      "ic| end_time_use: 1.47\n",
      "ic| first_perturbation_sim_score: 0.89145684\n",
      "  8%|█████                                                           | 4/50 [00:08<01:46,  2.31s/it]ic| len_text: 26\n",
      "ic| running_time_a: 0.89\n",
      "ic| end_time_use: 2.22\n",
      "ic| first_perturbation_sim_score: 0.92155635\n",
      " 10%|██████▍                                                         | 5/50 [00:12<02:11,  2.93s/it]ic| len_text: 22\n",
      "ic| running_time_a: 0.9\n",
      "ic| end_time_use: 3.89\n",
      "ic| first_perturbation_sim_score: 0.94965285\n",
      " 12%|███████▋                                                        | 6/50 [00:19<03:04,  4.20s/it]ic| len_text: 29\n",
      "ic| running_time_a: 1.01\n",
      "ic| end_time_use: 3.99\n",
      "ic| first_perturbation_sim_score: 0.99050194\n",
      " 14%|████████▉                                                       | 7/50 [00:26<03:38,  5.08s/it]ic| len_text: 1\n",
      "ic| running_time_a: 0.15\n",
      "ic| end_time_use: 0.41\n",
      "ic| first_perturbation_sim_score: 0.2863596\n",
      " 16%|██████████▏                                                     | 8/50 [00:27<02:39,  3.79s/it]ic| len_text: 18\n",
      "ic| running_time_a: 0.62\n",
      "ic| end_time_use: 1.65\n",
      "ic| first_perturbation_sim_score: 0.9791034\n",
      " 20%|████████████▌                                                  | 10/50 [00:30<01:50,  2.77s/it]ic| len_text: 15\n",
      "ic| running_time_a: 0.57\n",
      "ic| end_time_use: 2.1\n",
      "ic| first_perturbation_sim_score: 0.9293883\n",
      " 22%|█████████████▊                                                 | 11/50 [00:34<01:57,  3.02s/it]ic| len_text: 36\n",
      "ic| running_time_a: 1.26\n",
      "ic| end_time_use: 2.8\n",
      "ic| first_perturbation_sim_score: 0.9900516\n",
      " 24%|███████████████                                                | 12/50 [00:39<02:13,  3.50s/it]ic| len_text: 22\n",
      "ic| running_time_a: 0.84\n",
      "ic| end_time_use: 2.51\n",
      "ic| first_perturbation_sim_score: 0.9661272\n",
      " 26%|████████████████▍                                              | 13/50 [00:44<02:25,  3.95s/it]ic| len_text: 64\n",
      "ic| running_time_a: 2.15\n",
      "ic| end_time_use: 7.34\n",
      "ic| first_perturbation_sim_score: 0.9511417\n",
      " 28%|█████████████████▋                                             | 14/50 [00:55<03:35,  5.98s/it]ic| len_text: 5\n",
      "ic| running_time_a: 0.37\n",
      "ic| end_time_use: 0.59\n",
      "ic| first_perturbation_sim_score: 1.0\n",
      " 30%|██████████████████▉                                            | 15/50 [00:56<02:37,  4.51s/it]ic| len_text: 34\n",
      "ic| running_time_a: 1.21\n",
      "ic| end_time_use: 3.87\n",
      "ic| first_perturbation_sim_score: 1.0\n",
      " 32%|████████████████████▏                                          | 16/50 [01:04<03:13,  5.68s/it]ic| len_text: 20\n",
      "ic| running_time_a: 1.01\n",
      "ic| end_time_use: 3.26\n",
      "ic| first_perturbation_sim_score: 0.9850528\n",
      " 34%|█████████████████████▍                                         | 17/50 [01:09<03:00,  5.48s/it]ic| len_text: 18\n",
      "ic| running_time_a: 0.62\n",
      "ic| end_time_use: 2.04\n",
      "ic| first_perturbation_sim_score: 1.0\n",
      " 36%|██████████████████████▋                                        | 18/50 [01:15<02:56,  5.52s/it]ic| len_text: 27\n",
      "ic| running_time_a: 1.03\n",
      "ic| end_time_use: 4.45\n",
      "ic| first_perturbation_sim_score: 0.97114015\n",
      " 38%|███████████████████████▉                                       | 19/50 [01:23<03:18,  6.39s/it]ic| len_text: 21\n",
      "ic| running_time_a: 0.75\n",
      "ic| end_time_use: 1.89\n",
      "ic| first_perturbation_sim_score: 0.96162057\n",
      " 40%|█████████████████████████▏                                     | 20/50 [01:27<02:43,  5.45s/it]ic| len_text: 55\n",
      "ic| running_time_a: 2.02\n",
      "ic| end_time_use: 6.51\n",
      "ic| first_perturbation_sim_score: 0.9934869\n",
      " 42%|██████████████████████████▍                                    | 21/50 [01:37<03:20,  6.92s/it]ic| len_text: 38\n",
      "ic| running_time_a: 1.37\n",
      "ic| end_time_use: 4.24\n",
      "ic| first_perturbation_sim_score: 0.9844829\n",
      " 44%|███████████████████████████▋                                   | 22/50 [01:44<03:16,  7.03s/it]ic| len_text: 38\n",
      "ic| running_time_a: 1.5\n",
      "ic| end_time_use: 4.28\n",
      "ic| first_perturbation_sim_score: 0.97975916\n",
      " 46%|████████████████████████████▉                                  | 23/50 [01:50<03:02,  6.76s/it]ic| len_text: 20\n",
      "ic| running_time_a: 0.61\n",
      "ic| end_time_use: 3.15\n",
      "ic| first_perturbation_sim_score: 0.96821904\n",
      " 48%|██████████████████████████████▏                                | 24/50 [01:55<02:40,  6.19s/it]ic| len_text: 15\n",
      "ic| running_time_a: 0.39\n",
      "ic| end_time_use: 1.91\n",
      "ic| first_perturbation_sim_score: 0.93524384\n",
      " 50%|███████████████████████████████▌                               | 25/50 [01:58<02:12,  5.28s/it]ic| len_text: 46\n",
      "ic| running_time_a: 1.51\n",
      "ic| end_time_use: 5.29\n",
      "ic| first_perturbation_sim_score: 1.0\n",
      " 52%|████████████████████████████████▊                              | 26/50 [02:07<02:32,  6.34s/it]ic| len_text: 32\n",
      "ic| running_time_a: 1.33\n",
      "ic| end_time_use: 3.08\n",
      "ic| first_perturbation_sim_score: 1.0\n",
      " 54%|██████████████████████████████████                             | 27/50 [02:12<02:16,  5.93s/it]ic| len_text: 32\n",
      "ic| running_time_a: 1.21\n",
      "ic| end_time_use: 3.07\n",
      "ic| first_perturbation_sim_score: 0.9803485\n",
      " 56%|███████████████████████████████████▎                           | 28/50 [02:18<02:10,  5.93s/it]ic| len_text: 32\n",
      "ic| running_time_a: 1.09\n",
      "ic| end_time_use: 2.93\n",
      "ic| first_perturbation_sim_score: 1.0\n",
      " 60%|█████████████████████████████████████▊                         | 30/50 [02:23<01:17,  3.88s/it]ic| len_text: 35\n",
      "ic| running_time_a: 0.99\n",
      "ic| end_time_use: 4.22\n",
      "ic| first_perturbation_sim_score: 0.98758\n",
      " 62%|███████████████████████████████████████                        | 31/50 [02:30<01:32,  4.84s/it]ic| len_text: 49\n",
      "ic| running_time_a: 1.7\n",
      "ic| end_time_use: 4.02\n",
      "ic| first_perturbation_sim_score: 0.993444\n",
      " 64%|████████████████████████████████████████▎                      | 32/50 [02:36<01:36,  5.36s/it]ic| len_text: 29\n",
      "ic| running_time_a: 0.87\n",
      "ic| end_time_use: 2.42\n",
      "ic| first_perturbation_sim_score: 0.98938316\n",
      " 68%|██████████████████████████████████████████▊                    | 34/50 [02:41<01:03,  3.97s/it]ic| len_text: 25\n",
      "ic| running_time_a: 0.89\n",
      "ic| end_time_use: 2.28\n",
      "ic| first_perturbation_sim_score: 0.91217\n",
      " 70%|████████████████████████████████████████████                   | 35/50 [02:45<00:58,  3.89s/it]ic| len_text: 22\n",
      "ic| running_time_a: 0.75\n",
      "ic| end_time_use: 2.65\n",
      "ic| first_perturbation_sim_score: 0.8981528\n",
      " 72%|█████████████████████████████████████████████▎                 | 36/50 [02:50<00:59,  4.23s/it]ic| len_text: 18\n",
      "ic| running_time_a: 0.72\n",
      "ic| end_time_use: 2.27\n",
      "ic| first_perturbation_sim_score: 0.99020994\n",
      " 74%|██████████████████████████████████████████████▌                | 37/50 [02:53<00:51,  3.98s/it]ic| len_text: 59ic| len_text: 47\n",
      "ic| running_time_a: 1.85\n",
      "ic| end_time_use: 6.58\n",
      "ic| first_perturbation_sim_score: 0.9923531\n",
      " 94%|███████████████████████████████████████████████████████████▏   | 47/50 [04:02<00:21,  7.06s/it]ic| len_text: 53\n",
      "ic| running_time_a: 1.81\n",
      "ic| end_time_use: 6.68\n",
      "ic| first_perturbation_sim_score: 0.9946986\n",
      " 96%|████████████████████████████████████████████████████████████▍  | 48/50 [04:12<00:15,  7.96s/it]ic| len_text: 68\n",
      "ic| running_time_a: 2.55\n",
      " 96%|████████████████████████████████████████████████████████████▍  | 48/50 [04:21<00:10,  5.45s/it]\n"
     ]
    },
    {
     "ename": "NotValidPayload",
     "evalue": "3 --> text must be a valid text with maximum 5000 character, otherwise it cannot be translated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotValidPayload\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m     exp_dataset[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperturbed_text\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperturbed_semantic_sim\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_label\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_proba\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperturbed_label\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperturbed_proba\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslated_word(s)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrunning_time(s)\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[43mexp_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext_ls\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrue_label\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentiment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfinetuned_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43matt_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mperturb_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43mperturbation_technique\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mperturbation_technique\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlang_codemix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mperturb_lang\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43msim_predictor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muse\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexpand\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m downstream_task \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     16\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/std.py:814\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    816\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py:8833\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   8822\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m   8824\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m   8825\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   8826\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   8831\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   8832\u001b[0m )\n\u001b[0;32m-> 8833\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/apply.py:727\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[0;32m--> 727\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/apply.py:851\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/apply.py:867\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    868\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    869\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    870\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    871\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/std.py:809\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[1;32m    808\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 809\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      2\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m     exp_dataset[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperturbed_text\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperturbed_semantic_sim\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_label\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_proba\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperturbed_label\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperturbed_proba\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslated_word(s)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrunning_time(s)\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m exp_dataset\u001b[38;5;241m.\u001b[39mprogress_apply(\n\u001b[0;32m----> 5\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext_ls\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrue_label\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentiment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfinetuned_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43matt_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mperturb_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43mperturbation_technique\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mperturbation_technique\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlang_codemix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mperturb_lang\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43msim_predictor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muse\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, result_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpand\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m downstream_task \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     16\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mattack\u001b[0;34m(text_ls, true_label, predictor, tokenizer, att_ratio, perturbation_technique, lang_codemix, sim_predictor, sim_score_threshold, sim_score_window, batch_size, import_score_threshold)\u001b[0m\n\u001b[1;32m     84\u001b[0m trans_word \u001b[38;5;241m=\u001b[39m [twp[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m twp \u001b[38;5;129;01min\u001b[39;00m top_words_perturb]\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m perturbation_technique \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcodemixing\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 87\u001b[0m     perturbed_text \u001b[38;5;241m=\u001b[39m \u001b[43mcodemix_perturbation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_ls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang_codemix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_words_perturb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m perturbation_technique \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msynonym_replacement\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     89\u001b[0m     perturbed_text \u001b[38;5;241m=\u001b[39m synonym_replacement(text_ls, top_words_perturb)\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mcodemix_perturbation\u001b[0;34m(words, target_lang, words_perturb)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m wp \u001b[38;5;129;01min\u001b[39;00m words_perturb:\n\u001b[1;32m     60\u001b[0m     new_wp\u001b[38;5;241m.\u001b[39mappend(wp[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 62\u001b[0m new_wp_trans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(new_wp, \u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_wp\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     64\u001b[0m supported_langs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mms\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_lang \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m supported_langs:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/deep_translator/google.py:117\u001b[0m, in \u001b[0;36mGoogleTranslator.translate_batch\u001b[0;34m(self, batch, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranslate_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: List[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    translate a list of texts\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    @param batch: list of texts you want to translate\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m    @return: list of translations\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_translate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/deep_translator/base.py:123\u001b[0m, in \u001b[0;36mBaseTranslator._translate_batch\u001b[0;34m(self, batch, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m arr \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch):\n\u001b[0;32m--> 123\u001b[0m     translated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     arr\u001b[38;5;241m.\u001b[39mappend(translated)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/deep_translator/google.py:55\u001b[0m, in \u001b[0;36mGoogleTranslator.translate\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranslate\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m    function to translate a text\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    @param text: desired text to translate\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    @return: str: translated text\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_input_valid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     56\u001b[0m         text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_same_source_target() \u001b[38;5;129;01mor\u001b[39;00m is_empty(text):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/deep_translator/validate.py:18\u001b[0m, in \u001b[0;36mis_input_valid\u001b[0;34m(text, min_chars, max_chars)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03mvalidate the target text to translate\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m@param min_chars: min characters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m@return: bool\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m text\u001b[38;5;241m.\u001b[39misdigit():\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotValidPayload(text)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m min_chars \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m<\u001b[39m max_chars:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotValidLength(text, min_chars, max_chars)\n",
      "\u001b[0;31mNotValidPayload\u001b[0m: 3 --> text must be a valid text with maximum 5000 character, otherwise it cannot be translated"
     ]
    }
   ],
   "source": [
    "if downstream_task == 'sentiment':\n",
    "    text = 'text'\n",
    "    label = 'sentiment'\n",
    "    exp_dataset[['perturbed_text', 'perturbed_semantic_sim', 'pred_label', 'pred_proba', 'perturbed_label', 'perturbed_proba', 'translated_word(s)', 'running_time(s)']] = exp_dataset.progress_apply(\n",
    "        lambda row: attack(\n",
    "            text_ls = row.text,\n",
    "            true_label = row.sentiment,\n",
    "            predictor = finetuned_model,\n",
    "            tokenizer = tokenizer, \n",
    "            att_ratio = perturb_ratio,\n",
    "            perturbation_technique = perturbation_technique,\n",
    "            lang_codemix = perturb_lang,\n",
    "            sim_predictor = use), axis=1, result_type='expand'\n",
    "    )\n",
    "elif downstream_task == 'emotion':\n",
    "    text = 'tweet'\n",
    "    label = 'label'\n",
    "    exp_dataset[['perturbed_text', 'perturbed_semantic_sim', 'pred_label', 'pred_proba', 'perturbed_label', 'perturbed_proba', 'translated_word(s)', 'running_time(s)']] = exp_dataset.progress_apply(\n",
    "        lambda row: attack(\n",
    "            text_ls = row.tweet.values,\n",
    "            true_label = row.label.values,\n",
    "            predictor = finetuned_model,\n",
    "            tokenizer = tokenizer, \n",
    "            att_ratio = perturb_ratio,\n",
    "            perturbation_technique = perturbation_technique,\n",
    "            lang_codemix = perturb_lang,\n",
    "            sim_predictor = use), axis=1, result_type='expand'\n",
    "    )\n",
    "\n",
    "before_attack = accuracy_score(exp_dataset[label], exp_dataset['pred_label'])\n",
    "after_attack = accuracy_score(exp_dataset[label], exp_dataset['perturbed_label'])\n",
    "\n",
    "exp_dataset.loc[exp_dataset.index[0], 'before_attack_acc'] = before_attack\n",
    "exp_dataset.loc[exp_dataset.index[0], 'after_attack_acc'] = after_attack\n",
    "exp_dataset.loc[exp_dataset.index[0], 'avg_semantic_sim'] = exp_dataset[\"perturbed_semantic_sim\"].mean()\n",
    "exp_dataset.loc[exp_dataset.index[0], 'avg_running_time(s)'] = exp_dataset[\"running_time(s)\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83b16bb-b416-4519-909b-ad6211215ee6",
   "metadata": {},
   "source": [
    "### Fungsi Awal banget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "913a33b3-e1fa-401e-9e3e-20eec418fa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len_text: 60\n",
      "ic| running_time_a: 2.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(59, 'diragukan', 0.001480978060399707), (27, 'enak', 0.0006103660272316347), (18, 'burgundy', 0.0005351185233486433), (55, 'makanan', 0.000532135112141674), (12, 'dago', 0.0004499789485095107), (8, 'romantis', 0.0003965279976370084), (46, 'taman', 0.00039581217539819136), (40, 'rerumputan', 0.00039318750260974866), (7, 'malam', 0.000384060906586825)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| end_time_use: 13.71\n",
      "ic| first_perturbation_sim_score: 0.96317273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('buat kalian yang cuma tahu tempat makan night romantic yang ada di dago boleh dicoba deh makan di burgundy sini di sini sih menurut saya tempat yang nice banget buat ngadem selain tempat nya yang hening kita juga bisa melihat grass yang hijau meski cuma di garden nya hehehe tapi balik lagi buat saya sih food haha tidak usah doubtful',\n",
       " 0.96317273,\n",
       " 0,\n",
       " array([9.981e-01, 8.000e-04, 1.100e-03]),\n",
       " 0,\n",
       " array([0.9943, 0.0013, 0.0044]),\n",
       " {'diragukan': 'doubtful',\n",
       "  'enak': 'nice',\n",
       "  'burgundy': 'burgundy',\n",
       "  'makanan': 'food',\n",
       "  'dago': 'dago',\n",
       "  'romantis': 'romantic',\n",
       "  'taman': 'garden',\n",
       "  'rerumputan': 'grass',\n",
       "  'malam': 'night'},\n",
       " 17.46)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack(\n",
    "    text_ls = exp_dataset.iloc[[48]].text.item(),\n",
    "    true_label = 0,\n",
    "    predictor = finetuned_model,\n",
    "    tokenizer = tokenizer, \n",
    "    att_ratio = perturb_ratio,\n",
    "    perturbation_technique = perturbation_technique,\n",
    "    lang_codemix = perturb_lang,\n",
    "    sim_predictor = use\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdf9577-ef43-407b-b707-252c0ea1853f",
   "metadata": {},
   "source": [
    "### Coba optimize di USE nya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0752ac79-d4fc-4f0c-be49-d1117ac29329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len_text: 60\n",
      "ic| running_time_a: 2.94\n",
      "/tmp/ipykernel_3216979/2609633789.py:56: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  leave_1_probs = torch.tensor(leave_1_probs).to(\"cuda:0\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(59, 'diragukan', 0.001480978060399707), (27, 'enak', 0.0006103660272316347), (18, 'burgundy', 0.0005351185233486433), (55, 'makanan', 0.000532135112141674), (12, 'dago', 0.0004499789485095107), (8, 'romantis', 0.0003965279976370084), (46, 'taman', 0.00039581217539819136), (40, 'rerumputan', 0.00039318750260974866), (7, 'malam', 0.000384060906586825)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| end_time_use: 7.37\n",
      "ic| first_perturbation_sim_score: 0.9631725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('buat kalian yang cuma tahu tempat makan night romantic yang ada di dago boleh dicoba deh makan di burgundy sini di sini sih menurut saya tempat yang nice banget buat ngadem selain tempat nya yang hening kita juga bisa melihat grass yang hijau meski cuma di garden nya hehehe tapi balik lagi buat saya sih food haha tidak usah doubtful',\n",
       " 0.9631725,\n",
       " 0,\n",
       " array([9.981e-01, 8.000e-04, 1.100e-03]),\n",
       " 0,\n",
       " array([0.9943, 0.0013, 0.0044]),\n",
       " {'diragukan': 'doubtful',\n",
       "  'enak': 'nice',\n",
       "  'burgundy': 'burgundy',\n",
       "  'makanan': 'food',\n",
       "  'dago': 'dago',\n",
       "  'romantis': 'romantic',\n",
       "  'taman': 'garden',\n",
       "  'rerumputan': 'grass',\n",
       "  'malam': 'night'},\n",
       " 10.45)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack(\n",
    "    text_ls = exp_dataset.iloc[[48]].text.item(),\n",
    "    true_label = 0,\n",
    "    predictor = finetuned_model,\n",
    "    tokenizer = tokenizer, \n",
    "    att_ratio = perturb_ratio,\n",
    "    perturbation_technique = perturbation_technique,\n",
    "    lang_codemix = perturb_lang,\n",
    "    sim_predictor = use\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83533730-b3c7-471a-a06f-6233e86e8d2e",
   "metadata": {},
   "source": [
    "### Coba Optimize step 2+codemix perturbation pake batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd1475f7-9a37-4239-90ac-47db1fa26e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len_text: 60\n",
      "ic| running_time_a: 3.38\n",
      "/tmp/ipykernel_3411520/2609633789.py:56: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  leave_1_probs = torch.tensor(leave_1_probs).to(\"cuda:0\")\n",
      "ic| end_time_use: 6.72\n",
      "ic| first_perturbation_sim_score: 0.9972706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('buat kalian yang cuma tahu tempat makan night romantis yang ada di dago boleh dicoba deh makan di burgundy sini di sini sih menurut saya tempat yang enak banget buat ngadem selain tempat nya yang hening kita juga bisa melihat rerumputan yang hijau meski cuma di taman nya hehehe tapi balik lagi buat saya sih makanan haha tidak usah diragukan',\n",
       " 0.9972706,\n",
       " 0,\n",
       " array([9.981e-01, 8.000e-04, 1.100e-03]),\n",
       " 0,\n",
       " array([9.977e-01, 9.000e-04, 1.400e-03]),\n",
       " {'diragukan': 'doubtful',\n",
       "  'enak': 'nice',\n",
       "  'burgundy': 'burgundy',\n",
       "  'makanan': 'food',\n",
       "  'dago': 'dago',\n",
       "  'romantis': 'romantic',\n",
       "  'taman': 'garden',\n",
       "  'rerumputan': 'grass',\n",
       "  'malam': 'night'},\n",
       " 10.72)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack(\n",
    "    text_ls = exp_dataset.iloc[[48]].text.item(),\n",
    "    true_label = 0,\n",
    "    predictor = finetuned_model,\n",
    "    tokenizer = tokenizer, \n",
    "    att_ratio = perturb_ratio,\n",
    "    perturbation_technique = perturbation_technique,\n",
    "    lang_codemix = perturb_lang,\n",
    "    sim_predictor = use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0f70e96-30c9-4fd1-b2ae-1d3c61bdd7d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4100b193-fdc3-4d59-bc11-ab02fa4611f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddac99a0-2976-48c0-8c91-20e9c86de269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.37\n"
     ]
    }
   ],
   "source": [
    "word = 'buat kalian yang cuma tahu tempat makan malam romantis yang ada di dago , boleh dicoba deh makan di burgundy sini , di sini sih menurut saya tempat yang enak banget buat ngadem selain tempat nya yang hening kita juga bisa melihat rerumputan yang hijau meski cuma di taman nya , hehehe . tapi balik lagi buat saya sih makanan , haha . tidak usah diragukan .'\n",
    "words = word.split()\n",
    "wp = [(59, 'diragukan', 0.001480978060399707), (27, 'enak', 0.0006103660272316347), (18, 'burgundy', 0.0005351185233486433), (55, 'makanan', 0.000532135112141674), (12, 'dago', 0.0004499789485095107), (8, 'romantis', 0.0003965279976370084), (46, 'taman', 0.00039581217539819136), (40, 'rerumputan', 0.00039318750260974866), (7, 'malam', 0.000384060906586825)]\n",
    "target_lang = \"en\"\n",
    "\n",
    "start_time = time.time()\n",
    "codemix_perturbation(words, target_lang, wp)\n",
    "print(round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92985e6-016d-4609-bb85-70034bbeb2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00c2493-4560-4d0a-b00a-ac4ec680cb3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c95ec8-1c18-42da-8abc-8bc0bdab7b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e3f4a9-9263-40c8-bc14-523ab596c096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8d3139-25ad-4e6e-b504-d88b2a388cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24548f59-5e35-4aad-9b84-0ae0a81c5d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9583c461-28d2-4c46-b2ed-0dc9f27a2479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'buat kalian yang cuma tahu tempat makan malam romantis yang ada di dago , boleh dicoba deh makan di burgundy sini , di sini sih menurut saya tempat yang enak banget buat ngadem selain tempat nya yang hening kita juga bisa melihat rerumputan yang hijau meski cuma di taman nya , hehehe . tapi balik lagi buat saya sih makanan , haha . tidak usah diragukan .'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dataset.iloc[[48]].text.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aacafc0-3376-4cd2-884b-dc98c7967b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 08:25:36.804004: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib:/usr/local/cuda/lib\n",
      "2022-04-12 08:25:36.805836: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c01636e7-36cb-4cf4-aee6-da4316774ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs: 0\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs:\", len(physical_devices))\n",
    "# !pip list cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55317fa7-d77c-40a1-9ecd-a145385008e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.77\n"
     ]
    }
   ],
   "source": [
    "texts = ['diragukan', 'enak', 'burgundy', 'makanan', 'dago', 'romantis', 'taman', 'rerumputan', 'malam']\n",
    "\n",
    "start_time = time.time()\n",
    "translated_1 = []\n",
    "translator = GoogleTranslator(source=\"id\", target=\"en\")\n",
    "for txt in texts:\n",
    "    translated_1.append(translator.translate(txt))\n",
    "print(round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cbd92533-750e-4274-a722-04d0f0c89141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.44\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "translated_2 = translator.translate_batch(texts)\n",
    "print(round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469821c-a347-4a04-88df-c9fb8ad61798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !. myenv/bin/activate\n",
    "# !pip uninstall tensorflow -y\n",
    "# !pip uninstall tensorflow-gpu -y\n",
    "\n",
    "!pip install tensorflow --ignore-installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31ad62f6-736d-4323-a8c7-7b011b8004e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/cuda/lib:/usr/local/cuda/lib\n"
     ]
    }
   ],
   "source": [
    "# %export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64\n",
    "!echo $LD_LIBRARY_PATH"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
