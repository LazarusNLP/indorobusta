{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\omw.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Synonym Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quaker',\n",
       " 'associate',\n",
       " 'sister',\n",
       " 'blighter',\n",
       " 'chap',\n",
       " 'mate',\n",
       " 'flock',\n",
       " 'fella',\n",
       " 'lad',\n",
       " 'crony',\n",
       " 'pal',\n",
       " 'cuss',\n",
       " 'bloke',\n",
       " 'bedfellow',\n",
       " 'friend',\n",
       " 'sidekick',\n",
       " 'teammate',\n",
       " 'truelove',\n",
       " 'steady',\n",
       " 'gent',\n",
       " 'familiar',\n",
       " 'sweetie',\n",
       " 'acquaintance',\n",
       " 'brother',\n",
       " 'companion',\n",
       " 'feller',\n",
       " 'ally',\n",
       " 'buddy',\n",
       " 'chum',\n",
       " 'fellow',\n",
       " 'covey',\n",
       " 'comrade',\n",
       " 'sweetheart']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_synonyms(word):\n",
    "    \n",
    "    synonyms = set()\n",
    "    \n",
    "    for syn in wordnet.synsets(word, lang=\"ind\"):\n",
    "        for l in syn.lemmas():\n",
    "            synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "            synonyms.add(synonym) \n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "    \n",
    "    return list(synonyms)\n",
    "\n",
    "get_synonyms(\"kawan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = []\n",
    "for w in stopwords.words('indonesian'):\n",
    "    stop_words.append(w)\n",
    "# print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_replacement(words, n):\n",
    "    \n",
    "    words = words.split()\n",
    "    \n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "    # ic(random_word_list)\n",
    "\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    \n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        # ic(random_word)\n",
    "        # ic(synonyms)\n",
    "        \n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(list(synonyms))\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        \n",
    "        if num_replaced >= n: #only replace up to n words\n",
    "            break\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hai kawan bagaimana kabar\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.id import Indonesian\n",
    "\n",
    "teks = 'hai kawan bagaimana kabarmu'\n",
    "# tokenizer = Indonesian()\n",
    "\n",
    "\n",
    "# sent = \" \".join([token.lemma_ if len(token.lemma_) > 0 else token.text for token in tokenizer(teks)])\n",
    "# print(sent)\n",
    "teks = stemmer.stem(teks)\n",
    "print(teks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Example of Synonym Replacement: hai kawan bagaimana kabar\n",
      " Example of Synonym Replacement: sigh kawan bagaimana kabar\n",
      " Example of Synonym Replacement: suspire crony bagaimana kabar\n"
     ]
    }
   ],
   "source": [
    "for n in range(3):\n",
    "    print(f\" Example of Synonym Replacement: {synonym_replacement(teks,n)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('flock.n.02')\n",
      "Synset('covey.n.01')\n",
      "Synset('friend.n.05')\n",
      "Synset('acquaintance.n.03')\n",
      "Synset('ally.n.02')\n",
      "Synset('associate.n.01')\n",
      "Synset('bedfellow.n.01')\n",
      "Synset('brother.n.04')\n",
      "Synset('buddy.n.01')\n",
      "Synset('chap.n.01')\n",
      "Synset('companion.n.01')\n",
      "Synset('friend.n.01')\n",
      "Synset('mate.n.08')\n",
      "Synset('sister.n.03')\n",
      "Synset('sweetheart.n.01')\n",
      "Synset('teammate.n.01')\n"
     ]
    }
   ],
   "source": [
    "for syn in wordnet.synsets(\"kawan\", lang=\"ind\"):\n",
    "    print(syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('animal_group.n.01')]\n",
      "[Synset('gathering.n.01')]\n",
      "[Synset('christian.n.01')]\n",
      "[Synset('person.n.01')]\n",
      "[Synset('associate.n.01')]\n",
      "[Synset('peer.n.01')]\n",
      "[Synset('associate.n.01')]\n",
      "[Synset('friend.n.01')]\n",
      "[Synset('friend.n.01')]\n",
      "[Synset('male.n.02')]\n",
      "[Synset('friend.n.01')]\n",
      "[Synset('person.n.01')]\n",
      "[Synset('friend.n.01')]\n",
      "[Synset('member.n.01')]\n",
      "[Synset('lover.n.01')]\n",
      "[Synset('associate.n.01')]\n",
      "[Lemma('flock.n.02.kawan'), Lemma('covey.n.01.kawan'), Lemma('friend.n.05.kawan'), Lemma('acquaintance.n.03.kawan'), Lemma('ally.n.02.kawan'), Lemma('associate.n.01.kawan'), Lemma('bedfellow.n.01.kawan'), Lemma('brother.n.04.kawan'), Lemma('buddy.n.01.kawan'), Lemma('chap.n.01.kawan'), Lemma('companion.n.01.kawan'), Lemma('friend.n.01.kawan'), Lemma('mate.n.08.kawan'), Lemma('sister.n.03.kawan'), Lemma('sweetheart.n.01.kawan'), Lemma('teammate.n.01.kawan')]\n"
     ]
    }
   ],
   "source": [
    "kawan_lemmas = wordnet.lemmas(\"kawan\", lang=\"ind\")\n",
    "for lem in kawan_lemmas:\n",
    "    print(lem.synset().hypernyms())\n",
    "\n",
    "print(kawan_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Synset('animal_group.n.01')], [Synset('gathering.n.01')], [Synset('christian.n.01')], [Synset('person.n.01')], [Synset('associate.n.01')], [Synset('peer.n.01')], [Synset('associate.n.01')], [Synset('friend.n.01')], [Synset('friend.n.01')], [Synset('male.n.02')], [Synset('friend.n.01')], [Synset('person.n.01')], [Synset('friend.n.01')], [Synset('member.n.01')], [Synset('lover.n.01')], [Synset('associate.n.01')]]\n"
     ]
    }
   ],
   "source": [
    "hypernyms = []\n",
    "for lem in kawan_lemmas:\n",
    "    hypernyms.append(lem.synset().hypernyms())\n",
    "    \n",
    "print(hypernyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[Lemma('gathering.n.01.kumpulan'), Lemma('gathering.n.01.perhimpunan'), Lemma('gathering.n.01.persatuan'), Lemma('gathering.n.01.pertemuan'), Lemma('gathering.n.01.rapat'), Lemma('gathering.n.01.rapat_umum')]\n",
      "[Lemma('christian.n.01.Nasrani')]\n",
      "[Lemma('person.n.01.individu'), Lemma('person.n.01.insan'), Lemma('person.n.01.manusia'), Lemma('person.n.01.orang'), Lemma('person.n.01.seorang'), Lemma('person.n.01.seseorang'), Lemma('person.n.01.sukma'), Lemma('person.n.01.unik')]\n",
      "[Lemma('associate.n.01.bersekutu'), Lemma('associate.n.01.kawan'), Lemma('associate.n.01.rekan'), Lemma('associate.n.01.sekutu'), Lemma('associate.n.01.teman'), Lemma('associate.n.01.teman_sejawat'), Lemma('associate.n.01.kolega')]\n",
      "[Lemma('peer.n.01.sesama'), Lemma('peer.n.01.sama'), Lemma('peer.n.01.setara'), Lemma('peer.n.01.tolok')]\n",
      "[Lemma('associate.n.01.bersekutu'), Lemma('associate.n.01.kawan'), Lemma('associate.n.01.rekan'), Lemma('associate.n.01.sekutu'), Lemma('associate.n.01.teman'), Lemma('associate.n.01.teman_sejawat'), Lemma('associate.n.01.kolega')]\n",
      "[Lemma('friend.n.01.dongan'), Lemma('friend.n.01.handai'), Lemma('friend.n.01.ikhwan'), Lemma('friend.n.01.kanti'), Lemma('friend.n.01.kawan'), Lemma('friend.n.01.sahabat'), Lemma('friend.n.01.saki'), Lemma('friend.n.01.teman'), Lemma('friend.n.01.bendu'), Lemma('friend.n.01.mitra'), Lemma('friend.n.01.tolan')]\n",
      "[Lemma('friend.n.01.dongan'), Lemma('friend.n.01.handai'), Lemma('friend.n.01.ikhwan'), Lemma('friend.n.01.kanti'), Lemma('friend.n.01.kawan'), Lemma('friend.n.01.sahabat'), Lemma('friend.n.01.saki'), Lemma('friend.n.01.teman'), Lemma('friend.n.01.bendu'), Lemma('friend.n.01.mitra'), Lemma('friend.n.01.tolan')]\n",
      "[Lemma('male.n.02.jantan'), Lemma('male.n.02.lelaki'), Lemma('male.n.02.manusia'), Lemma('male.n.02.pria')]\n",
      "[Lemma('friend.n.01.dongan'), Lemma('friend.n.01.handai'), Lemma('friend.n.01.ikhwan'), Lemma('friend.n.01.kanti'), Lemma('friend.n.01.kawan'), Lemma('friend.n.01.sahabat'), Lemma('friend.n.01.saki'), Lemma('friend.n.01.teman'), Lemma('friend.n.01.bendu'), Lemma('friend.n.01.mitra'), Lemma('friend.n.01.tolan')]\n",
      "[Lemma('person.n.01.individu'), Lemma('person.n.01.insan'), Lemma('person.n.01.manusia'), Lemma('person.n.01.orang'), Lemma('person.n.01.seorang'), Lemma('person.n.01.seseorang'), Lemma('person.n.01.sukma'), Lemma('person.n.01.unik')]\n",
      "[Lemma('friend.n.01.dongan'), Lemma('friend.n.01.handai'), Lemma('friend.n.01.ikhwan'), Lemma('friend.n.01.kanti'), Lemma('friend.n.01.kawan'), Lemma('friend.n.01.sahabat'), Lemma('friend.n.01.saki'), Lemma('friend.n.01.teman'), Lemma('friend.n.01.bendu'), Lemma('friend.n.01.mitra'), Lemma('friend.n.01.tolan')]\n",
      "[Lemma('member.n.01.ahli'), Lemma('member.n.01.anggota'), Lemma('member.n.01.warga')]\n",
      "[Lemma('lover.n.01.kekasih'), Lemma('lover.n.01.kendak')]\n",
      "[Lemma('associate.n.01.bersekutu'), Lemma('associate.n.01.kawan'), Lemma('associate.n.01.rekan'), Lemma('associate.n.01.sekutu'), Lemma('associate.n.01.teman'), Lemma('associate.n.01.teman_sejawat'), Lemma('associate.n.01.kolega')]\n"
     ]
    }
   ],
   "source": [
    "for hypernym in hypernyms:\n",
    "    print(hypernym[0].lemmas(lang=\"ind\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Random Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_deletion(words, p):\n",
    "\n",
    "    words = words.split()\n",
    "    \n",
    "    #obviously, if there's only one word, don't delete it\n",
    "    if len(words) == 1:\n",
    "        return words\n",
    "\n",
    "    #randomly delete words with probability p\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        r = random.uniform(0, 1)\n",
    "        if r > p:\n",
    "            new_words.append(word)\n",
    "\n",
    "    #if you end up deleting all words, just return a random word\n",
    "    if len(new_words) == 0:\n",
    "        rand_int = random.randint(0, len(words)-1)\n",
    "        return [words[rand_int]]\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hai kawan bagaimana kabar\n",
      "hai kawan bagaimana\n",
      "kabar\n"
     ]
    }
   ],
   "source": [
    "print(random_deletion(teks,0.2))\n",
    "print(random_deletion(teks,0.3))\n",
    "print(random_deletion(teks,0.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Random Swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_word(new_words):\n",
    "    \n",
    "    random_idx_1 = random.randint(0, len(new_words)-1)\n",
    "    random_idx_2 = random_idx_1\n",
    "    counter = 0\n",
    "    \n",
    "    while random_idx_2 == random_idx_1:\n",
    "        random_idx_2 = random.randint(0, len(new_words)-1)\n",
    "        counter += 1\n",
    "        \n",
    "        if counter > 3:\n",
    "            return new_words\n",
    "    \n",
    "    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] \n",
    "    return new_words\n",
    "\n",
    "# This will Swap the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_swap(words, n):\n",
    "    \n",
    "    words = words.split()\n",
    "    new_words = words.copy()\n",
    "    # n is the number of words to be swapped\n",
    "    for _ in range(n):\n",
    "        new_words = swap_word(new_words)\n",
    "        \n",
    "    sentence = ' '.join(new_words)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hai kawan kabar bagaimana\n",
      "kawan bagaimana hai kabar\n",
      "bagaimana hai kabar kawan\n"
     ]
    }
   ],
   "source": [
    "print(random_swap(teks,1))\n",
    "print(random_swap(teks,2))\n",
    "print(random_swap(teks,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Random Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_insertion(words, n):\n",
    "    \n",
    "    words = words.split()\n",
    "    new_words = words.copy()\n",
    "    \n",
    "    for _ in range(n):\n",
    "        add_word(new_words)\n",
    "        \n",
    "    sentence = ' '.join(new_words)\n",
    "    return sentence\n",
    "\n",
    "def add_word(new_words):\n",
    "    \n",
    "    synonyms = []\n",
    "    counter = 0\n",
    "    \n",
    "    while len(synonyms) < 1:\n",
    "        random_word = new_words[random.randint(0, len(new_words)-1)]\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        counter += 1\n",
    "        if counter >= 10:\n",
    "            return\n",
    "        \n",
    "    random_synonym = synonyms[0]\n",
    "    random_idx = random.randint(0, len(new_words)-1)\n",
    "    new_words.insert(random_idx, random_synonym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hai kawan however bagaimana kabar\n",
      "hai quaker kawan bagaimana quaker kabar\n",
      "however sigh hai kawan however bagaimana kabar\n"
     ]
    }
   ],
   "source": [
    "print(random_insertion(teks,1))\n",
    "print(random_insertion(teks,2))\n",
    "print(random_insertion(teks,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original Sentence : hai kawan bagaimana kabar\n",
      " SR Augmented Sentence : sigh mate bagaimana kabar\n",
      " RD Augmented Sentence : bagaimana kabar\n",
      " RS Augmented Sentence : kawan bagaimana hai kabar\n",
      " RI Augmented Sentence : quaker hai kawan sigh bagaimana kabar\n"
     ]
    }
   ],
   "source": [
    "def aug(sent,n,p):\n",
    "    print(f\" Original Sentence : {sent}\")\n",
    "    print(f\" SR Augmented Sentence : {synonym_replacement(sent,n)}\")\n",
    "    print(f\" RD Augmented Sentence : {random_deletion(sent,p)}\")\n",
    "    print(f\" RS Augmented Sentence : {random_swap(sent,n)}\")\n",
    "    print(f\" RI Augmented Sentence : {random_insertion(sent,n)}\")\n",
    "    \n",
    "aug(teks,2,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e915f0a29dc84041eaeb02b7b1a21c440e37a87b61d44d5e84a515737dc82bc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
